---
title: "Day-3 Perceptron"
date: "2025-06-25"
thumbnail: "../../../assets/img/ARM/AI/image copy 10.png"
---

## 1. Perceptron ì´ë€?
- ì¸ê°„ì˜ ë‡Œë¥¼ ê¸°ê³„ì ìœ¼ë¡œ ëª¨ë¸ë§
  
- í¼ì…‰íŠ¸ë¡ ì€ ìƒë¬¼í•™ì  ë‰´ëŸ°ì„ ìˆ˜í•™ì ìœ¼ë¡œ ëª¨ë¸ë§í•œ 'ì¸ê³µ ë‰´ëŸ°'ìœ¼ë¡œ, ì—¬ëŸ¬ ì…ë ¥ ì‹ í˜¸ë¥¼ ë°›ì•„ ê°ê°ì˜ ê°€ì¤‘ì¹˜ë¥¼ ê³±í•œ í›„
ì´ë¥¼ í•©ì‚°í•´ í™œì„±í™” í•¨ìˆ˜ë¥¼ í†µí•´ ë‹¨ì¼ ì‹ í˜¸ë¥¼ ì¶œë ¥í•œë‹¤.

- í¼ì…‰íŠ¸ë¡ ì˜ ì¶œë ¥ì€ ì‹ í˜¸ ìœ ë¬´(1 ë˜ëŠ” 0)ë¡œ í‘œí˜„ë˜ë©°, ì´ì§„ ë¶„ë¥˜ ë¬¸ì œ í•´ê²°ì— íš¨ê³¼ì ì´ë‹¤.

- ì…ë ¥ ì‹ í˜¸ì˜ ì¤‘ìš”ì„±ì„ ë‚˜íƒ€ë‚´ëŠ” ê°€ì¤‘ì¹˜ëŠ” ë¨¸ì‹ ëŸ¬ë‹ì˜ 'í•™ìŠµ' ê³¼ì •ì—ì„œ ì¡°ì •ëœë‹¤.

## 2-1. êµ¬ì¡°
```
ì…ë ¥(x) â†’ ê°€ì¤‘ì¹˜(w) â†’ ê°€ì¤‘í•©(Î£) â†’ í™œì„±í™” í•¨ìˆ˜(f) â†’ ì¶œë ¥(y)
```
![alt text](<../../../assets/img/ARM/AI/image copy 26.png>)
- ì…ë ¥(Input) : AND ë˜ëŠ” OR ì—°ì‚°ì„ ìœ„í•œ ì…ë ¥ ì‹ í˜¸

- ê°€ì¤‘ì¹˜(Weight) : ì…ë ¥ ì‹ í˜¸ì— ë¶€ì—¬ë˜ëŠ” ì¤‘ìš”ë„ë¡œ, ê°€ì¤‘ì¹˜ê°€ í¬ë‹¤ëŠ” ê²ƒì€ ê·¸ ì…ë ¥ì´ ì¶œë ¥ì„ ê²°ì •í•˜ëŠ” ë° í° ì—­í• ì„ í•œë‹¤ëŠ” ì˜ë¯¸

- ê°€ì¤‘í•©(Weighted Sum) : ì…ë ¥ê°’ê³¼ ê°€ì¤‘ì¹˜ì˜ ê³±ì„ ëª¨ë‘ í•©í•œ ê°’

![alt text](<../../../assets/img/ARM/AI/image copy 25.png>)

- í™œì„±í™” í•¨ìˆ˜(Activation Function) : ì–´ë– í•œ ì‹ í˜¸ë¥¼ ì…ë ¥ë°›ì•„ ì´ë¥¼ ì ì ˆí•˜ê²Œ ì²˜ë¦¬í•˜ì—¬ ì¶œë ¥í•´ ì£¼ëŠ” í•¨ìˆ˜ë¡œ, ê°€ì¤‘í•©ì´ ì„ê³„ì¹˜(Threshold)ë¥¼ ë„˜ìœ¼ë©´ 1, ê·¸ë ‡ì§€ ì•Šìœ¼ë©´ 0ì„ ì¶œë ¥í•¨
  
- ì¶œë ¥(Output) : ìµœì¢… ê²°ê³¼(ë¶„ë¥˜)
---

## 2.-. ë™ì‘ ë°©ì‹

**Perceptronì˜ ì‘ë™ ë°©ì‹**

í¼ì…‰íŠ¸ë¡ ì€ ì§€ë„ í•™ìŠµ(supervised learning) ë°©ì‹ìœ¼ë¡œ í•™ìŠµí•œë‹¤. ì¦‰, ì…ë ¥ ë°ì´í„°ì™€ ê·¸ì— í•´ë‹¹í•˜ëŠ” ì •ë‹µ(ë ˆì´ë¸”)ì„ í•¨ê»˜ ì‚¬ìš©í•˜ì—¬ ëª¨ë¸ì„ í›ˆë ¨ì‹œí‚¤ë©° í•™ìŠµ ê³¼ì •ì€ ë‹¤ìŒê³¼ ê°™ë‹¤.

1. ì´ˆê¸°í™”: ê°€ì¤‘ì¹˜(wi)ì™€ í¸í–¥(b)ì„ ì‘ì€ ì„ì˜ì˜ ê°’ìœ¼ë¡œ ì´ˆê¸°í™”í•œë‹¤.

2. ì˜ˆì¸¡: í›ˆë ¨ ë°ì´í„°ì˜ í•œ ìƒ˜í”Œì„ ì…ë ¥ìœ¼ë¡œ ë°›ì•„ ê°€ì¤‘í•©ì„ ê³„ì‚°í•˜ê³ , í™œì„±í™” í•¨ìˆ˜ë¥¼ ì ìš©í•˜ì—¬ ì¶œë ¥ ê°’ì„ ì˜ˆì¸¡í•œë‹¤.

3. ì˜¤ë¥˜ ê³„ì‚°: ì˜ˆì¸¡ëœ ì¶œë ¥ ê°’ê³¼ ì‹¤ì œ ì •ë‹µ(ë ˆì´ë¸”)ì„ ë¹„êµí•˜ì—¬ ì˜¤ë¥˜ë¥¼ ê³„ì‚°í•œë‹¤.

4. ê°€ì¤‘ì¹˜ ë° í¸í–¥ ì—…ë°ì´íŠ¸: ì˜¤ë¥˜ê°€ ë°œìƒí•˜ë©´, ì˜¤ë¥˜ë¥¼ ì¤„ì´ëŠ” ë°©í–¥ìœ¼ë¡œ ê°€ì¤‘ì¹˜ì™€ í¸í–¥ì„ ì—…ë°ì´íŠ¸í•˜ë©° ì´ ì—…ë°ì´íŠ¸ëŠ” ì¼ë°˜ì ìœ¼ë¡œ ë‹¤ìŒ ê·œì¹™ì„ ë”°ë¥¸ë‹¤.

    ![alt text](<../../../assets/img/ARM/AI/image copy 24.png>)

    ì—¬ê¸°ì„œ Î· (eta)ëŠ” í•™ìŠµë¥ (learning rate)ë¡œ, ê°€ì¤‘ì¹˜ë¥¼ ì–¼ë§ˆë‚˜ í¬ê²Œ ì—…ë°ì´íŠ¸í• ì§€ë¥¼ ê²°ì •í•˜ëŠ” ê°’ì´ë‹¤. yëŠ” ì‹¤ì œ ì •ë‹µ, y^ëŠ” ì˜ˆì¸¡ ê°’ì„ ì˜ë¯¸í•œë‹¤.

5. ë°˜ë³µ: ëª¨ë“  í›ˆë ¨ ë°ì´í„° ìƒ˜í”Œì— ëŒ€í•´ 2~4ë‹¨ê³„ë¥¼ ë°˜ë³µí•œë‹¤. ì´ ê³¼ì •ì„ ì—¬ëŸ¬ ë²ˆ ë°˜ë³µí•˜ì—¬ (ì—í¬í¬, epoch) ê°€ì¤‘ì¹˜ì™€ í¸í–¥ì´ ìµœì ì˜ ê°’ì— ìˆ˜ë ´í•˜ë„ë¡ í•œë‹¤.

>epochs -> í•™ìŠµ íšŸìˆ˜

>lr -> learning rate (í•™ìŠµë¥ ) 
>
>lr = 0.1 â†’ ë³´í†µ ì‹œì‘ê°’ìœ¼ë¡œ ì ì ˆ
>
>lr = 0.01 â†’ ë” ëŠë¦¬ì§€ë§Œ ì•ˆì •ì 
>
>lr = 1.0 â†’ ë„ˆë¬´ í¬ë©´ ë°œì‚°í•  ìˆ˜ ìˆìŒ

---

## 3. ì‹¤ìŠµ 

**1. AND**
```python
import numpy as np
import matplotlib.pyplot as plt

class Perceptron:
    def __init__(self, input_size, lr=0.1, epochs=10):
        self.weights = np.zeros(input_size)
        self.bias = 0
        self.lr = lr
        self.epochs = epochs
        self.errors = []

    def activation(self, x):
        return np.where(x > 0, 1, 0)

    def predict(self, x):
        linear_output = np.dot(x, self.weights) + self.bias
        return self.activation(linear_output)

    def train(self, X, y):
        for epoch in range(self.epochs):
            total_error = 0
            for xi, target in zip(X, y):
                prediction = self.predict(xi)
                update = self.lr * (target - prediction)
                self.weights += update * xi
                self.bias += update
                total_error += int(update != 0.0)
            self.errors.append(total_error)
            print(f"Epoch {epoch+1}/{self.epochs}, Errors: {total_error}")
      
# AND ê²Œì´íŠ¸ ë°ì´í„°
X_and = np.array([[0,0],[0,1],[1,0],[1,1]])
y_and = np.array([0,0,0,1])

# í¼ì…‰íŠ¸ë¡  ëª¨ë¸ í›ˆë ¨
ppn_and = Perceptron(input_size=2)
ppn_and.train(X_and, y_and)

# ì˜ˆì¸¡ ê²°ê³¼ í™•ì¸
print("\nAND Gate Test:")
for x in X_and:
    print(f"Input: {x}, Predicted Output: {ppn_and.predict(x)}")
```

### í•™ìŠµ ë¡œê·¸
```
Epoch 1/10, Errors: 1
Epoch 2/10, Errors: 3
Epoch 3/10, Errors: 3
Epoch 4/10, Errors: 2
Epoch 5/10, Errors: 1
Epoch 6/10, Errors: 0
Epoch 7/10, Errors: 0
Epoch 8/10, Errors: 0
Epoch 9/10, Errors: 0
Epoch 10/10, Errors: 0
```
### ì˜ˆì¸¡ ê²°ê³¼
```
AND Gate Test:
Input: [0 0], Predicted Output:ê³¼ 0
Input: [0 1], Predicted Output: 0
Input: [1 0], Predicted Output: 0
Input: [1 1], Predicted Output: 1
```

### ê²½ê³„ ê²°ì • ì‹œê°í™”
```python
from matplotlib.colors import ListedColormap

def plot_decision_boundary(X, y, model):
    cmap_light = ListedColormap(['#FFAAAA', '#AAAAFF'])
    cmap_bold = ListedColormap(['#FF0000', '#0000FF'])

    h = .02  # mesh grid ê°„ê²©
    x_min, x_max = X[:, 0].min() - 1, X[:, 0].max() + 1
    y_min, y_max = X[:, 1].min() - 1, X[:, 1].max() + 1
    xx, yy = np.meshgrid(np.arange(x_min, x_max, h),
                         np.arange(y_min, y_max, h))

    Z = model.predict(np.c_[xx.ravel(), yy.ravel()])
    Z = Z.reshape(xx.shape)

    plt.figure(figsize=(8, 6))
    plt.contourf(xx, yy, Z, cmap=cmap_light)

    # ì‹¤ì œ ë°ì´í„° í¬ì¸íŠ¸ í‘œì‹œ
    plt.scatter(X[:, 0], X[:, 1], c=y, cmap=cmap_bold,
                edgecolor='k', s=100, marker='o')
    plt.xlabel('Input 1')
    plt.ylabel('Input 2')
    plt.title('Perceptron Decision Boundary')
    plt.show()

# AND ê²Œì´íŠ¸ ê²°ì • ê²½ê³„ ì‹œê°í™”
plot_decision_boundary(X_and, y_and, ppn_and)
```
### ê²½ê³„ ê²°ì • ì‹œê°í™” ê²°ê³¼
![alt text](<../../../assets/img/ARM/AI/image copy 11.png>)

### ì˜¤ë¥˜ ì‹œê°í™”
```python
#ì˜¤ë¥˜ ì‹œê°í™”
plt.figure(figsize=(8, 5))
plt.plot(range(1, len(ppn_and.errors) + 1), ppn_and.errors, marker='o')
plt.xlabel('Epochs')
plt.ylabel('Number of Errors')
plt.title('Perceptron Learning Error Over Epochs (And Gate)')
plt.grid(True)
plt.show()
```
### ì˜¤ë¥˜ ì‹œê°í™” ê²°ê³¼
![alt text](<../../../assets/img/ARM/AI/image copy 12.png>)

---
**2. OR**
```python
import numpy as np
import matplotlib.pyplot as plt

class Perceptron:
    def __init__(self, input_size, lr=0.1, epochs=10):
        self.weights = np.zeros(input_size)
        self.bias = 0
        self.lr = lr
        self.epochs = epochs
        self.errors = []

    def activation(self, x):
        return np.where(x > 0, 1, 0)

    def predict(self, x):
        linear_output = np.dot(x, self.weights) + self.bias
        return self.activation(linear_output)

    def train(self, X, y):
        for epoch in range(self.epochs):
            total_error = 0
            for xi, target in zip(X, y):
                prediction = self.predict(xi)
                update = self.lr * (target - prediction)
                self.weights += update * xi
                self.bias += update
                total_error += int(update != 0.0)
            self.errors.append(total_error)
            print(f"Epoch {epoch+1}/{self.epochs}, Errors: {total_error}")
      
# OR ê²Œì´íŠ¸ ë°ì´í„°
X_or = np.array([[0,0],[0,1],[1,0],[1,1]])
y_or = np.array([0,1,1,1])

# í¼ì…‰íŠ¸ë¡  ëª¨ë¸ í›ˆë ¨
ppn_or = Perceptron(input_size=2)
ppn_or.train(X_or, y_or)

# ì˜ˆì¸¡ ê²°ê³¼ í™•ì¸
print("\nOR Gate Test:")
for x in X_or:
    print(f"Input: {x}, Predicted Output: {ppn_or.predict(x)}")
```
### í•™ìŠµ ë¡œê·¸
```
Epoch 1/10, Errors: 1
Epoch 2/10, Errors: 2
Epoch 3/10, Errors: 1
Epoch 4/10, Errors: 0
Epoch 5/10, Errors: 0
Epoch 6/10, Errors: 0
Epoch 7/10, Errors: 0
Epoch 8/10, Errors: 0
Epoch 9/10, Errors: 0
Epoch 10/10, Errors: 0
```
### ì˜ˆì¸¡ ê²°ê³¼
```
OR Gate Test:
Input: [0 0], Predicted Output: 0
Input: [0 1], Predicted Output: 1
Input: [1 0], Predicted Output: 1
Input: [1 1], Predicted Output: 1
```
### ê²½ê³„ ê²°ì • ì‹œê°í™”
```python
from matplotlib.colors import ListedColormap

def plot_decision_boundary(X, y, model):
    cmap_light = ListedColormap(['#FFAAAA', '#AAAAFF'])
    cmap_bold = ListedColormap(['#FF0000', '#0000FF'])

    h = .02  # mesh grid ê°„ê²©
    x_min, x_max = X[:, 0].min() - 1, X[:, 0].max() + 1
    y_min, y_max = X[:, 1].min() - 1, X[:, 1].max() + 1
    xx, yy = np.meshgrid(np.arange(x_min, x_max, h),
                         np.arange(y_min, y_max, h))

    Z = model.predict(np.c_[xx.ravel(), yy.ravel()])
    Z = Z.reshape(xx.shape)

    plt.figure(figsize=(8, 6))
    plt.contourf(xx, yy, Z, cmap=cmap_light)

    # ì‹¤ì œ ë°ì´í„° í¬ì¸íŠ¸ í‘œì‹œ
    plt.scatter(X[:, 0], X[:, 1], c=y, cmap=cmap_bold,
                edgecolor='k', s=100, marker='o')
    plt.xlabel('Input 1')
    plt.ylabel('Input 2')
    plt.title('Perceptron Decision Boundary')
    plt.show()

# OR ê²Œì´íŠ¸ ê²°ì • ê²½ê³„ ì‹œê°í™”
plot_decision_boundary(X_or, y_or, ppn_or)
```

### ê²½ê³„ ê²°ì • ì‹œê°í™” ê²°ê³¼

![alt text](<../../../assets/img/ARM/AI/image copy 13.png>)

### ì˜¤ë¥˜ ì‹œê°í™”
```python
#ì˜¤ë¥˜ ì‹œê°í™”
plt.figure(figsize=(8, 5))
plt.plot(range(1, len(ppn_or.errors) + 1), ppn_or.errors, marker='o')
plt.xlabel('Epochs')
plt.ylabel('Number of Errors')
plt.title('Perceptron Learning Error Over Epochs (OR Gate)')
plt.grid(True)
plt.show()
```

### ì˜¤ë¥˜ ì‹œê°í™” ê²°ê³¼
![alt text](<../../../assets/img/ARM/AI/image copy 18.png>)

---
**3. NAND**
```python
import numpy as np
import matplotlib.pyplot as plt

class Perceptron:
    def __init__(self, input_size, lr=0.1, epochs=10):
        self.weights = np.zeros(input_size)
        self.bias = 0
        self.lr = lr
        self.epochs = epochs
        self.errors = []

    def activation(self, x):
        return np.where(x > 0, 1, 0)

    def predict(self, x):
        linear_output = np.dot(x, self.weights) + self.bias
        return self.activation(linear_output)

    def train(self, X, y):
        for epoch in range(self.epochs):
            total_error = 0
            for xi, target in zip(X, y):
                prediction = self.predict(xi)
                update = self.lr * (target - prediction)
                self.weights += update * xi
                self.bias += update
                total_error += int(update != 0.0)
            self.errors.append(total_error)
            print(f"Epoch {epoch+1}/{self.epochs}, Errors: {total_error}")
      
# NAND ê²Œì´íŠ¸ ë°ì´í„°
X_nand = np.array([[0,0],[0,1],[1,0],[1,1]])
y_nand = np.array([1,1,1,0])

# í¼ì…‰íŠ¸ë¡  ëª¨ë¸ í›ˆë ¨
ppn_nand = Perceptron(input_size=2)
ppn_nand.train(X_nand, y_nand)

# ì˜ˆì¸¡ ê²°ê³¼ í™•ì¸
print("\nNAND Gate Test:")
for x in X_nand:
    print(f"Input: {x}, Predicted Output: {ppn_nand.predict(x)}")
```
### í•™ìŠµ ë¡œê·¸
```
Epoch 1/10, Errors: 2
Epoch 2/10, Errors: 3
Epoch 3/10, Errors: 3
Epoch 4/10, Errors: 0
Epoch 5/10, Errors: 0
Epoch 6/10, Errors: 0
Epoch 7/10, Errors: 0
Epoch 8/10, Errors: 0
Epoch 9/10, Errors: 0
Epoch 10/10, Errors: 0
```
### ì˜ˆì¸¡ ê²°ê³¼
```
NAND Gate Test:
Input: [0 0], Predicted Output: 1
Input: [0 1], Predicted Output: 1
Input: [1 0], Predicted Output: 1
Input: [1 1], Predicted Output: 0

```
### ê²½ê³„ ê²°ì • ì‹œê°í™”
```python
from matplotlib.colors import ListedColormap

def plot_decision_boundary(X, y, model):
    cmap_light = ListedColormap(['#FFAAAA', '#AAAAFF'])
    cmap_bold = ListedColormap(['#FF0000', '#0000FF'])

    h = .02  # mesh grid ê°„ê²©
    x_min, x_max = X[:, 0].min() - 1, X[:, 0].max() + 1
    y_min, y_max = X[:, 1].min() - 1, X[:, 1].max() + 1
    xx, yy = np.meshgrid(np.arange(x_min, x_max, h),
                         np.arange(y_min, y_max, h))

    Z = model.predict(np.c_[xx.ravel(), yy.ravel()])
    Z = Z.reshape(xx.shape)

    plt.figure(figsize=(8, 6))
    plt.contourf(xx, yy, Z, cmap=cmap_light)

    # ì‹¤ì œ ë°ì´í„° í¬ì¸íŠ¸ í‘œì‹œ
    plt.scatter(X[:, 0], X[:, 1], c=y, cmap=cmap_bold,
                edgecolor='k', s=100, marker='o')
    plt.xlabel('Input 1')
    plt.ylabel('Input 2')
    plt.title('Perceptron Decision Boundary')
    plt.show()

# NAND ê²Œì´íŠ¸ ê²°ì • ê²½ê³„ ì‹œê°í™”
plot_decision_boundary(X_nand, y_nand, ppn_nand)
```

### ê²½ê³„ ê²°ì • ì‹œê°í™” ê²°ê³¼
![alt text](<../../../assets/img/ARM/AI/image copy 15.png>)

### ì˜¤ë¥˜ ì‹œê°í™”
```python
#ì˜¤ë¥˜ ì‹œê°í™”
plt.figure(figsize=(8, 5))
plt.plot(range(1, len(ppn_nand.errors) + 1), ppn_nand.errors, marker='o')
plt.xlabel('Epochs')
plt.ylabel('Number of Errors')
plt.title('Perceptron Learning Error Over Epochs (NAND Gate)')
plt.grid(True)
plt.show()
```
### ì˜¤ë¥˜ ì‹œê°í™” ê²°ê³¼
![alt text](<../../../assets/img/ARM/AI/image copy 17.png>)

---

**4. XOR**
```python
import numpy as np
import matplotlib.pyplot as plt

class Perceptron:
    def __init__(self, input_size, lr=0.1, epochs=10):
        self.weights = np.zeros(input_size)
        self.bias = 0
        self.lr = lr
        self.epochs = epochs
        self.errors = []

    def activation(self, x):
        return np.where(x > 0, 1, 0)

    def predict(self, x):
        linear_output = np.dot(x, self.weights) + self.bias
        return self.activation(linear_output)

    def train(self, X, y):
        for epoch in range(self.epochs):
            total_error = 0
            for xi, target in zip(X, y):
                prediction = self.predict(xi)
                update = self.lr * (target - prediction)
                self.weights += update * xi
                self.bias += update
                total_error += int(update != 0.0)
            self.errors.append(total_error)
            print(f"Epoch {epoch+1}/{self.epochs}, Errors: {total_error}")

# XOR ê²Œì´íŠ¸ ë°ì´í„°
X_xor = np.array([[0,0],[0,1],[1,0],[1,1]])
y_xor = np.array([0,1,1,0])

# í¼ì…‰íŠ¸ë¡  ëª¨ë¸ í›ˆë ¨
ppn_xor = Perceptron(input_size=2)
ppn_xor.train(X_xor, y_xor)

# ì˜ˆì¸¡ ê²°ê³¼ í™•ì¸
print("\nXOR Gate Test:")
for x in X_xor:
    print(f"Input: {x}, Predicted Output: {ppn_xor.predict(x)}")
```
### í•™ìŠµ ë¡œê·¸
```
Epoch 1/10, Errors: 2
Epoch 2/10, Errors: 3
Epoch 3/10, Errors: 4
Epoch 4/10, Errors: 4
Epoch 5/10, Errors: 4
Epoch 6/10, Errors: 4
Epoch 7/10, Errors: 4
Epoch 8/10, Errors: 4
Epoch 9/10, Errors: 4
Epoch 10/10, Errors: 4
```
### ì˜ˆì¸¡ ê²°ê³¼
```
XOR Gate Test:
Input: [0 0], Predicted Output: 1
Input: [0 1], Predicted Output: 1
Input: [1 0], Predicted Output: 0
Input: [1 1], Predicted Output: 0
```
### ê²½ê³„ ê²°ì • ì‹œê°í™”
```python
from matplotlib.colors import ListedColormap

def plot_decision_boundary(X, y, model):
    cmap_light = ListedColormap(['#FFAAAA', '#AAAAFF'])
    cmap_bold = ListedColormap(['#FF0000', '#0000FF'])

    h = .02  # mesh grid ê°„ê²©
    x_min, x_max = X[:, 0].min() - 1, X[:, 0].max() + 1
    y_min, y_max = X[:, 1].min() - 1, X[:, 1].max() + 1
    xx, yy = np.meshgrid(np.arange(x_min, x_max, h),
                         np.arange(y_min, y_max, h))

    Z = model.predict(np.c_[xx.ravel(), yy.ravel()])
    Z = Z.reshape(xx.shape)

    plt.figure(figsize=(8, 6))
    plt.contourf(xx, yy, Z, cmap=cmap_light)

    # ì‹¤ì œ ë°ì´í„° í¬ì¸íŠ¸ í‘œì‹œ
    plt.scatter(X[:, 0], X[:, 1], c=y, cmap=cmap_bold,
                edgecolor='k', s=100, marker='o')
    plt.xlabel('Input 1')
    plt.ylabel('Input 2')
    plt.title('Perceptron Decision Boundary')
    plt.show()

# XOR ê²Œì´íŠ¸ ê²°ì • ê²½ê³„ ì‹œê°í™”
plot_decision_boundary(X_xor, y_xor, ppn_xor)
```

### ê²½ê³„ ê²°ì • ì‹œê°í™” ê²°ê³¼
![alt text](<../../../assets/img/ARM/AI/image copy 19.png>)

### ì˜¤ë¥˜ ì‹œê°í™”
```python
#ì˜¤ë¥˜ ì‹œê°í™”
plt.figure(figsize=(8, 5))
plt.plot(range(1, len(ppn_xor.errors) + 1), ppn_xor.errors, marker='o')
plt.xlabel('Epochs')
plt.ylabel('Number of Errors')
plt.title('Perceptron Learning Error Over Epochs (XOR Gate)')
plt.grid(True)
plt.show()
```
### ì˜¤ë¥˜ ì‹œê°í™” ê²°ê³¼
![alt text](<../../../assets/img/ARM/AI/image copy 20.png>)

---

### ê³ ì°° -> XORì˜ Error ì´ìœ 

> AND, OR, NAND gate ë“¤ì€ ì„ í˜• ë¶„ë¦¬ ê°€ëŠ¥. (Z=wx+b)â€‹
>
>ì¦‰, ë°ì´í„° í¬ì¸íŠ¸ë¥¼ ë‘ ê·¸ë£¹ìœ¼ë¡œ ë‚˜ëˆŒ ìˆ˜ ìˆìŒ. â€‹
>
>ê·¸ëŸ¬ë‚˜, XORì˜ ê²½ìš° ë°ì´í„°ë¥¼ ë‘ ê·¸ë£¹ìœ¼ë¡œ ë¶„ë¦¬ê°€ ë¶ˆê°€ëŠ¥.â€‹
>
>![alt text](<../../../assets/img/ARM/AI/image copy 27.png>)
>
>ì„ í˜• ë¶„ë¦¬ ë¶ˆê°€ëŠ¥(linearly inseparable)
>
>XORì˜ ì…ë ¥ ê°’ì€ [0,0],[0,1],[1,0],[1,1]
>
>XORì˜ ì¶œë ¥ ê°’ì€ [0, 1, 1, 0] ì— í•´ë‹¹ë©ë‹ˆë‹¤
>
>y=1 classëŠ” [0,1], [1,0]
>
>y=0 classëŠ” [0,0], [1,1]
>
>ë”°ë¼ì„œ í´ë˜ìŠ¤ 0ê³¼ í´ë˜ìŠ¤ 1ì€ Xìë¡œ êµì°¨ëœë‹¤.
>
>ë”°ë¼ì„œ í•œ ê°œì˜ ì§ì„ ìœ¼ë¡œëŠ” ì´ ë‘˜ì„ ë‚˜ëˆŒ ìˆ˜ ì—†ë‹¤.
>
>ê²°êµ­ ì„ í˜• ê²°ì • ê²½ê³„ë¡œ ë§Œë“œëŠ” í¼ì…‰íŠ¸ë¡ ì€ í•˜ë‚˜ì˜ ì§ì„ ìœ¼ë¡œ ë¶„ë¥˜í•  ìˆ˜ ì—†ê¸°ì— ì˜¤ë¥˜ê°€ ë‚œ ê²ƒì´ë‹¤.
>
>ì´ë¥¼ í•´ê²°í•˜ê¸° ìœ„í•´ì„œëŠ” ë¹„ì„ í˜• ê²°ì • ê²½ê³„ê°€ í•„ìš”í•˜ë‹¤. 

---

### MLP (Multi Layer Perceptron)â€‹

>![alt text](<../../../assets/img/ARM/AI/ìŠ¤í¬ë¦°ìƒ· 2025-06-26 09-00-23.png>)
>
>hidden Layerë¥¼ í†µí•´ ì…ì¶œë ¥ ì‚¬ì´ì˜ ë³µì¡í•œ íŒ¨í„´ì„ ì¶”ì¶œí•˜ê³  í•™ìŠµí•œë‹¤.

>
>![alt text](<../../../assets/img/ARM/AI/ìŠ¤í¬ë¦°ìƒ· 2025-06-26 09-00-37.png>)
>
>ìˆœì „íŒŒ : ì…ë ¥ ë°ì´í„°ì— ê°€ì¤‘í•© ê³„ì‚°ì„ ì ìš©í•˜ì—¬ í™œì„±í™” í•¨ìˆ˜ë¥¼ í†µê³¼ì‹œì¼œ ìµœì¢… ì¶œë ¥ì„ ìƒì„±.â€‹
>
>ì—­ì „íŒŒ : ì¶œë ¥ì—ì„œ ë°œìƒí•œ ì˜¤ì°¨ë¥¼ ë°”íƒ•ìœ¼ë¡œ ê°€ì¤‘ì¹˜ì™€ í¸í–¥ì„ ê°±ì‹ . (ì˜¤ì°¨ ìµœì†Œí™”)â€‹

>![alt text](<../../../assets/img/ARM/AI/ìŠ¤í¬ë¦°ìƒ· 2025-06-26 09-00-50.png>)
>
>ë¹„ì„ í˜• ì´ì§„ ë¶„ë¥˜ë¥¼ ìœ„í•œ í™œì„±í™” í•¨ìˆ˜ ì¶œë ¥ ê°’ì„ í™•ë¥ ë¡œ íŒë‹¨í•˜ì—¬ ì •í•¨.

### XOR_MLP -> í•´ê²° ë°©ì•ˆ

```python
import numpy as np
import matplotlib.pyplot as plt
from sklearn.neural_network import MLPClassifier
from matplotlib.colors import ListedColormap

# 1. XOR ë°ì´í„°
X = np.array([[0,0], [0,1], [1,0], [1,1]])
y = np.array([0, 1, 1, 0])

# 2. MLP ëª¨ë¸ ì •ì˜ (ë¹„ì„ í˜• í•´ê²° ê°€ëŠ¥)
mlp = MLPClassifier(hidden_layer_sizes=(2,),   # ì€ë‹‰ì¸µ 1ê°œ, ë…¸ë“œ 2ê°œ
                    activation='tanh',         # ë¹„ì„ í˜• í™œì„±í™” í•¨ìˆ˜
                    solver='adam',
                    learning_rate_init=0.1,
                    max_iter=1000,
                    random_state=42)

# 3. í›ˆë ¨
mlp.fit(X, y)

# 4. í•™ìŠµ ë¡œê·¸ ì¶œë ¥
print("í•™ìŠµ ë¡œê·¸")
for i, loss in enumerate(mlp.loss_curve_):
    print(f"Epoch {i+1}/{len(mlp.loss_curve_)}, Loss: {loss:.4f}")

# 5. ì˜ˆì¸¡ ê²°ê³¼ ì¶œë ¥
print("\nì˜ˆì¸¡ ê²°ê³¼")
print("XOR Gate Test:")
for x in X:
    pred = mlp.predict([x])[0]
    print(f"Input: {x}, Predicted Output: {pred}")
```

### í•™ìŠµ ë¡œê·¸ (Layer = 1, Node = 2)
```
í•™ìŠµ ë¡œê·¸
Epoch 1/202, Loss: 0.8554
Epoch 2/202, Loss: 0.7697
Epoch 3/202, Loss: 0.7273
Epoch 4/202, Loss: 0.7169
Epoch 5/202, Loss: 0.7138
Epoch 6/202, Loss: 0.7074
Epoch 7/202, Loss: 0.6985
Epoch 8/202, Loss: 0.6910
Epoch 9/202, Loss: 0.6874
Epoch 10/202, Loss: 0.6880
Epoch 11/202, Loss: 0.6903
Epoch 12/202, Loss: 0.6921
Epoch 13/202, Loss: 0.6921
Epoch 14/202, Loss: 0.6908
Epoch 15/202, Loss: 0.6889
Epoch 16/202, Loss: 0.6874
Epoch 17/202, Loss: 0.6867
Epoch 18/202, Loss: 0.6867
Epoch 19/202, Loss: 0.6868
Epoch 20/202, Loss: 0.6865
Epoch 21/202, Loss: 0.6853
Epoch 22/202, Loss: 0.6829
Epoch 23/202, Loss: 0.6795
Epoch 24/202, Loss: 0.6755
Epoch 25/202, Loss: 0.6708
Epoch 26/202, Loss: 0.6656
Epoch 27/202, Loss: 0.6596
Epoch 28/202, Loss: 0.6525
Epoch 29/202, Loss: 0.6446
Epoch 30/202, Loss: 0.6360
Epoch 31/202, Loss: 0.6272
Epoch 32/202, Loss: 0.6185
Epoch 33/202, Loss: 0.6096
Epoch 34/202, Loss: 0.6004
Epoch 35/202, Loss: 0.5907
Epoch 36/202, Loss: 0.5808
Epoch 37/202, Loss: 0.5706
Epoch 38/202, Loss: 0.5601
Epoch 39/202, Loss: 0.5490
Epoch 40/202, Loss: 0.5374
Epoch 41/202, Loss: 0.5255
Epoch 42/202, Loss: 0.5139
Epoch 43/202, Loss: 0.5030
Epoch 44/202, Loss: 0.4932
Epoch 45/202, Loss: 0.4841
Epoch 46/202, Loss: 0.4753
Epoch 47/202, Loss: 0.4664
Epoch 48/202, Loss: 0.4573
Epoch 49/202, Loss: 0.4485
Epoch 50/202, Loss: 0.4405
Epoch 51/202, Loss: 0.4337
Epoch 52/202, Loss: 0.4277
Epoch 53/202, Loss: 0.4222
Epoch 54/202, Loss: 0.4168
Epoch 55/202, Loss: 0.4115
Epoch 56/202, Loss: 0.4066
Epoch 57/202, Loss: 0.4023
Epoch 58/202, Loss: 0.3985
Epoch 59/202, Loss: 0.3952
Epoch 60/202, Loss: 0.3922
Epoch 61/202, Loss: 0.3894
Epoch 62/202, Loss: 0.3868
Epoch 63/202, Loss: 0.3844
Epoch 64/202, Loss: 0.3823
Epoch 65/202, Loss: 0.3804
Epoch 66/202, Loss: 0.3787
Epoch 67/202, Loss: 0.3771
Epoch 68/202, Loss: 0.3757
Epoch 69/202, Loss: 0.3743
Epoch 70/202, Loss: 0.3730
Epoch 71/202, Loss: 0.3718
Epoch 72/202, Loss: 0.3707
Epoch 73/202, Loss: 0.3697
Epoch 74/202, Loss: 0.3689
Epoch 75/202, Loss: 0.3681
Epoch 76/202, Loss: 0.3673
Epoch 77/202, Loss: 0.3665
Epoch 78/202, Loss: 0.3658
Epoch 79/202, Loss: 0.3652
Epoch 80/202, Loss: 0.3645
Epoch 81/202, Loss: 0.3640
Epoch 82/202, Loss: 0.3635
Epoch 83/202, Loss: 0.3630
Epoch 84/202, Loss: 0.3626
Epoch 85/202, Loss: 0.3621
Epoch 86/202, Loss: 0.3617
Epoch 87/202, Loss: 0.3613
Epoch 88/202, Loss: 0.3609
Epoch 89/202, Loss: 0.3606
Epoch 90/202, Loss: 0.3602
Epoch 91/202, Loss: 0.3599
Epoch 92/202, Loss: 0.3596
Epoch 93/202, Loss: 0.3593
Epoch 94/202, Loss: 0.3590
Epoch 95/202, Loss: 0.3588
Epoch 96/202, Loss: 0.3585
Epoch 97/202, Loss: 0.3582
Epoch 98/202, Loss: 0.3580
Epoch 99/202, Loss: 0.3578
Epoch 100/202, Loss: 0.3575
Epoch 101/202, Loss: 0.3573
Epoch 102/202, Loss: 0.3571
Epoch 103/202, Loss: 0.3569
Epoch 104/202, Loss: 0.3567
Epoch 105/202, Loss: 0.3564
Epoch 106/202, Loss: 0.3562
Epoch 107/202, Loss: 0.3560
Epoch 108/202, Loss: 0.3558
Epoch 109/202, Loss: 0.3556
Epoch 110/202, Loss: 0.3554
Epoch 111/202, Loss: 0.3552
Epoch 112/202, Loss: 0.3550
Epoch 113/202, Loss: 0.3547
Epoch 114/202, Loss: 0.3545
Epoch 115/202, Loss: 0.3543
Epoch 116/202, Loss: 0.3540
Epoch 117/202, Loss: 0.3538
Epoch 118/202, Loss: 0.3535
Epoch 119/202, Loss: 0.3532
Epoch 120/202, Loss: 0.3529
Epoch 121/202, Loss: 0.3526
Epoch 122/202, Loss: 0.3522
Epoch 123/202, Loss: 0.3518
Epoch 124/202, Loss: 0.3514
Epoch 125/202, Loss: 0.3509
Epoch 126/202, Loss: 0.3503
Epoch 127/202, Loss: 0.3497
Epoch 128/202, Loss: 0.3490
Epoch 129/202, Loss: 0.3481
Epoch 130/202, Loss: 0.3471
Epoch 131/202, Loss: 0.3459
Epoch 132/202, Loss: 0.3445
Epoch 133/202, Loss: 0.3427
Epoch 134/202, Loss: 0.3404
Epoch 135/202, Loss: 0.3374
Epoch 136/202, Loss: 0.3336
Epoch 137/202, Loss: 0.3284
Epoch 138/202, Loss: 0.3212
Epoch 139/202, Loss: 0.3113
Epoch 140/202, Loss: 0.2973
Epoch 141/202, Loss: 0.2778
Epoch 142/202, Loss: 0.2516
Epoch 143/202, Loss: 0.2190
Epoch 144/202, Loss: 0.1827
Epoch 145/202, Loss: 0.1477
Epoch 146/202, Loss: 0.1179
Epoch 147/202, Loss: 0.0938
Epoch 148/202, Loss: 0.0750
Epoch 149/202, Loss: 0.0608
Epoch 150/202, Loss: 0.0507
Epoch 151/202, Loss: 0.0439
Epoch 152/202, Loss: 0.0394
Epoch 153/202, Loss: 0.0365
Epoch 154/202, Loss: 0.0344
Epoch 155/202, Loss: 0.0327
Epoch 156/202, Loss: 0.0312
Epoch 157/202, Loss: 0.0296
Epoch 158/202, Loss: 0.0278
Epoch 159/202, Loss: 0.0258
Epoch 160/202, Loss: 0.0239
Epoch 161/202, Loss: 0.0220
Epoch 162/202, Loss: 0.0203
Epoch 163/202, Loss: 0.0188
Epoch 164/202, Loss: 0.0174
Epoch 165/202, Loss: 0.0163
Epoch 166/202, Loss: 0.0153
Epoch 167/202, Loss: 0.0145
Epoch 168/202, Loss: 0.0138
Epoch 169/202, Loss: 0.0131
Epoch 170/202, Loss: 0.0126
Epoch 171/202, Loss: 0.0122
Epoch 172/202, Loss: 0.0118
Epoch 173/202, Loss: 0.0114
Epoch 174/202, Loss: 0.0111
Epoch 175/202, Loss: 0.0108
Epoch 176/202, Loss: 0.0106
Epoch 177/202, Loss: 0.0104
Epoch 178/202, Loss: 0.0101
Epoch 179/202, Loss: 0.0100
Epoch 180/202, Loss: 0.0098
Epoch 181/202, Loss: 0.0096
Epoch 182/202, Loss: 0.0094
Epoch 183/202, Loss: 0.0093
Epoch 184/202, Loss: 0.0091
Epoch 185/202, Loss: 0.0090
Epoch 186/202, Loss: 0.0088
Epoch 187/202, Loss: 0.0087
Epoch 188/202, Loss: 0.0086
Epoch 189/202, Loss: 0.0085
Epoch 190/202, Loss: 0.0084
Epoch 191/202, Loss: 0.0083
Epoch 192/202, Loss: 0.0082
Epoch 193/202, Loss: 0.0081
Epoch 194/202, Loss: 0.0080
Epoch 195/202, Loss: 0.0079
Epoch 196/202, Loss: 0.0078
Epoch 197/202, Loss: 0.0077
Epoch 198/202, Loss: 0.0077
Epoch 199/202, Loss: 0.0076
Epoch 200/202, Loss: 0.0075
Epoch 201/202, Loss: 0.0074
Epoch 202/202, Loss: 0.0074
```
### í•™ìŠµ ë¡œê·¸ (Layer = 1, Node = 4)
```
í•™ìŠµ ë¡œê·¸
Epoch 1/117, Loss: 0.8091
Epoch 2/117, Loss: 0.7159
Epoch 3/117, Loss: 0.6959
Epoch 4/117, Loss: 0.7019
Epoch 5/117, Loss: 0.6989
Epoch 6/117, Loss: 0.6821
Epoch 7/117, Loss: 0.6606
Epoch 8/117, Loss: 0.6430
Epoch 9/117, Loss: 0.6324
Epoch 10/117, Loss: 0.6254
Epoch 11/117, Loss: 0.6160
Epoch 12/117, Loss: 0.6006
Epoch 13/117, Loss: 0.5795
Epoch 14/117, Loss: 0.5553
Epoch 15/117, Loss: 0.5312
Epoch 16/117, Loss: 0.5086
Epoch 17/117, Loss: 0.4866
Epoch 18/117, Loss: 0.4630
Epoch 19/117, Loss: 0.4368
Epoch 20/117, Loss: 0.4083
Epoch 21/117, Loss: 0.3786
Epoch 22/117, Loss: 0.3493
Epoch 23/117, Loss: 0.3213
Epoch 24/117, Loss: 0.2950
Epoch 25/117, Loss: 0.2702
Epoch 26/117, Loss: 0.2464
Epoch 27/117, Loss: 0.2235
Epoch 28/117, Loss: 0.2019
Epoch 29/117, Loss: 0.1818
Epoch 30/117, Loss: 0.1635
Epoch 31/117, Loss: 0.1472
Epoch 32/117, Loss: 0.1328
Epoch 33/117, Loss: 0.1200
Epoch 34/117, Loss: 0.1086
Epoch 35/117, Loss: 0.0983
Epoch 36/117, Loss: 0.0891
Epoch 37/117, Loss: 0.0808
Epoch 38/117, Loss: 0.0734
Epoch 39/117, Loss: 0.0667
Epoch 40/117, Loss: 0.0609
Epoch 41/117, Loss: 0.0558
Epoch 42/117, Loss: 0.0513
Epoch 43/117, Loss: 0.0474
Epoch 44/117, Loss: 0.0440
Epoch 45/117, Loss: 0.0411
Epoch 46/117, Loss: 0.0384
Epoch 47/117, Loss: 0.0361
Epoch 48/117, Loss: 0.0340
Epoch 49/117, Loss: 0.0322
Epoch 50/117, Loss: 0.0305
Epoch 51/117, Loss: 0.0290
Epoch 52/117, Loss: 0.0276
Epoch 53/117, Loss: 0.0263
Epoch 54/117, Loss: 0.0252
Epoch 55/117, Loss: 0.0241
Epoch 56/117, Loss: 0.0231
Epoch 57/117, Loss: 0.0222
Epoch 58/117, Loss: 0.0214
Epoch 59/117, Loss: 0.0206
Epoch 60/117, Loss: 0.0198
Epoch 61/117, Loss: 0.0191
Epoch 62/117, Loss: 0.0185
Epoch 63/117, Loss: 0.0178
Epoch 64/117, Loss: 0.0172
Epoch 65/117, Loss: 0.0167
Epoch 66/117, Loss: 0.0162
Epoch 67/117, Loss: 0.0157
Epoch 68/117, Loss: 0.0152
Epoch 69/117, Loss: 0.0147
Epoch 70/117, Loss: 0.0143
Epoch 71/117, Loss: 0.0139
Epoch 72/117, Loss: 0.0136
Epoch 73/117, Loss: 0.0132
Epoch 74/117, Loss: 0.0129
Epoch 75/117, Loss: 0.0126
Epoch 76/117, Loss: 0.0122
Epoch 77/117, Loss: 0.0120
Epoch 78/117, Loss: 0.0117
Epoch 79/117, Loss: 0.0114
Epoch 80/117, Loss: 0.0111
Epoch 81/117, Loss: 0.0109
Epoch 82/117, Loss: 0.0106
Epoch 83/117, Loss: 0.0104
Epoch 84/117, Loss: 0.0102
Epoch 85/117, Loss: 0.0100
Epoch 86/117, Loss: 0.0098
Epoch 87/117, Loss: 0.0096
Epoch 88/117, Loss: 0.0094
Epoch 89/117, Loss: 0.0092
Epoch 90/117, Loss: 0.0090
Epoch 91/117, Loss: 0.0089
Epoch 92/117, Loss: 0.0087
Epoch 93/117, Loss: 0.0086
Epoch 94/117, Loss: 0.0084
Epoch 95/117, Loss: 0.0083
Epoch 96/117, Loss: 0.0081
Epoch 97/117, Loss: 0.0080
Epoch 98/117, Loss: 0.0078
Epoch 99/117, Loss: 0.0077
Epoch 100/117, Loss: 0.0076
Epoch 101/117, Loss: 0.0075
Epoch 102/117, Loss: 0.0074
Epoch 103/117, Loss: 0.0072
Epoch 104/117, Loss: 0.0071
Epoch 105/117, Loss: 0.0070
Epoch 106/117, Loss: 0.0069
Epoch 107/117, Loss: 0.0068
Epoch 108/117, Loss: 0.0067
Epoch 109/117, Loss: 0.0066
Epoch 110/117, Loss: 0.0066
Epoch 111/117, Loss: 0.0065
Epoch 112/117, Loss: 0.0064
Epoch 113/117, Loss: 0.0063
Epoch 114/117, Loss: 0.0062
Epoch 115/117, Loss: 0.0062
Epoch 116/117, Loss: 0.0061
Epoch 117/117, Loss: 0.0060

```

### í•™ìŠµ ë¡œê·¸ (Layer = 4, Node = 2)
```
í•™ìŠµ ë¡œê·¸
Epoch 1/88, Loss: 0.7066
Epoch 2/88, Loss: 0.6742
Epoch 3/88, Loss: 0.6643
Epoch 4/88, Loss: 0.6517
Epoch 5/88, Loss: 0.6237
Epoch 6/88, Loss: 0.5871
Epoch 7/88, Loss: 0.5514
Epoch 8/88, Loss: 0.5164
Epoch 9/88, Loss: 0.4758
Epoch 10/88, Loss: 0.4329
Epoch 11/88, Loss: 0.3933
Epoch 12/88, Loss: 0.3550
Epoch 13/88, Loss: 0.3154
Epoch 14/88, Loss: 0.2773
Epoch 15/88, Loss: 0.2432
Epoch 16/88, Loss: 0.2129
Epoch 17/88, Loss: 0.1851
Epoch 18/88, Loss: 0.1601
Epoch 19/88, Loss: 0.1383
Epoch 20/88, Loss: 0.1197
Epoch 21/88, Loss: 0.1039
Epoch 22/88, Loss: 0.0905
Epoch 23/88, Loss: 0.0791
Epoch 24/88, Loss: 0.0696
Epoch 25/88, Loss: 0.0618
Epoch 26/88, Loss: 0.0552
Epoch 27/88, Loss: 0.0497
Epoch 28/88, Loss: 0.0450
Epoch 29/88, Loss: 0.0409
Epoch 30/88, Loss: 0.0373
Epoch 31/88, Loss: 0.0342
Epoch 32/88, Loss: 0.0315
Epoch 33/88, Loss: 0.0291
Epoch 34/88, Loss: 0.0271
Epoch 35/88, Loss: 0.0252
Epoch 36/88, Loss: 0.0236
Epoch 37/88, Loss: 0.0222
Epoch 38/88, Loss: 0.0209
Epoch 39/88, Loss: 0.0198
Epoch 40/88, Loss: 0.0188
Epoch 41/88, Loss: 0.0179
Epoch 42/88, Loss: 0.0170
Epoch 43/88, Loss: 0.0163
Epoch 44/88, Loss: 0.0156
Epoch 45/88, Loss: 0.0149
Epoch 46/88, Loss: 0.0144
Epoch 47/88, Loss: 0.0138
Epoch 48/88, Loss: 0.0134
Epoch 49/88, Loss: 0.0129
Epoch 50/88, Loss: 0.0125
Epoch 51/88, Loss: 0.0121
Epoch 52/88, Loss: 0.0118
Epoch 53/88, Loss: 0.0115
Epoch 54/88, Loss: 0.0111
Epoch 55/88, Loss: 0.0109
Epoch 56/88, Loss: 0.0106
Epoch 57/88, Loss: 0.0104
Epoch 58/88, Loss: 0.0101
Epoch 59/88, Loss: 0.0099
Epoch 60/88, Loss: 0.0097
Epoch 61/88, Loss: 0.0095
Epoch 62/88, Loss: 0.0093
Epoch 63/88, Loss: 0.0091
Epoch 64/88, Loss: 0.0090
Epoch 65/88, Loss: 0.0088
Epoch 66/88, Loss: 0.0087
Epoch 67/88, Loss: 0.0085
Epoch 68/88, Loss: 0.0084
Epoch 69/88, Loss: 0.0082
Epoch 70/88, Loss: 0.0081
Epoch 71/88, Loss: 0.0080
Epoch 72/88, Loss: 0.0079
Epoch 73/88, Loss: 0.0078
Epoch 74/88, Loss: 0.0076
Epoch 75/88, Loss: 0.0075
Epoch 76/88, Loss: 0.0074
Epoch 77/88, Loss: 0.0073
Epoch 78/88, Loss: 0.0072
Epoch 79/88, Loss: 0.0071
Epoch 80/88, Loss: 0.0070
Epoch 81/88, Loss: 0.0069
Epoch 82/88, Loss: 0.0069
Epoch 83/88, Loss: 0.0068
Epoch 84/88, Loss: 0.0067
Epoch 85/88, Loss: 0.0066
Epoch 86/88, Loss: 0.0065
Epoch 87/88, Loss: 0.0064
Epoch 88/88, Loss: 0.0063
```

### Epoch ìˆ˜ê°€ ë‹¬ë¼ì§€ëŠ” ì´ìœ ?

- `MLPClassifier`ëŠ” lossê°€ ìˆ˜ë ´í•˜ë©´ **ì¡°ê¸°ì— í•™ìŠµì„ ì¢…ë£Œ**í•œë‹¤.
- **ë…¸ë“œ ìˆ˜ ë˜ëŠ” ì¸µ ìˆ˜ê°€ ë§ì•„ì§€ë©´ í‘œí˜„ë ¥ì´ ë†’ì•„ì ¸ ë” ë¹ ë¥´ê²Œ ìˆ˜ë ´**í•œë‹¤.
- ë°˜ëŒ€ë¡œ, ë…¸ë“œ ìˆ˜ë‚˜ ì¸µ ìˆ˜ê°€ ì ìœ¼ë©´ í‘œí˜„ë ¥ì´ ë‚®ì•„ **ë” ë§ì€ epoch**ì´ í•„ìš”í•˜ë‹¤.
- ë”°ë¼ì„œ ë™ì¼í•œ `max_iter`ë¥¼ ì£¼ë”ë¼ë„ ì‹¤ì œ í•™ìŠµ ë°˜ë³µ ìˆ˜ëŠ” **ëª¨ë¸ êµ¬ì¡°ì— ë”°ë¼ ë‹¬ë¼ì§ˆ ìˆ˜ ìˆë‹¤**.

### ì˜ˆì¸¡ ê²°ê³¼
```
ì˜ˆì¸¡ ê²°ê³¼
XOR Gate Test:
Input: [0 0], Predicted Output: 0
Input: [0 1], Predicted Output: 1
Input: [1 0], Predicted Output: 1
Input: [1 1], Predicted Output: 0
```
### ê²½ê³„ ê²°ì • ì‹œê°í™”
```python
# 6. ê²°ì • ê²½ê³„ ì‹œê°í™” í•¨ìˆ˜
def plot_decision_boundary_proba(X, y, model):
    cmap_light = ListedColormap(['#FFBBBB', '#BBBBFF'])
    cmap_bold = ListedColormap(['#FF0000', '#0000FF'])

    h = .01
    x_min, x_max = X[:, 0].min() - 0.5, X[:, 0].max() + 0.5
    y_min, y_max = X[:, 1].min() - 0.5, X[:, 1].max() + 0.5
    xx, yy = np.meshgrid(np.arange(x_min, x_max, h),
                         np.arange(y_min, y_max, h))

    Z_proba = model.predict_proba(np.c_[xx.ravel(), yy.ravel()])[:, 1]
    Z = Z_proba.reshape(xx.shape)

    # ì‹œê°í™”
    plt.figure(figsize=(8, 6))
    cs = plt.contourf(xx, yy, Z, levels=np.linspace(0, 1, 100), cmap=cmap_light, alpha=0.9)
    plt.colorbar(cs, label='Probability of Class 1')

    # ê²°ì • ê²½ê³„ (0.5 ê¸°ì¤€)
    plt.contour(xx, yy, Z, levels=[0.5], colors='black', linewidths=2)

    # ë°ì´í„° ì  í‘œì‹œ
    plt.scatter(X[:, 0], X[:, 1], c=y, cmap=cmap_bold, edgecolor='k', s=100)
    plt.xlabel("Input 1")
    plt.ylabel("Input 2")
    plt.title("MLP Decision Boundary (XOR Gate)")
    plt.grid(True)
    plt.show()

# 7. ê²°ì • ê²½ê³„ ì‹œê°í™”
plot_decision_boundary_proba(X, y, mlp)
```
### ê²½ê³„ ê²°ì • ì‹œê°í™” ê²°ê³¼
> Layer = 1, Node = 2
>![alt text](<../../../assets/img/ARM/AI/image copy 29.png>)

> Layer = 1, Node = 4
>![alt text](<../../../assets/img/ARM/AI/image copy 22.png>)

> Layer = 4, Node = 2
>![alt text](<../../../assets/img/ARM/AI/image copy 30.png>)

### ì˜¤ë¥˜ ì‹œê°í™”
```python
# 8. ì†ì‹¤ ê³¡ì„  ì‹œê°í™”
plt.figure(figsize=(8, 5))
plt.plot(mlp.loss_curve_, marker='o')
plt.xlabel("Epochs")
plt.ylabel("Loss")
plt.title("MLP Loss Curve (XOR Gate)")
plt.grid(True)
plt.show()
```
### ì˜¤ë¥˜ ì‹œê°í™” ê²°ê³¼
>**Layer = 1, Node = 2**
>![alt text](<../../../assets/img/ARM/AI/image copy 28.png>)

>**Layer = 1, Node = 4**
>![alt text](<../../../assets/img/ARM/AI/image copy 23.png>)

>**Layer = 4, Node = 2**
>
>![alt text](<../../../assets/img/ARM/AI/image copy 31.png>)

---

# âœ… XOR ë¬¸ì œ í•´ê²°ì„ ìœ„í•œ MLP êµ¬ì¡° ë¶„ì„

## ğŸ” ì‹¤í—˜ ëª©ì 
XOR ë¬¸ì œëŠ” ì„ í˜• ë¶„ë¥˜ê¸°ë¡œ í•´ê²°í•  ìˆ˜ ì—†ëŠ” ëŒ€í‘œì ì¸ ë¹„ì„ í˜• ë¬¸ì œì…ë‹ˆë‹¤.  
MLP(Multi-Layer Perceptron)ì˜ ì€ë‹‰ì¸µ ìˆ˜ì™€ ë…¸ë“œ ìˆ˜ì— ë”°ë¼ ì–´ë–»ê²Œ ì„±ëŠ¥ì´ ë‹¬ë¼ì§€ëŠ”ì§€ ì‹¤í—˜ì„ í†µí•´ ë¹„êµ ë¶„ì„í•©ë‹ˆë‹¤.

---

## 1. ê²°ì • ê²½ê³„ ì‹œê°í™” (Decision Boundary)

### ğŸ”¹ Layer = 1, Node = 2
- ê²°ì • ê²½ê³„ê°€ ì§ì„ ì— ê°€ê¹Œì›Œ ë‹¨ìˆœí•¨
- XOR ë¬¸ì œë¥¼ ì™„ì „íˆ ë¶„ë¦¬í•˜ê¸´ ì–´ë ¤ì›€
- ì¤‘ê°„ì— ì˜¤ë¶„ë¥˜ ê°€ëŠ¥ì„± ìˆìŒ

### ğŸ”¸ Layer = 1, Node = 4
- ê³¡ì„  í˜•íƒœì˜ ê²°ì • ê²½ê³„ë¥¼ í˜•ì„±
- ì…ë ¥ ê³µê°„ì„ ë” ì„¸ë°€í•˜ê²Œ ë¶„í• 
- XOR ì •ë‹µ íŒ¨í„´ì„ ë” ì˜ í•™ìŠµí•¨

### ğŸ”¶ Layer = 4, Node = 2
- ì¸µì€ ê¹Šì§€ë§Œ ê²°ì • ê²½ê³„ê°€ ì„ í˜•ì— ê°€ê¹Œì›€
- ë³µì¡ë„ ëŒ€ë¹„ ê²°ì • ê²½ê³„ í’ˆì§ˆì€ ë–¨ì–´ì§ˆ ìˆ˜ ìˆìŒ
- ì¸µì´ ë§ë‹¤ê³  í•­ìƒ ì„±ëŠ¥ì´ ì¢‹ì€ ê±´ ì•„ë‹˜

---

## 2. Loss ê³¡ì„  ë¹„êµ (Loss Curve)

| êµ¬ì„±                 | ìˆ˜ë ´ ì†ë„           | íŠ¹ì§• ìš”ì•½                          |
|----------------------|---------------------|------------------------------------|
| Layer = 1, Node = 2  | ëŠë¦¼ (150 epoch ì´ìƒ) | ì¤‘ê°„ plateau, ìˆ˜ë ´ì€ í•˜ë‚˜ ë”ë”¤     |
| Layer = 1, Node = 4  | ë¹ ë¦„ (40 epoch ì „í›„) | ì´ˆë°˜ë¶€í„° ë¹ ë¥´ê²Œ ê°ì†Œ, ì•ˆì •ì        |
| Layer = 4, Node = 2  | ë§¤ìš° ë¹ ë¦„ (20 epoch ë‚´ì™¸) | ë¹ ë¥´ê²Œ ìˆ˜ë ´í•˜ì§€ë§Œ í’ˆì§ˆì€ ì œí•œì  |

---

## 3. ì¢…í•© ë¹„êµ ìš”ì•½

| êµ¬ì„±                 | ê²°ì • ê²½ê³„                    | í•™ìŠµ ì†ë„          | í•™ìŠµ ì•ˆì •ì„±        |
|----------------------|------------------------------|---------------------|---------------------|
| Layer = 1, Node = 2  | ë‹¨ìˆœ, í‘œí˜„ë ¥ ë¶€ì¡±             | ëŠë¦¼                | ë³´í†µ                |
| Layer = 1, Node = 4  | ì •êµí•˜ê³  ê³¡ì„ í˜•               | ë¹ ë¦„                | ë§¤ìš° ì•ˆì •ì          |
| Layer = 4, Node = 2  | ë³µì¡í•˜ë‚˜ íš¨ê³¼ëŠ” ì œí•œì         | ë§¤ìš° ë¹ ë¦„           | ë¶ˆì•ˆì • ê°€ëŠ¥ì„± ìˆìŒ   |

---

## âœ… ìµœì¢… ê²°ë¡ 

- XOR ë¬¸ì œëŠ” **ì€ë‹‰ì¸µ 1ê°œ**ë§Œìœ¼ë¡œë„ ì¶©ë¶„íˆ í•´ê²° ê°€ëŠ¥
- **ë…¸ë“œ ìˆ˜ë¥¼ ëŠ˜ë¦¬ë©´** ê²°ì • ê²½ê³„ê°€ ì •êµí•´ì§€ê³  í•™ìŠµ ì•ˆì •ì„±ë„ í–¥ìƒ
- **ì¸µì„ ê¹Šê²Œ ìŒ“ëŠ” ê²ƒë§Œìœ¼ë¡œëŠ”** ì¢‹ì€ ê²°ì • ê²½ê³„ë¥¼ ë§Œë“¤ ìˆ˜ ì—†ìŒ
- **ì ì ˆí•œ ì¸µ/ë…¸ë“œ ì¡°í•©ê³¼ ê³¼ì í•© ë°©ì§€ë¥¼ ìœ„í•œ ì •ê·œí™”/ê²€ì¦**ì´ ì¤‘ìš”í•¨

> ğŸ’¡ **XOR ë¬¸ì œëŠ” ê¹Šì´ë³´ë‹¤ëŠ” ì ì ˆí•œ ë„ˆë¹„(ë…¸ë“œ ìˆ˜)ê°€ í•µì‹¬!**

---

# ğŸ“˜ ì€ë‹‰ì¸µ(Hidden Layer)ê³¼ ë…¸ë“œ(Node) ê°œë… ì •ë¦¬

## ê¸°ë³¸ ê°œë…

| ìš©ì–´       | ì„¤ëª… |
|------------|------|
| **ì€ë‹‰ì¸µ** | ì…ë ¥ì¸µê³¼ ì¶œë ¥ì¸µ ì‚¬ì´ì˜ ê³„ì‚°ì¸µ (ì¤‘ê°„ ì²˜ë¦¬ ë‹¨ê³„) |
| **ë…¸ë“œ**   | ì€ë‹‰ì¸µ ë‚´ë¶€ì˜ ë‰´ëŸ°, ì…ë ¥ì„ ë°›ì•„ ë¹„ì„ í˜• ì¶œë ¥ ìƒì„± |

---

## ì—­í•  ë° ì°¨ì´ì 

| í•­ëª©       | ì€ë‹‰ì¸µ | ë…¸ë“œ |
|------------|--------|------|
| ì˜ë¯¸        | ê¹Šì´ (Depth) | ë„ˆë¹„ (Width) |
| ì—­í•         | ê³ ì°¨ì› ì¶”ìƒí™” | ë‹¤ì–‘í•œ íŒ¨í„´ ì¸ì‹ |
| ì˜í–¥ë ¥      | ê³„ì‚° ë‹¨ê³„ë¥¼ ì—¬ëŸ¬ ë²ˆ ë‚˜ëˆ” | ê° ë‹¨ê³„ì˜ ê³„ì‚°ë ¥ì„ ë†’ì„ |
| íš¨ê³¼        | ë” ë³µì¡í•œ í•¨ìˆ˜ í•™ìŠµ ê°€ëŠ¥ | ë” í’ë¶€í•œ í‘œí˜„ ê°€ëŠ¥ |
| ì£¼ì˜ì       | ê¸°ìš¸ê¸° ì†Œì‹¤, ëŠë¦° í•™ìŠµ | ê³¼ì í•©, ê³„ì‚°ëŸ‰ ì¦ê°€ |

---

## XOR ë¬¸ì œ ê¸°ì¤€

| í•­ëª© | í•„ìš” ì¡°ê±´ |
|------|-----------|
| **ì€ë‹‰ì¸µ** | 1ê°œë©´ ì¶©ë¶„ |
| **ë…¸ë“œ ìˆ˜** | ìµœì†Œ 2ê°œ, ì•ˆì •ì  í•™ìŠµì„ ìœ„í•´ 3~4ê°œ ê¶Œì¥ |

---

## âš ï¸ ê³¼ë„í•œ ì¸µ/ë…¸ë“œ ì‚¬ìš© ì‹œ ì£¼ì˜ì 

- ì¸µì´ ê¹Šìœ¼ë©´: ê¸°ìš¸ê¸° ì†Œì‹¤, ì˜¤ë²„ìŠ¤í™ êµ¬ì¡°, í•™ìŠµ ë¶ˆì•ˆì •
- ë…¸ë“œê°€ ë„ˆë¬´ ë§ìœ¼ë©´: í•™ìŠµì€ ë¹¨ë¼ì§ˆ ìˆ˜ ìˆìœ¼ë‚˜ ê³¼ì í•© ë°œìƒ ìœ„í—˜ ì¦ê°€

---