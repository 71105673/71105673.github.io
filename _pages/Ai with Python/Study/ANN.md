---
title: "Day-5 ANN" 
date: "2025-06-27"
thumbnail: "../../../assets/img/ARM/AI/image copy 32.png"
---
# ì„ í˜• ëª¨ë¸ í•™ìŠµ

## ì„ í˜• ëª¨ë¸ í•™ìŠµ ê³¼ì •
![alt text](<../../../assets/img/ARM/AI/image copy 37.png>)

## ë²¡í„°í™” 
- ì„ í˜• ëª¨ë¸ì—ì„œëŠ” ì…ë ¥ ë°ì´í„°ëŠ” ë²¡í„° í˜•íƒœë¡œ ì •ë¦¬
- 2ì°¨ì› ë˜ëŠ” 3ì°¨ì› ì´ë¯¸ì§€ ë°ì´í„°ë¥¼ 1ì°¨ì› ë²¡í„°ë¡œ ë³€í™˜
- ì„ í˜• ëª¨ë¸ì—ì„œëŠ” ì…ë ¥ì´ ë°˜ë“œì‹œ 1ì°¨ì› ë²¡í„°, ë”°ë¼ì„œ ë²¡í„°í™” í•„ìˆ˜ì´ë‹¤.
- ë‹¤ìŒì€ 4X4 í”½ì…€ ì´ë¯¸ì§€ ì˜ˆì‹œì´ë‹¤
![alt text](<../../../assets/img/ARM/AI/image copy 38.png>)

## ë²¡í„°í™” ì½”ë“œ

```python
import numpy as np

# 0~255 ì‚¬ì´ì˜ ì„ì˜ì˜ ì •ìˆ˜ë¡œ êµ¬ì„±ëœ 4x4 í–‰ë ¬ ìƒì„±
a = np.random.randint(0, 255, (4, 4))
print("ì›ë³¸ 4x4 í–‰ë ¬:")
print(a)

# flattenì„ ì‚¬ìš©í•´ 1ì°¨ì› ë°°ì—´(ë²¡í„°)ë¡œ ë³€í™˜
b = a.flatten()
print("\nFlattenëœ 1ì°¨ì› ë°°ì—´:")
print(b)

# reshapeì„ ì‚¬ìš©í•´ í–‰ë ¬ í¬ê¸°ë¥¼ ë³€ê²½
# -1ì€ ìë™ ê³„ì‚°ë˜ë©°, ì´ ê²½ìš° ì´ ì›ì†Œ ìˆ˜ê°€ 16ì´ë¯€ë¡œ reshape(-1)ì€ (16,)ê³¼ ë™ì¼
# ì˜ˆ: (2, 8)ë¡œ ë°”ê¾¸ê³  ì‹¶ë‹¤ë©´ reshape(2, -1) ë˜ëŠ” reshape(2, 8) ëª¨ë‘ ê°€ëŠ¥
c = a.reshape(-1)
print("\nReshape(-1) ê²°ê³¼:")
print(c)
```
## ì„ í˜• ë¶„ë¥˜ê¸° - Score í•¨ìˆ˜

**score = WÂ·x + b**

- x: ì…ë ¥ ë²¡í„° (flattenëœ ì´ë¯¸ì§€)

- W: ê°€ì¤‘ì¹˜ í–‰ë ¬ (í´ë˜ìŠ¤ ìˆ˜ Ã— ì…ë ¥ íŠ¹ì„± ìˆ˜)

- b: ë°”ì´ì–´ìŠ¤ ë²¡í„°

- score: ê° í´ë˜ìŠ¤ì— ëŒ€í•œ ì ìˆ˜ (score vector)

![alt text](<../../../assets/img/ARM/AI/image copy 39.png>)

**ë³‘ë ¬ì²˜ë¦¬**

+ X: mê°œì˜ ì…ë ¥ ìƒ˜í”Œ (m, n), W: ê°€ì¤‘ì¹˜ (k, n)

+ ê²°ê³¼ S: score (m, k)

+ S = np.dot(X, W.T) + b

![alt text](<../../../assets/img/ARM/AI/image copy 40.png>)

## Sofemax ë¶„ë¥˜ê¸°
SoftmaxëŠ” ê° í´ë˜ìŠ¤ì˜ ì ìˆ˜ë¥¼ í™•ë¥ ë¡œ ë³€í™˜í•©ë‹ˆë‹¤:
![alt text](<../../../assets/img/ARM/AI/image copy 45.png>)

s_j: í´ë˜ìŠ¤ jì˜ ì ìˆ˜ (score)

ì „ì²´ í´ë˜ìŠ¤ì˜ scoreë¥¼ softmaxì— í†µê³¼ì‹œì¼œ í™•ë¥  ë¶„í¬ë¡œ ë§Œë“­ë‹ˆë‹¤.

![alt text](<../../../assets/img/ARM/AI/image copy 41.png>)

**ì§„í–‰ ê³¼ì •**
  
![alt text](<../../../assets/img/ARM/AI/image copy 42.png>)

**ross Entropy Loss ê³¼ì •**

Softmaxì˜ ì¶œë ¥ ê²°ê³¼ì™€ ì‹¤ì œ ì •ë‹µ ê°„ì˜ ì°¨ì´ë¥¼ ì¸¡ì •í•˜ëŠ” ì†ì‹¤ í•¨ìˆ˜:

![alt text](<../../../assets/img/ARM/AI/image copy 47.png>)

y_true ìœ„ì¹˜ì˜ softmax í™•ë¥ ì— -logë¥¼ ì·¨í•œ ê°’

ì •ë‹µ í´ë˜ìŠ¤ì˜ í™•ë¥ ì´ ë†’ì„ìˆ˜ë¡ lossê°€ ë‚®ì•„ì§

![alt text](<../../../assets/img/ARM/AI/image copy 43.png>)

**ìµœì í™”: SGD**

í•™ìŠµì€ ê²½ì‚¬ í•˜ê°•ë²•ì„ ê¸°ë°˜ìœ¼ë¡œ ìµœì í™”ë©ë‹ˆë‹¤:

    Full Gradient DescentëŠ” ì „ì²´ ë°ì´í„°ë¥¼ ì‚¬ìš©í•˜ì§€ë§Œ ì—°ì‚°ëŸ‰ì´ ë§ìŒ

    ëŒ€ì‹  **Stochastic Gradient Descent (SGD)**ëŠ” ì¼ë¶€ ë°°ì¹˜ë§Œ ì‚¬ìš©:

ì‘ì€ ë°°ì¹˜(mini-batch) ë‹¨ìœ„ë¡œ íŒŒë¼ë¯¸í„°ë¥¼ ì¡°ê¸ˆì”© ì—…ë°ì´íŠ¸í•˜ë©´ì„œ ì „ì²´ì ìœ¼ë¡œ ì†ì‹¤ì„ ì¤„ì—¬ë‚˜ê°€ëŠ” ë°©ì‹

![alt text](<../../../assets/img/ARM/AI/image copy 43.png>)


## ğŸ” ì „ì²´ í•™ìŠµ íë¦„ ìš”ì•½

    ì´ë¯¸ì§€ â†’ ë²¡í„°í™”

    ë²¡í„° â†’ Score ê³„ì‚° (Wx + b)

    Score â†’ Softmax â†’ í™•ë¥  ë¶„í¬

    ì˜ˆì¸¡ í™•ë¥ ê³¼ ì‹¤ì œ ë¼ë²¨ë¡œ Cross Entropy Loss ê³„ì‚°

    Lossì— ë”°ë¼ SGDë¡œ íŒŒë¼ë¯¸í„° ì—…ë°ì´íŠ¸



# Mnist ì‹¤ìŠµ
```python
import numpy as np
import pandas as pd

from tensorflow.keras.datasets.mnist import load_data
(train_x, train_y), (test_x, test_y) = load_data()

train_x.shape, train_y.shape, # Train ë°ì´í„° í¬ê¸° í™•ì¸
test_x.shape, test_y.shape # Test ë°ì´í„° í¬ê¸° í™•ì¸
```
> ê²°ê³¼ : ((10000, 28, 28), (10000,))
```python
# ì´ë¯¸ì§€ í™•ì¸í•˜ê¸°
from PIL import Image
img=train_x[0]

import matplotlib.pyplot as plt
img1 = Image.fromarray(img, mode = 'L')
plt.imshow(img1)

train_y[0]
```
> ê²°ê³¼:![alt text](<../../../assets/img/ARM/AI/image copy 48.png>)
```python
# ë°ì´í„° ì „ì²˜ë¦¬

## ì…ë ¥ í˜•íƒœ ë³€í™˜: 3-> 2 ì°¨ì›
### ë°ì´í„°ë¥¼ 2ì°¨ì› í˜•íƒœë¡œ ë³€í™˜: ì…ë ¥ ë°ì´í„°ê°€ ì„ í˜• ëª¨ë¸ì—ì„œëŠ” ë²¡í„° í˜•íƒœ
train_x1 = train_x.reshape(60000, -1)
test_x1 = test_x.reshape(10000, -1)

### ë°ì´í„° ê°’ì˜ í¬ê¸° ì¡°ì ˆ: 0~1 ì‚¬ì´ ê°’ìœ¼ë¡œ ë³€í™˜
train_x2 = train_x1 / 255
test_x2 = test_x1 / 255
```
```python
# ëª¨ë¸ ì„¤ì •

## ë¼ì´ë¸ŒëŸ¬ë¦¬ ë¶ˆëŸ¬ì˜¤ê¸°
from tensorflow.keras.models import Sequential
from tensorflow.keras.layers import Dense

## ëª¨ë¸ ì„¤ì •
md = Sequential()
md.add(Dense(10, activation='softmax', input_shape=(28*28,)))
md.summary()  # ëª¨ë¸ ìš”ì•½
```
> ê²°ê³¼:![alt text](<../../../assets/img/ARM/AI/image copy 49.png>)

```python
# ëª¨ë¸ í•™ìŠµ ì§„í–‰
## ëª¨ë¸ complile: ì†ì‹¤ í•¨ìˆ˜, ìµœì í™” í•¨ìˆ˜, ì¸¡ì • í•¨ìˆ˜ ì„¤ì •
md.compile(loss='sparse_categorical_crossentropy', optimizer='sgd', metrics=['acc'])

## ëª¨ë¸ í•™ìŠµ: í•™ìŠµ íšŸìˆ˜. batch_size, ê²€ì¦ìš© ë°ì´í„° ì„¤ì •
hist = md.fit(train_x2, train_y, epochs=30, batch_size=64, validation_split=0.2)
```
```
ê²°ê³¼:
Epoch 1/30
750/750 â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â” 3s 3ms/step - acc: 0.5941 - loss: 1.4901 - val_acc: 0.8569 - val_loss: 0.6543
Epoch 2/30
750/750 â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â” 2s 3ms/step - acc: 0.8506 - loss: 0.6448 - val_acc: 0.8757 - val_loss: 0.5063
Epoch 3/30
750/750 â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â” 2s 2ms/step - acc: 0.8672 - loss: 0.5302 - val_acc: 0.8859 - val_loss: 0.4481
Epoch 4/30
750/750 â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â” 3s 2ms/step - acc: 0.8812 - loss: 0.4678 - val_acc: 0.8905 - val_loss: 0.4164
Epoch 5/30
750/750 â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â” 3s 2ms/step - acc: 0.8831 - loss: 0.4416 - val_acc: 0.8957 - val_loss: 0.3948
Epoch 6/30
750/750 â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â” 3s 3ms/step - acc: 0.8844 - loss: 0.4267 - val_acc: 0.8987 - val_loss: 0.3801
Epoch 7/30
750/750 â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â” 2s 3ms/step - acc: 0.8912 - loss: 0.4030 - val_acc: 0.9002 - val_loss: 0.3687
Epoch 8/30
750/750 â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â” 2s 2ms/step - acc: 0.8933 - loss: 0.3907 - val_acc: 0.9028 - val_loss: 0.3596
Epoch 9/30
750/750 â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â” 3s 2ms/step - acc: 0.8933 - loss: 0.3876 - val_acc: 0.9030 - val_loss: 0.3526
Epoch 10/30
750/750 â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â” 3s 2ms/step - acc: 0.8954 - loss: 0.3798 - val_acc: 0.9061 - val_loss: 0.3466
Epoch 11/30
750/750 â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â” 3s 3ms/step - acc: 0.8990 - loss: 0.3698 - val_acc: 0.9076 - val_loss: 0.3411
Epoch 12/30
750/750 â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â” 2s 2ms/step - acc: 0.9017 - loss: 0.3588 - val_acc: 0.9087 - val_loss: 0.3369
Epoch 13/30
750/750 â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â” 2s 2ms/step - acc: 0.8977 - loss: 0.3606 - val_acc: 0.9088 - val_loss: 0.3329
Epoch 14/30
750/750 â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â” 3s 2ms/step - acc: 0.9024 - loss: 0.3509 - val_acc: 0.9093 - val_loss: 0.3294
Epoch 15/30
750/750 â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â” 2s 2ms/step - acc: 0.9030 - loss: 0.3540 - val_acc: 0.9097 - val_loss: 0.3261
Epoch 16/30
750/750 â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â” 3s 3ms/step - acc: 0.9023 - loss: 0.3441 - val_acc: 0.9097 - val_loss: 0.3237
Epoch 17/30
750/750 â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â” 3s 3ms/step - acc: 0.9039 - loss: 0.3392 - val_acc: 0.9120 - val_loss: 0.3209
Epoch 18/30
750/750 â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â” 2s 2ms/step - acc: 0.9045 - loss: 0.3394 - val_acc: 0.9114 - val_loss: 0.3185
Epoch 19/30
750/750 â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â” 3s 2ms/step - acc: 0.9081 - loss: 0.3316 - val_acc: 0.9131 - val_loss: 0.3165
Epoch 20/30
750/750 â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â” 2s 2ms/step - acc: 0.9072 - loss: 0.3361 - val_acc: 0.9129 - val_loss: 0.3144
Epoch 21/30
750/750 â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â” 2s 2ms/step - acc: 0.9093 - loss: 0.3287 - val_acc: 0.9137 - val_loss: 0.3124
Epoch 22/30
750/750 â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â” 4s 3ms/step - acc: 0.9061 - loss: 0.3333 - val_acc: 0.9141 - val_loss: 0.3110
Epoch 23/30
750/750 â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â” 2s 2ms/step - acc: 0.9098 - loss: 0.3243 - val_acc: 0.9144 - val_loss: 0.3093
Epoch 24/30
750/750 â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â” 2s 2ms/step - acc: 0.9109 - loss: 0.3231 - val_acc: 0.9152 - val_loss: 0.3078
Epoch 25/30
750/750 â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â” 2s 2ms/step - acc: 0.9099 - loss: 0.3222 - val_acc: 0.9159 - val_loss: 0.3065
Epoch 26/30
750/750 â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â” 3s 2ms/step - acc: 0.9094 - loss: 0.3223 - val_acc: 0.9156 - val_loss: 0.3052
Epoch 27/30
750/750 â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â” 2s 2ms/step - acc: 0.9093 - loss: 0.3212 - val_acc: 0.9161 - val_loss: 0.3039
Epoch 28/30
750/750 â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â” 3s 3ms/step - acc: 0.9124 - loss: 0.3136 - val_acc: 0.9164 - val_loss: 0.3027
Epoch 29/30
750/750 â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â” 2s 2ms/step - acc: 0.9122 - loss: 0.3174 - val_acc: 0.9168 - val_loss: 0.3016
Epoch 30/30
750/750 â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â” 2s 2ms/step - acc: 0.9140 - loss: 0.3116 - val_acc: 0.9178 - val_loss: 0.3005
```

```python
acc = hist.history['acc']
val_acc = hist.history['val_acc']
epoch = np.arange(1, len(acc)+1)

# í•™ìŠµ ê²°ê³¼ ë¶„ì„: í•™ìŠµ ê³¡ì„  ê·¸ë¦¬ê¸°
plt.figure(figsize=(10, 8))
plt.plot(epoch, acc, 'b', label='Training accuracy')
plt.plot(epoch, val_acc, 'r', label='Validation accuracy')
plt.title('Training and validation accuracy')
plt.xlabel('Epochs')
plt.ylabel('Accuracy')
plt.legend()
plt.show()
```
> ê²°ê³¼: ![alt text](<../../../assets/img/ARM/AI/image copy 50.png>)
```python
# í…ŒìŠ¤íŠ¸ìš© ë°ì´í„° í‰ê°€
md.evaluate(test_x2, test_y)

# ê°€ì¤‘ì¹˜ ì €ì¥
weight = md.get_weights()
weight
```
> ê²°ê³¼: ![alt text](<../../../assets/img/ARM/AI/image copy 51.png>)
```python
# Model Loss ì‹œê°í™”
plt.plot(hist.history['loss'], label='loss')
plt.plot(hist.history['val_loss'], label='val_loss')
plt.title('model loss')
plt.ylabel('loss')
plt.xlabel('epoch')
plt.legend(['train', 'test'], loc='upper left')
plt.show()
```
> ê²°ê³¼:![alt text](<../../../assets/img/ARM/AI/image copy 52.png>)

















# ANN ëª¨ë¸ ì‹¤ìŠµ

## ì ‘ê·¼ 
>mkdir F_MNIST
>
>cd F_MNIST
>
>python3 -m venv .fmnist
>
>source .fmnist/bin/activate
>
>pip install tensorflow matplotlib PyQt5 scikit-learn
>
>export QT_QPA_PLATFORM=wayland -> í„°ë¯¸ë„ ì˜¤í”ˆì‹œ ì‹¤í–‰
>
>python fs_mnist.py

## fs_mnist.py

```python
import tensorflow as tf
from tensorflow import keras

import numpy as np
import matplotlib
import matplotlib.pyplot as plt

# dataset load
fashion_mnist = keras.datasets.fashion_mnist

# spilt data (train / test)
(train_images, train_labels), (test_images, test_labels) = fashion_mnist.load_data()

print(train_images.shape)
print(train_labels.shape)
print(test_images.shape)
print(test_labels.shape)

class_names = ['T-shirt/top', 'Trouser', 'Pullover', 'Dress', 'Coat',
               'Sandal', 'Shirt', 'Sneaker', 'Bag', 'Ankle boot']
               
matplotlib.use('Qt5Agg')
NUM=20
plt.figure(figsize=(15,15))
plt.subplots_adjust(hspace=1)
for idx in range(NUM):
    sp = plt.subplot(5,5,idx+1)
    plt.imshow(train_images[idx])
    plt.title(f'{class_names[train_labels[idx]]}')
plt.show()

plt.figure()
plt.imshow(train_images[0])
plt.colorbar()
plt.grid(False)
plt.show()

# ê°„ë‹¨í•œ ì´ë¯¸ì§€ ì „ì²˜ë¦¬ (for ANN)
train_images = train_images / 255.0
test_images = test_images / 255.0

class_names = ['T-shirt/top', 'Trouser', 'Pullover', 'Dress', 'Coat',
               'Sandal', 'Shirt', 'Sneaker', 'Bag', 'Ankle boot']

plt.figure(figsize=(10,8))
for i in range(20):
    plt.subplot(4,5,i+1)
    plt.xticks([])
    plt.yticks([])
    plt.grid(False)
    plt.imshow(train_images[i], cmap=plt.cm.binary)
    plt.xlabel(class_names[train_labels[i]])
plt.show()

model = keras.Sequential ([
    keras.layers.Flatten(input_shape=(28,28)),
    keras.layers.Dense(128, activation='relu'),
    keras.layers.Dense(10, activation='softmax'),
])

model.summary()

model.compile(optimizer='adam',
              loss='sparse_categorical_crossentropy',
              metrics=['accuracy'])

model.fit(train_images, train_labels, epochs=20)

predictions = model.predict(test_images)

predictions[0]

np.argmax(predictions[0])

test_labels[0]

def plot_image(i, predictions_array, true_label, img):
  predictions_array, true_label, img = predictions_array[i], true_label[i], img[i]
  plt.grid(False)
  plt.xticks([])
  plt.yticks([])

  plt.imshow(img, cmap=plt.cm.binary)

  predicted_label = np.argmax(predictions_array)
  if predicted_label == true_label:
    color = 'blue'
  else:
    color = 'red'

  plt.xlabel("{} {:2.0f}% ({})".format(class_names[predicted_label],
                                100*np.max(predictions_array),
                                class_names[true_label]),
                                color=color)

def plot_value_array(i, predictions_array, true_label):
  predictions_array, true_label = predictions_array[i], true_label[i]
  plt.grid(False)
  plt.xticks([])
  plt.yticks([])
  thisplot = plt.bar(range(10), predictions_array, color="#777777")
  plt.ylim([0, 1])
  predicted_label = np.argmax(predictions_array)

  thisplot[predicted_label].set_color('red')
  thisplot[true_label].set_color('blue')

num_rows = 5
num_cols = 3
num_images = num_rows*num_cols
plt.figure(figsize=(2*2*num_cols, 2*num_rows))
for i in range(num_images):
  plt.subplot(num_rows, 2*num_cols, 2*i+1)
  plot_image(i, predictions, test_labels, test_images)
  plt.subplot(num_rows, 2*num_cols, 2*i+2)
  plot_value_array(i, predictions, test_labels)
plt.show()

from sklearn.metrics import accuracy_score
print('accuracy score : ', accuracy_score(tf.math.argmax(predictions, -1), test_labels))
```