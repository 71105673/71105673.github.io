---
title: "Day-5 ANN" 
date: "2025-06-27"
thumbnail: "../../../assets/img/ARM/AI/image copy 32.png"
---
# ì„ í˜• ëª¨ë¸ í•™ìŠµ

## ì„ í˜• ëª¨ë¸ í•™ìŠµ ê³¼ì •
![alt text](<../../../assets/img/ARM/AI/image copy 37.png>)

## ë²¡í„°í™” 
- ì„ í˜• ëª¨ë¸ì—ì„œëŠ” ì…ë ¥ ë°ì´í„°ëŠ” ë²¡í„° í˜•íƒœë¡œ ì •ë¦¬
- 2ì°¨ì› ë˜ëŠ” 3ì°¨ì› ì´ë¯¸ì§€ ë°ì´í„°ë¥¼ 1ì°¨ì› ë²¡í„°ë¡œ ë³€í™˜
- ì„ í˜• ëª¨ë¸ì—ì„œëŠ” ì…ë ¥ì´ ë°˜ë“œì‹œ 1ì°¨ì› ë²¡í„°, ë”°ë¼ì„œ ë²¡í„°í™” í•„ìˆ˜ì´ë‹¤.
- ë‹¤ìŒì€ 4X4 í”½ì…€ ì´ë¯¸ì§€ ì˜ˆì‹œì´ë‹¤
![alt text](<../../../assets/img/ARM/AI/image copy 38.png>)

## ë²¡í„°í™” ì½”ë“œ

```python
import numpy as np

# 0~255 ì‚¬ì´ì˜ ì„ì˜ì˜ ì •ìˆ˜ë¡œ êµ¬ì„±ëœ 4x4 í–‰ë ¬ ìƒì„±
a = np.random.randint(0, 255, (4, 4))
print("ì›ë³¸ 4x4 í–‰ë ¬:")
print(a)

# flattenì„ ì‚¬ìš©í•´ 1ì°¨ì› ë°°ì—´(ë²¡í„°)ë¡œ ë³€í™˜
b = a.flatten()
print("\nFlattenëœ 1ì°¨ì› ë°°ì—´:")
print(b)

# reshapeì„ ì‚¬ìš©í•´ í–‰ë ¬ í¬ê¸°ë¥¼ ë³€ê²½
# -1ì€ ìë™ ê³„ì‚°ë˜ë©°, ì´ ê²½ìš° ì´ ì›ì†Œ ìˆ˜ê°€ 16ì´ë¯€ë¡œ reshape(-1)ì€ (16,)ê³¼ ë™ì¼
# ì˜ˆ: (2, 8)ë¡œ ë°”ê¾¸ê³  ì‹¶ë‹¤ë©´ reshape(2, -1) ë˜ëŠ” reshape(2, 8) ëª¨ë‘ ê°€ëŠ¥
c = a.reshape(-1)
print("\nReshape(-1) ê²°ê³¼:")
print(c)
```
## ì„ í˜• ë¶„ë¥˜ê¸° - Score í•¨ìˆ˜

**score = WÂ·x + b**

- x: ì…ë ¥ ë²¡í„° (flattenëœ ì´ë¯¸ì§€)

- W: ê°€ì¤‘ì¹˜ í–‰ë ¬ (í´ë˜ìŠ¤ ìˆ˜ Ã— ì…ë ¥ íŠ¹ì„± ìˆ˜)

- b: ë°”ì´ì–´ìŠ¤ ë²¡í„°

- score: ê° í´ë˜ìŠ¤ì— ëŒ€í•œ ì ìˆ˜ (score vector)

![alt text](<../../../assets/img/ARM/AI/image copy 39.png>)

**ë³‘ë ¬ì²˜ë¦¬**

+ X: mê°œì˜ ì…ë ¥ ìƒ˜í”Œ (m, n), W: ê°€ì¤‘ì¹˜ (k, n)

+ ê²°ê³¼ S: score (m, k)

+ S = np.dot(X, W.T) + b

![alt text](<../../../assets/img/ARM/AI/image copy 40.png>)

## Sofemax ë¶„ë¥˜ê¸°
SoftmaxëŠ” ê° í´ë˜ìŠ¤ì˜ ì ìˆ˜ë¥¼ í™•ë¥ ë¡œ ë³€í™˜í•©ë‹ˆë‹¤:
![alt text](<../../../assets/img/ARM/AI/image copy 45.png>)

s_j: í´ë˜ìŠ¤ jì˜ ì ìˆ˜ (score)

ì „ì²´ í´ë˜ìŠ¤ì˜ scoreë¥¼ softmaxì— í†µê³¼ì‹œì¼œ í™•ë¥  ë¶„í¬ë¡œ ë§Œë“­ë‹ˆë‹¤.

![alt text](<../../../assets/img/ARM/AI/image copy 41.png>)

**ì§„í–‰ ê³¼ì •**
  
![alt text](<../../../assets/img/ARM/AI/image copy 42.png>)

**ross Entropy Loss ê³¼ì •**

Softmaxì˜ ì¶œë ¥ ê²°ê³¼ì™€ ì‹¤ì œ ì •ë‹µ ê°„ì˜ ì°¨ì´ë¥¼ ì¸¡ì •í•˜ëŠ” ì†ì‹¤ í•¨ìˆ˜:

![alt text](<../../../assets/img/ARM/AI/image copy 47.png>)

y_true ìœ„ì¹˜ì˜ softmax í™•ë¥ ì— -logë¥¼ ì·¨í•œ ê°’

ì •ë‹µ í´ë˜ìŠ¤ì˜ í™•ë¥ ì´ ë†’ì„ìˆ˜ë¡ lossê°€ ë‚®ì•„ì§

![alt text](<../../../assets/img/ARM/AI/image copy 43.png>)

**ìµœì í™”: SGD**

í•™ìŠµì€ ê²½ì‚¬ í•˜ê°•ë²•ì„ ê¸°ë°˜ìœ¼ë¡œ ìµœì í™”ë©ë‹ˆë‹¤:

    Full Gradient DescentëŠ” ì „ì²´ ë°ì´í„°ë¥¼ ì‚¬ìš©í•˜ì§€ë§Œ ì—°ì‚°ëŸ‰ì´ ë§ìŒ

    ëŒ€ì‹  **Stochastic Gradient Descent (SGD)**ëŠ” ì¼ë¶€ ë°°ì¹˜ë§Œ ì‚¬ìš©:

ì‘ì€ ë°°ì¹˜(mini-batch) ë‹¨ìœ„ë¡œ íŒŒë¼ë¯¸í„°ë¥¼ ì¡°ê¸ˆì”© ì—…ë°ì´íŠ¸í•˜ë©´ì„œ ì „ì²´ì ìœ¼ë¡œ ì†ì‹¤ì„ ì¤„ì—¬ë‚˜ê°€ëŠ” ë°©ì‹

![alt text](<../../../assets/img/ARM/AI/image copy 43.png>)


## ğŸ” ì „ì²´ í•™ìŠµ íë¦„ ìš”ì•½

    ì´ë¯¸ì§€ â†’ ë²¡í„°í™”

    ë²¡í„° â†’ Score ê³„ì‚° (Wx + b)

    Score â†’ Softmax â†’ í™•ë¥  ë¶„í¬

    ì˜ˆì¸¡ í™•ë¥ ê³¼ ì‹¤ì œ ë¼ë²¨ë¡œ Cross Entropy Loss ê³„ì‚°

    Lossì— ë”°ë¼ SGDë¡œ íŒŒë¼ë¯¸í„° ì—…ë°ì´íŠ¸


# í™œì„±í™” í•¨ìˆ˜ 
## Sigmoid
**ìì—°ì–´ ì²˜ë¦¬ë“±ì˜ íŠ¹ë³„í•œ ê²½ìš°ë¥¼ ì œì™¸í•˜ê³  ì˜ ì‚¬ìš©ë˜ì§€ ì•ŠìŒ (ì»´í“¨í„° ìì›ì„ ë§ì´ ì†Œëª¨í•¨)**

![alt text](<../../../assets/img/ARM/AI/image copy 54.png>)

## tanh
**ì›ì  ëŒ€ì¹­ íŠ¹ì§•ìœ¼ë¡œ ì¸í•˜ì—¬ sigmoidë³´ë‹¤ ë‚˜ì•„ì¡Œì§€ë§Œ, ì—¬ì „íˆ í•™ìŠµì´ ë˜ì§€ ì•ŠëŠ” êµ¬ê°„ì´ ë§ë‹¤.**

![alt text](<../../../assets/img/ARM/AI/image copy 55.png>)

## ReLU
**ì„±ëŠ¥ì´ ìš°ìˆ˜í•˜ë©° ìŒìˆ˜ì¸ ê²½ìš°ëŠ” ì œì™¸ëœë‹¤**

![alt text](<../../../assets/img/ARM/AI/image copy 56.png>)

# í•™ìŠµ ë¶„ì„

## ê³¼ì í•©

![alt text](<../../../assets/img/ARM/AI/image copy 57.png>)

## í•™ìŠµê³¡ì„ 

![alt text](<../../../assets/img/ARM/AI/image copy 58.png>)

![alt text](<../../../assets/img/ARM/AI/image copy 59.png>)

# Mnist ì‹¤ìŠµ
```python
import numpy as np
import pandas as pd

from tensorflow.keras.datasets.mnist import load_data
(train_x, train_y), (test_x, test_y) = load_data()

train_x.shape, train_y.shape, # Train ë°ì´í„° í¬ê¸° í™•ì¸
test_x.shape, test_y.shape # Test ë°ì´í„° í¬ê¸° í™•ì¸
```
> ê²°ê³¼ : ((10000, 28, 28), (10000,))
```python
# ì´ë¯¸ì§€ í™•ì¸í•˜ê¸°
from PIL import Image
img=train_x[0]

import matplotlib.pyplot as plt
img1 = Image.fromarray(img, mode = 'L')
plt.imshow(img1)

train_y[0]
```
> ê²°ê³¼:![alt text](<../../../assets/img/ARM/AI/image copy 48.png>)
```python
# ë°ì´í„° ì „ì²˜ë¦¬

## ì…ë ¥ í˜•íƒœ ë³€í™˜: 3-> 2 ì°¨ì›
### ë°ì´í„°ë¥¼ 2ì°¨ì› í˜•íƒœë¡œ ë³€í™˜: ì…ë ¥ ë°ì´í„°ê°€ ì„ í˜• ëª¨ë¸ì—ì„œëŠ” ë²¡í„° í˜•íƒœ
train_x1 = train_x.reshape(60000, -1)
test_x1 = test_x.reshape(10000, -1)

### ë°ì´í„° ê°’ì˜ í¬ê¸° ì¡°ì ˆ: 0~1 ì‚¬ì´ ê°’ìœ¼ë¡œ ë³€í™˜
train_x2 = train_x1 / 255
test_x2 = test_x1 / 255
```
```python
# ëª¨ë¸ ì„¤ì •

## ë¼ì´ë¸ŒëŸ¬ë¦¬ ë¶ˆëŸ¬ì˜¤ê¸°
from tensorflow.keras.models import Sequential
from tensorflow.keras.layers import Dense

## ëª¨ë¸ ì„¤ì •
md = Sequential()
md.add(Dense(10, activation='softmax', input_shape=(28*28,)))
md.summary()  # ëª¨ë¸ ìš”ì•½
```
> ê²°ê³¼:![alt text](<../../../assets/img/ARM/AI/image copy 49.png>)

```python
# ëª¨ë¸ í•™ìŠµ ì§„í–‰
## ëª¨ë¸ complile: ì†ì‹¤ í•¨ìˆ˜, ìµœì í™” í•¨ìˆ˜, ì¸¡ì • í•¨ìˆ˜ ì„¤ì •
md.compile(loss='sparse_categorical_crossentropy', optimizer='sgd', metrics=['acc'])

## ëª¨ë¸ í•™ìŠµ: í•™ìŠµ íšŸìˆ˜. batch_size, ê²€ì¦ìš© ë°ì´í„° ì„¤ì •
hist = md.fit(train_x2, train_y, epochs=30, batch_size=64, validation_split=0.2)
```
> ê²°ê³¼ :![alt text](<../../../assets/img/ARM/AI/image copy 53.png>)
 
```python
acc = hist.history['acc']
val_acc = hist.history['val_acc']
epoch = np.arange(1, len(acc)+1)

# í•™ìŠµ ê²°ê³¼ ë¶„ì„: í•™ìŠµ ê³¡ì„  ê·¸ë¦¬ê¸°
plt.figure(figsize=(10, 8))
plt.plot(epoch, acc, 'b', label='Training accuracy')
plt.plot(epoch, val_acc, 'r', label='Validation accuracy')
plt.title('Training and validation accuracy')
plt.xlabel('Epochs')
plt.ylabel('Accuracy')
plt.legend()
plt.show()
```
> ê²°ê³¼: ![alt text](<../../../assets/img/ARM/AI/image copy 50.png>)

```python
# í…ŒìŠ¤íŠ¸ìš© ë°ì´í„° í‰ê°€
md.evaluate(test_x2, test_y)

# ê°€ì¤‘ì¹˜ ì €ì¥
weight = md.get_weights()
weight
```
> ê²°ê³¼: ![alt text](<../../../assets/img/ARM/AI/image copy 51.png>)


```python
# Model Loss ì‹œê°í™”
plt.plot(hist.history['loss'], label='loss')
plt.plot(hist.history['val_loss'], label='val_loss')
plt.title('model loss')
plt.ylabel('loss')
plt.xlabel('epoch')
plt.legend(['train', 'test'], loc='upper left')
plt.show()
```
> ê²°ê³¼:![alt text](<../../../assets/img/ARM/AI/image copy 52.png>)

# CIFAR10 ë°ì´í„°ì…‹ ì´ìš©

## sqd
```python
import numpy as np
import pandas as pd
import matplotlib.pyplot as plt
from PIL import Image

# 1. CIFAR-10 ë°ì´í„° ë¶ˆëŸ¬ì˜¤ê¸°
from tensorflow.keras.datasets import cifar10
(train_x, train_y), (test_x, test_y) = cifar10.load_data()

print(train_x.shape, train_y.shape)  # (50000, 32, 32, 3)
print(test_x.shape, test_y.shape)    # (10000, 32, 32, 3)

# 2. ì´ë¯¸ì§€ í™•ì¸
img = train_x[0]
img1 = Image.fromarray(img)
plt.imshow(img1)
plt.title(f'Class: {train_y[0][0]}')
plt.show()

# 3. ë°ì´í„° ì „ì²˜ë¦¬

## 3-1: 4ì°¨ì› â†’ 2ì°¨ì› (ë²¡í„°í™”: 32x32x3 = 3072)
train_x1 = train_x.reshape(50000, -1)
test_x1 = test_x.reshape(10000, -1)

## 3-2: í”½ì…€ ì •ê·œí™” (0~1)
train_x2 = train_x1 / 255.0
test_x2 = test_x1 / 255.0

# 4. ëª¨ë¸ ì„¤ì •
from tensorflow.keras.models import Sequential
from tensorflow.keras.layers import Dense

md = Sequential()
md.add(Dense(10, activation='softmax', input_shape=(32 * 32 * 3,)))  # CIFAR-10ì— ë§ëŠ” ì…ë ¥ í¬ê¸°
md.summary()

# 5. ëª¨ë¸ ì»´íŒŒì¼
md.compile(loss='sparse_categorical_crossentropy', optimizer='sgd', metrics=['accuracy'])

# 6. ëª¨ë¸ í•™ìŠµ
hist = md.fit(train_x2, train_y, epochs=30, batch_size=128, validation_split=0.1)

# 7. í•™ìŠµ ê²°ê³¼ ì‹œê°í™”

## ì •í™•ë„ ì‹œê°í™”
acc = hist.history['accuracy']
val_acc = hist.history['val_accuracy']
epoch = np.arange(1, len(acc) + 1)

plt.figure(figsize=(10, 8))
plt.plot(epoch, acc, 'b', label='Training accuracy')
plt.plot(epoch, val_acc, 'r', label='Validation accuracy')
plt.title('Training and validation accuracy')
plt.xlabel('Epochs')
plt.ylabel('Accuracy')
plt.legend()
plt.show()

# 8. í…ŒìŠ¤íŠ¸ ë°ì´í„° í‰ê°€
test_loss, test_acc = md.evaluate(test_x2, test_y)
print(f'Test Accuracy: {test_acc:.4f}, Test Loss: {test_loss:.4f}')

# 9. ê°€ì¤‘ì¹˜ ì €ì¥
weights = md.get_weights()
print("Model weights:", weights)

# 10. ì†ì‹¤ ì‹œê°í™”
plt.plot(hist.history['loss'], label='loss')
plt.plot(hist.history['val_loss'], label='val_loss')
plt.title('Model Loss')
plt.ylabel('Loss')
plt.xlabel('Epoch')
plt.legend(['train', 'test'], loc='upper left')
plt.show()
```
### sgd (epoch = 30, Batch_size = 64, validation_split = 0.2)
**Test Accuracy: 0.4045, Test Loss: 1.7288**

![alt text](<../../../assets/img/ARM/AI/image copy 60.png>)
![alt text](<../../../assets/img/ARM/AI/image copy 61.png>)


### Sgd(epoch = 30, Batch_size = 128, validation_split = 0.2)
**Test Accuracy: 0.3945, Test Loss: 1.7484**

![alt text](<../../../assets/img/ARM/AI/image copy 62.png>)
![alt text](<../../../assets/img/ARM/AI/image copy 63.png>)

### Sgd(epoch = 30, Batch_size = 64, validation_split = 0.1)
**Test Accuracy: 0.3430, Test Loss: 1.8638**

![alt text](<../../../assets/img/ARM/AI/image copy 64.png>)
![alt text](<../../../assets/img/ARM/AI/image copy 65.png>)

### Sgd(epoch = 30, Batch_size = 128, validation_split = 0.1)
**Test Accuracy: 0.4017, Test Loss: 1.7334**

![alt text](<../../../assets/img/ARM/AI/image copy 66.png>)
![alt text](<../../../assets/img/ARM/AI/image copy 67.png>)

### Sgd(epoch = 30, Batch_size = 64, validation_split = 0.3)
**Test Accuracy: 0.3939, Test Loss: 1.7393**

![alt text](<../../../assets/img/ARM/AI/image copy 68.png>)
![alt text](<../../../assets/img/ARM/AI/image copy 69.png>)

### Sgd(epoch = 30, Batch_size = 128, validation_split = 0.3)
**Test Accuracy: 0.3965, Test Loss: 1.7414**

## Adam

```python
import numpy as np
import pandas as pd
import matplotlib.pyplot as plt
from PIL import Image

# 1. CIFAR-10 ë°ì´í„° ë¶ˆëŸ¬ì˜¤ê¸°
from tensorflow.keras.datasets import cifar10
(train_x, train_y), (test_x, test_y) = cifar10.load_data()

print(train_x.shape, train_y.shape)  # (50000, 32, 32, 3)
print(test_x.shape, test_y.shape)    # (10000, 32, 32, 3)

# 2. ì´ë¯¸ì§€ í™•ì¸
img = train_x[0]
img1 = Image.fromarray(img)
plt.imshow(img1)
plt.title(f'Class: {train_y[0][0]}')
plt.show()

# 3. ë°ì´í„° ì „ì²˜ë¦¬

## 3-1: 4ì°¨ì› â†’ 2ì°¨ì› (ë²¡í„°í™”: 32x32x3 = 3072)
train_x1 = train_x.reshape(50000, -1)
test_x1 = test_x.reshape(10000, -1)

## 3-2: í”½ì…€ ì •ê·œí™” (0~1)
train_x2 = train_x1 / 255.0
test_x2 = test_x1 / 255.0

# 4. ëª¨ë¸ ì„¤ì •

## 4-1 ë¼ì´ë¸ŒëŸ¬ë¦¬ ë¶ˆëŸ¬ì˜¤ê¸°
from tensorflow.keras.models import Sequential
from tensorflow.keras.layers import Dense, Input
from tensorflow.keras.optimizers import Adam


## 4-2. ëª¨ë¸ ì„¤ì •
md = Sequential()
md.add(Input(shape=(32*32*3,)))         # CIFAR10ì€ 32x32x3 = 3072
md.add(Dense(10, activation='softmax')) # ë‹¨ì¼ ì¶œë ¥ì¸µ
md.summary()  # ëª¨ë¸ ìš”ì•½

# 5. ëª¨ë¸ í•™ìŠµ ì§„í–‰
## ëª¨ë¸ complile: ì†ì‹¤ í•¨ìˆ˜, ìµœì í™” í•¨ìˆ˜, ì¸¡ì • í•¨ìˆ˜ ì„¤ì •
md.compile(loss='sparse_categorical_crossentropy', optimizer=Adam(learning_rate=0.001), metrics=['accuracy'])

# 6. ëª¨ë¸ í•™ìŠµ: í•™ìŠµ íšŸìˆ˜. batch_size, ê²€ì¦ìš© ë°ì´í„° ì„¤ì •
hist = md.fit(train_x2, train_y, epochs=30, batch_size=128, validation_split=0.05)

# 7. í•™ìŠµ ê²°ê³¼ ì‹œê°í™”

## ì •í™•ë„ ì‹œê°í™”
acc = hist.history['accuracy']
val_acc = hist.history['val_accuracy']
epoch = np.arange(1, len(acc) + 1)

plt.figure(figsize=(10, 8))
plt.plot(epoch, acc, 'b', label='Training accuracy')
plt.plot(epoch, val_acc, 'r', label='Validation accuracy')
plt.title('Training and validation accuracy')
plt.xlabel('Epochs')
plt.ylabel('Accuracy')
plt.legend()
plt.show()

# 8. í…ŒìŠ¤íŠ¸ ë°ì´í„° í‰ê°€
test_loss, test_acc = md.evaluate(test_x2, test_y)
print(f'Test Accuracy: {test_acc:.4f}, Test Loss: {test_loss:.4f}')

# 9. ê°€ì¤‘ì¹˜ ì €ì¥
weights = md.get_weights()
print("Model weights:", weights)

# 10. ì†ì‹¤ ì‹œê°í™”
plt.plot(hist.history['loss'], label='loss')
plt.plot(hist.history['val_loss'], label='val_loss')
plt.title('Model Loss')
plt.ylabel('Loss')
plt.xlabel('Epoch')
plt.legend(['train', 'test'], loc='upper left')
plt.show()
```

### Adam(epoch = 30, Batch_size = 64, validation_split = 0.15)
**Test Accuracy: 0.3892, Test Loss: 1.7702**
![alt text](../../../assets/img/ARM/AI/Adam/image.png)
![alt text](<../../../assets/img/ARM/AI/Adam/image copy.png>)

### Adam(epoch = 30, Batch_size= 128, validation_split=0.15)
**Test Accuracy: 0.3517, Test Loss: 1.8472**
![alt text](<../../../assets/img/ARM/AI/Adam/image copy 2.png>)
![alt text](<../../../assets/img/ARM/AI/Adam/image copy 3.png>)

### Adam(epoch = 30, Batch_size = 64, validation_split = 0.05)
**Test Accuracy: 0.3892, Test Loss: 1.7702**

![alt text](<../../../assets/img/ARM/AI/Adam/image copy 2.png>)
![alt text](<../../../assets/img/ARM/AI/Adam/image copy 3.png>)

### Adam(epoch = 30, Batch_size = 128, validation_split = 0.05)
**Test Accuracy: 0.3737, Test Loss: 1.8149**

![alt text](<../../../assets/img/ARM/AI/Adam/image copy 6.png>)
![alt text](<../../../assets/img/ARM/AI/Adam/image copy 7.png>)

### Adam(epoch = 30, Batch_size = 64, validation_split = 0.25)
**Test Accuracy: 0.3822, Test Loss: 1.7803**

![alt text](<../../../assets/img/ARM/AI/Adam/image copy 8.png>)
![alt text](<../../../assets/img/ARM/AI/Adam/image copy 9.png>)

### Adam(epoch = 30, Batch_size = 128, validation_split = 0.25)
**Test Accuracy: 0.3738, Test Loss: 1.7920**

![alt text](<../../../assets/img/ARM/AI/Adam/image copy 10.png>)
![alt text](<../../../assets/img/ARM/AI/Adam/image copy 11.png>)

## ğŸ” ê²°ë¡  ë° ê³ ì°°

### âœ… ê²°ë¡ 

ì´ë²ˆ ì‹¤í—˜ì—ì„œëŠ” CIFAR-10 ë°ì´í„°ì…‹ì„ ê¸°ë°˜ìœ¼ë¡œ ë‹¨ì¼ ë ˆì´ì–´ë¥¼ ì‚¬ìš©í•˜ëŠ” ì„ í˜• ëª¨ë¸ì„ êµ¬ì„±í•˜ê³ , ë‘ ê°€ì§€ ëŒ€í‘œì ì¸ ì˜µí‹°ë§ˆì´ì €ì¸ **SGD (Stochastic Gradient Descent)** ì™€ **Adam**ì˜ ì„±ëŠ¥ì„ ë¹„êµí•˜ì˜€ë‹¤.  
ê·¸ ê²°ê³¼, ë‹¤ìŒê³¼ ê°™ì€ ì°¨ì´ì ì´ ë‚˜íƒ€ë‚¬ë‹¤:

- **SGD**: í•™ìŠµë¥ ì´ ê³ ì •ë˜ì–´ ìˆì–´ ìˆ˜ë ´ ì†ë„ëŠ” ëŠë¦¬ì§€ë§Œ, **í›ˆë ¨ ì •í™•ë„ì™€ ì†ì‹¤ì´ ì ì§„ì ìœ¼ë¡œ ì•ˆì •ì ìœ¼ë¡œ í–¥ìƒ**ë˜ì—ˆë‹¤.
- **Adam**: ëª¨ë©˜í…€ê³¼ ì ì‘ì  í•™ìŠµë¥ ì„ í™œìš©í•˜ì—¬ **ì´ˆê¸° í•™ìŠµ ì†ë„ê°€ ë¹ ë¥´ì§€ë§Œ**, **ê²€ì¦ ì •í™•ë„(val_accuracy)ëŠ” ì˜¤íˆë ¤ í•˜ë½**í•˜ê±°ë‚˜ ë¶ˆì•ˆì •í•œ ëª¨ìŠµì„ ë³´ì˜€ë‹¤.

ì´ëŠ” Adamì´ ë” ë³µì¡í•œ ëª¨ë¸ êµ¬ì¡°ë‚˜ ì •ê·œí™” ê¸°ë²•ê³¼ í•¨ê»˜ ì‚¬ìš©ë  ë•Œ ê°•ì ì„ ë³´ì´ì§€ë§Œ, ë‹¨ì¸µ ì„ í˜• ëª¨ë¸ì—ì„œëŠ” **ê³¼ì í•©(overfitting)** ë˜ëŠ” **overshooting í˜„ìƒ**ì´ ë‚˜íƒ€ë‚  ìˆ˜ ìˆë‹¤ëŠ” ì ì„ ì‹œì‚¬í•œë‹¤.

---

### âš™ í•˜ì´í¼íŒŒë¼ë¯¸í„° ì¸ìì˜ ì˜í–¥

- `batch_size = 64`:  
  í•œ ë²ˆì— ì²˜ë¦¬ë˜ëŠ” ìƒ˜í”Œ ìˆ˜ê°€ ì‘ì„ìˆ˜ë¡ ëª¨ë¸ì´ ìì£¼ ì—…ë°ì´íŠ¸ë˜ë©°,  
  - **Adam**: ë¹ ë¥¸ ë°˜ì‘ â†’ ê³¼ì í•© ê°€ëŠ¥ì„± ì¦ê°€  
  - **SGD**: ì•ˆì •ì ì´ê³  ì™„ë§Œí•œ ìˆ˜ë ´ ê²½í–¥

- `validation_split = 0.05`:  
  ì „ì²´ ë°ì´í„° ì¤‘ 5%ë§Œì„ ê²€ì¦ì— ì‚¬ìš© â†’ ê²€ì¦ ì •í™•ë„(val_accuracy)ì˜ **ë³€ë™ì„±ì´ í¼**.  
  Adamì²˜ëŸ¼ **ì´ˆê¸° ê³¼ì í•© ê²½í–¥ì´ í° ì˜µí‹°ë§ˆì´ì €ì˜ ë‹¨ì ì´ ë” ë„ë“œë¼ì§**.

---

### ğŸ’¡ ê³ ì°°

Adamì€ ë¹ ë¥¸ ì´ˆê¸° ìˆ˜ë ´ì´ ê°€ëŠ¥í•˜ë‹¤ëŠ” ì¥ì ì´ ìˆìœ¼ë‚˜, **ë‹¨ì¼ ë ˆì´ì–´ ëª¨ë¸ êµ¬ì¡°ì—ì„œëŠ” ê³¼ì í•© ë° í•™ìŠµ ë¶ˆì•ˆì •ì„±**ì´ ë°œìƒí•  ìˆ˜ ìˆë‹¤. ë°˜ë©´, SGDëŠ” ëŠë¦¬ì§€ë§Œ **ê²°êµ­ ë” ì•ˆì •ì ìœ¼ë¡œ í•™ìŠµ**ì´ ì§„í–‰ë˜ì—ˆìœ¼ë©°, **ì¼ë°˜í™” ì„±ëŠ¥ì—ì„œë„ ë” ë‚˜ì€ ê²°ê³¼**ë¥¼ ë³´ì˜€ë‹¤.

ì´ë²ˆ ì‹¤í—˜ì„ í†µí•´ ë‹¤ìŒê³¼ ê°™ì€ ì‹œì‚¬ì ì„ ì–»ì„ ìˆ˜ ìˆë‹¤:

- **ë‹¨ìˆœ ëª¨ë¸ì—ì„œëŠ” SGDê°€ ë” ì•ˆì •ì **ì¼ ìˆ˜ ìˆë‹¤.
- **Adam ì‚¬ìš© ì‹œ learning_rate ì¡°ì •**, ë˜ëŠ” **Dropout ë“±ì˜ ì •ê·œí™” ê¸°ë²• ë„ì…**ì´ í•„ìš”í•˜ë‹¤.
- ì˜µí‹°ë§ˆì´ì €ì˜ ì„±ëŠ¥ì€ ëª¨ë¸ êµ¬ì¡° ë° í•˜ì´í¼íŒŒë¼ë¯¸í„° ì„¤ì •ì— ë”°ë¼ ë¯¼ê°í•˜ê²Œ ë‹¬ë¼ì§ˆ ìˆ˜ ìˆìœ¼ë¯€ë¡œ, **ëª¨ë¸ ë³µì¡ë„ì— ë§ì¶˜ ì˜µí‹°ë§ˆì´ì € ì„ íƒ**ì´ ì¤‘ìš”í•˜ë‹¤.













# ANN ëª¨ë¸ ì‹¤ìŠµ

## ì ‘ê·¼ 
>mkdir F_MNIST
>
>cd F_MNIST
>
>python3 -m venv .fmnist
>
>source .fmnist/bin/activate
>
>pip install tensorflow matplotlib PyQt5 scikit-learn
>
>export QT_QPA_PLATFORM=wayland -> í„°ë¯¸ë„ ì˜¤í”ˆì‹œ ì‹¤í–‰
>
>python fs_mnist.py

## fs_mnist.py

```python
import tensorflow as tf
from tensorflow import keras

import numpy as np
import matplotlib
import matplotlib.pyplot as plt

# dataset load
fashion_mnist = keras.datasets.fashion_mnist

# spilt data (train / test)
(train_images, train_labels), (test_images, test_labels) = fashion_mnist.load_data()

print(train_images.shape)
print(train_labels.shape)
print(test_images.shape)
print(test_labels.shape)

class_names = ['T-shirt/top', 'Trouser', 'Pullover', 'Dress', 'Coat',
               'Sandal', 'Shirt', 'Sneaker', 'Bag', 'Ankle boot']
               
matplotlib.use('Qt5Agg')
NUM=20
plt.figure(figsize=(15,15))
plt.subplots_adjust(hspace=1)
for idx in range(NUM):
    sp = plt.subplot(5,5,idx+1)
    plt.imshow(train_images[idx])
    plt.title(f'{class_names[train_labels[idx]]}')
plt.show()

plt.figure()
plt.imshow(train_images[0])
plt.colorbar()
plt.grid(False)
plt.show()

# ê°„ë‹¨í•œ ì´ë¯¸ì§€ ì „ì²˜ë¦¬ (for ANN)
train_images = train_images / 255.0
test_images = test_images / 255.0

class_names = ['T-shirt/top', 'Trouser', 'Pullover', 'Dress', 'Coat',
               'Sandal', 'Shirt', 'Sneaker', 'Bag', 'Ankle boot']

plt.figure(figsize=(10,8))
for i in range(20):
    plt.subplot(4,5,i+1)
    plt.xticks([])
    plt.yticks([])
    plt.grid(False)
    plt.imshow(train_images[i], cmap=plt.cm.binary)
    plt.xlabel(class_names[train_labels[i]])
plt.show()

model = keras.Sequential ([
    keras.layers.Flatten(input_shape=(28,28)),
    keras.layers.Dense(128, activation='relu'),
    keras.layers.Dense(10, activation='softmax'),
])

model.summary()

model.compile(optimizer='adam',
              loss='sparse_categorical_crossentropy',
              metrics=['accuracy'])

model.fit(train_images, train_labels, epochs=20)

predictions = model.predict(test_images)

predictions[0]

np.argmax(predictions[0])

test_labels[0]

def plot_image(i, predictions_array, true_label, img):
  predictions_array, true_label, img = predictions_array[i], true_label[i], img[i]
  plt.grid(False)
  plt.xticks([])
  plt.yticks([])

  plt.imshow(img, cmap=plt.cm.binary)

  predicted_label = np.argmax(predictions_array)
  if predicted_label == true_label:
    color = 'blue'
  else:
    color = 'red'

  plt.xlabel("{} {:2.0f}% ({})".format(class_names[predicted_label],
                                100*np.max(predictions_array),
                                class_names[true_label]),
                                color=color)

def plot_value_array(i, predictions_array, true_label):
  predictions_array, true_label = predictions_array[i], true_label[i]
  plt.grid(False)
  plt.xticks([])
  plt.yticks([])
  thisplot = plt.bar(range(10), predictions_array, color="#777777")
  plt.ylim([0, 1])
  predicted_label = np.argmax(predictions_array)

  thisplot[predicted_label].set_color('red')
  thisplot[true_label].set_color('blue')

num_rows = 5
num_cols = 3
num_images = num_rows*num_cols
plt.figure(figsize=(2*2*num_cols, 2*num_rows))
for i in range(num_images):
  plt.subplot(num_rows, 2*num_cols, 2*i+1)
  plot_image(i, predictions, test_labels, test_images)
  plt.subplot(num_rows, 2*num_cols, 2*i+2)
  plot_value_array(i, predictions, test_labels)
plt.show()

from sklearn.metrics import accuracy_score
print('accuracy score : ', accuracy_score(tf.math.argmax(predictions, -1), test_labels))
```