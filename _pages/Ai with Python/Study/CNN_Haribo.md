---
title: "Day-7 CNN_Haribo" 
date: "2025-07-01"
thumbnail: "../../../assets/img/ARM/AI/CNN/image copy 45.png"
---

# Haribo Mix Gummy

## ê°ì²´ ë¶„ë¥˜
**Heart**
![alt text](../../../assets/img/ARM/AI/CNN/haribo/image.png)

**Bear**
![alt text](<../../../assets/img/ARM/AI/CNN/haribo/image copy 3.png>)

**Cola**
![alt text](<../../../assets/img/ARM/AI/CNN/haribo/image copy 2.png>)

**Egg**
![alt text](<../../../assets/img/ARM/AI/CNN/haribo/image copy.png>)

**Ring**

![alt text](<../../../assets/img/ARM/AI/CNN/haribo/image copy 8.png>)

## Code

```python
from google.colab import drive
drive.mount('/content/drive')

import matplotlib.pyplot as plt
import numpy as np
from tensorflow.keras.preprocessing.image import ImageDataGenerator
from tensorflow.keras.applications import MobileNetV2
from tensorflow.keras import models, layers
from tensorflow.keras.optimizers import Adam
from tensorflow.keras.callbacks import EarlyStopping, ModelCheckpoint

# ë°ì´í„° ê²½ë¡œ (êµ¬ê¸€ ë“œë¼ì´ë¸Œ ë§ˆìš´íŠ¸ëœ ê²½ë¡œ ê¸°ì¤€)
dataset_path = '/content/drive/MyDrive/haribo_dataset'

# Best Model ë§Œë“¤ê¸°
checkpoint = ModelCheckpoint('best_model.h5', save_best_only=True)

# âœ… í´ë˜ìŠ¤ ì´ë¦„ (ring ì¶”ê°€)
class_names = ['bear', 'cola', 'egg', 'heart', 'ring']

# âœ… ë°ì´í„° ì¦ê°• ì„¤ì •
datagen = ImageDataGenerator(
    rescale=1./255,
    validation_split=0.2,
    rotation_range=10,
    width_shift_range=0.1,
    height_shift_range=0.1,
    shear_range=0.1,
    zoom_range=0.1,
    horizontal_flip=True,
    fill_mode='nearest'
)

# âœ… í•™ìŠµìš© ë°ì´í„° ìƒì„±ê¸°
train_generator = datagen.flow_from_directory(
    dataset_path,
    target_size=(96, 96),
    batch_size=32,
    class_mode='categorical',
    subset='training',
    shuffle=True
)

# âœ… ê²€ì¦ìš© ë°ì´í„° ìƒì„±ê¸°
val_generator = datagen.flow_from_directory(
    dataset_path,
    target_size=(96, 96),
    batch_size=32,
    class_mode='categorical',
    subset='validation',
    shuffle=True
)

# âœ… MobileNetV2 ê¸°ë°˜ ì „ì´í•™ìŠµ ëª¨ë¸
base_model = MobileNetV2(input_shape=(96, 96, 3), include_top=False, weights='imagenet')
base_model.trainable = False

model = models.Sequential([
    base_model,
    layers.GlobalAveragePooling2D(),
    layers.Dense(128, activation='relu'),
    layers.Dropout(0.5),
    layers.Dense(len(class_names), activation='softmax')  # í´ë˜ìŠ¤ ìˆ˜ ìë™ ë°˜ì˜
])

# âœ… ì˜µí‹°ë§ˆì´ì € ë³€ê²½ (Adam)
model.compile(optimizer=Adam(learning_rate=1e-4),
              loss='categorical_crossentropy',
              metrics=['accuracy'])

# âœ… ì½œë°± ì„¤ì •
early_stop = EarlyStopping(monitor='val_loss', patience=5, restore_best_weights=True)
checkpoint = ModelCheckpoint('best_model.h5', save_best_only=True)

# âœ… í•™ìŠµ ì‹¤í–‰
history = model.fit(
    train_generator,
    validation_data=val_generator,
    epochs=50,
    callbacks=[early_stop, checkpoint],
    verbose=2
)

# âœ… ì •í™•ë„ ì‹œê°í™”
acc = history.history['accuracy']
val_acc = history.history['val_accuracy']
loss = history.history['loss']
val_loss = history.history['val_loss']
epochs = range(len(acc))

plt.plot(epochs, acc, 'bo', label='Training acc')
plt.plot(epochs, val_acc, 'b', label='Validation acc')
plt.title('Training and Validation Accuracy')
plt.legend()
plt.show()

plt.plot(epochs, loss, 'bo', label='Training loss')
plt.plot(epochs, val_loss, 'b', label='Validation loss')
plt.title('Training and Validation Loss')
plt.legend()
plt.show()

# âœ… ì¦ê°•ëœ ì´ë¯¸ì§€ ìƒ˜í”Œ ì‹œê°í™”
x_batch, y_batch = next(train_generator)
plt.figure(figsize=(10, 10))
for i in range(25):
    plt.subplot(5, 5, i + 1)
    plt.xticks([]); plt.yticks([]); plt.grid(False)
    plt.imshow(x_batch[i])
    label_idx = np.argmax(y_batch[i])
    plt.xlabel(class_names[label_idx])
plt.tight_layout()
plt.show()
```




## ê²°ê³¼

## DropOut(0.5), rotation_range = 10

### Accuracy, Loss (0.8351, 0.4658)
![alt text](<../../../assets/img/ARM/AI/CNN/haribo/image copy 4.png>)
![alt text](<../../../assets/img/ARM/AI/CNN/haribo/image copy 5.png>)

### graph
![alt text](<../../../assets/img/ARM/AI/CNN/haribo/image copy 6.png>)
### í•™ìŠµ ì´ë¯¸ì§€ ì¶œë ¥
![alt text](<../../../assets/img/ARM/AI/CNN/haribo/image copy 7.png>)




## DropOut(0.3), rotation_range = 45

### Accuracy, Loss (0.8763, 0.3486)
![alt text](<../../../assets/img/ARM/AI/CNN/haribo/image copy 16.png>)
### graph
![alt text](<../../../assets/img/ARM/AI/CNN/haribo/image copy 17.png>)
### í•™ìŠµ ì´ë¯¸ì§€ ì¶œë ¥
![alt text](<../../../assets/img/ARM/AI/CNN/haribo/image copy 18.png>)


## DropOut(0.4), rotation_range = 90

### Accuracy, Loss (0.8866, 0.2899)
![alt text](<../../../assets/img/ARM/AI/CNN/haribo/image copy 19.png>)
![alt text](<../../../assets/img/ARM/AI/CNN/haribo/image copy 20.png>)
### graph
![alt text](<../../../assets/img/ARM/AI/CNN/haribo/image copy 21.png>)
### í•™ìŠµ ì´ë¯¸ì§€ ì¶œë ¥
![alt text](<../../../assets/img/ARM/AI/CNN/haribo/image copy 22.png>)


# ì„¤ëª…

## ë°ì´í„° ì¦ê°• 

í›ˆë ¨ ì´ë¯¸ì§€ì˜ ë‹¤ì–‘ì„±ì„ ì¸ìœ„ì ìœ¼ë¡œ ëŠ˜ë ¤ì„œ **ê³¼ì í•©(overfitting)**ì„ ë°©ì§€í•˜ê³  ëª¨ë¸ì´ ë” ì¼ë°˜í™”(generalization)ë˜ë„ë¡ ë•ê¸° ìœ„í•´ ì‚¬ìš©í•©ë‹ˆë‹¤.

ğŸ“Œ ì£¼ìš” íŒŒë¼ë¯¸í„° ì„¤ëª…:

    rescale=1./255
    â†’ í”½ì…€ ê°’(0255)ì„ **01ë¡œ ì •ê·œí™”**í•´ì¤Œ. ì‹ ê²½ë§ í•™ìŠµ ì•ˆì •ì„±ì„ ë†’ì´ê¸° ìœ„í•´ í•„ìš”í•©ë‹ˆë‹¤.

    validation_split=0.2
    â†’ ì „ì²´ ë°ì´í„° ì¤‘ 20%ë¥¼ ê²€ì¦ìš©, ë‚˜ë¨¸ì§€ë¥¼ í•™ìŠµìš©ìœ¼ë¡œ ìë™ ë¶„ë¦¬í•©ë‹ˆë‹¤.

    rotation_range=10
    â†’ ì´ë¯¸ì§€ë¥¼ ìµœëŒ€ Â±10ë„ íšŒì „. íšŒì „ì— ê°•í•œ ëª¨ë¸ì„ ë§Œë“­ë‹ˆë‹¤.

    width_shift_range=0.1, height_shift_range=0.1
    â†’ ì´ë¯¸ì§€ì˜ ìˆ˜í‰/ìˆ˜ì§ ë°©í–¥ìœ¼ë¡œ 10% ì´ë™. ìœ„ì¹˜ì— ëœ ë¯¼ê°í•˜ë„ë¡ í•©ë‹ˆë‹¤.

    shear_range=0.1
    â†’ ë¹„ìŠ¤ë“¬íˆ ê¸°ìš¸ì´ëŠ” ë³€í˜•(ì „ë‹¨ shear)ì„ ì ìš©í•©ë‹ˆë‹¤.

    zoom_range=0.1
    â†’ í™•ëŒ€/ì¶•ì†Œë¥¼ í†µí•´ í¬ê¸° ë³€í™”ì— ì ì‘í•˜ëŠ” ëª¨ë¸ì„ ë§Œë“­ë‹ˆë‹¤.

    horizontal_flip=True
    â†’ ì¢Œìš° ë°˜ì „. ë¹„ëŒ€ì¹­ì ì¸ ë°ì´í„°ë„ ì˜ í•™ìŠµí•˜ê²Œ í•©ë‹ˆë‹¤.

    fill_mode='nearest'
    â†’ ì´ë¯¸ì§€ ì´ë™/íšŒì „ ì‹œ ìƒê¸´ ë¹ˆ ë¶€ë¶„ì€ ê°€ì¥ ê°€ê¹Œìš´ í”½ì…€ ê°’ìœ¼ë¡œ ì±„ì›ë‹ˆë‹¤.

## ì „ì´ í•™ìŠµ ëª¨ë¸ êµ¬ì¡°

ğŸ¯ MobileNetV2ë€?

    ImageNet ë°ì´í„°ì…‹ìœ¼ë¡œ ë¯¸ë¦¬ í•™ìŠµëœ ê²½ëŸ‰ CNN ëª¨ë¸

    ëª¨ë°”ì¼ì´ë‚˜ ì„ë² ë””ë“œ í™˜ê²½ì—ì„œë„ ì„±ëŠ¥ê³¼ ì†ë„ ê· í˜•ì´ ì¢‹ìŒ

ğŸ“Œ ì£¼ìš” ì„¤ì •:

    include_top=False:
    â†’ MobileNetV2ì˜ ë§ˆì§€ë§‰ fully connected ë¶„ë¥˜ ì¸µì€ ì œì™¸í•˜ê³ , **íŠ¹ì§• ì¶”ì¶œê¸°(feature extractor)**ë¡œë§Œ ì‚¬ìš©í•©ë‹ˆë‹¤.

    weights='imagenet':
    â†’ ImageNetì—ì„œ í•™ìŠµëœ ê°€ì¤‘ì¹˜ë¥¼ ê°€ì ¸ì™€ ì‚¬ìš©í•©ë‹ˆë‹¤.

    base_model.trainable = False:
    â†’ ê¸°ì¡´ì— í•™ìŠµëœ ê°€ì¤‘ì¹˜ë¥¼ ê³ ì •(freeze). ìƒˆë¡œìš´ ë°ì´í„°ì…‹ì—ì„œ ë‹¤ì‹œ í•™ìŠµí•˜ì§€ ì•ŠìŒ.

GlobalAveragePooling2D:

    í‰ê· ì„ ì´ìš©í•˜ì—¬ ì „ì²´ íŠ¹ì§• ë§µì„ ë²¡í„°ë¡œ ë°”ê¿”ì£¼ëŠ” ì¸µ. Flattenë³´ë‹¤ ê³¼ì í•©ì— ëœ ë¯¼ê°í•¨.

Dropout:

    ì¼ë¶€ ë‰´ëŸ°ì„ ëœë¤í•˜ê²Œ ì œê±°í•˜ì—¬ ì¼ë°˜í™” ì„±ëŠ¥ í–¥ìƒ.

Softmax ì¶œë ¥ì¸µ:

    ë‹¤ì¤‘ í´ë˜ìŠ¤ ë¶„ë¥˜ ë¬¸ì œì´ë¯€ë¡œ softmax ì‚¬ìš©.

## ì½œë°± ì„¤ì • ë° í•™ìŠµ

ğŸ¯ ì½œë°±(CallBacks)ì´ë€?

ëª¨ë¸ í•™ìŠµ ë„ì¤‘ íŠ¹ì • ì¡°ê±´ì„ ë§Œì¡±í•  ë•Œ ìë™ìœ¼ë¡œ ë™ì‘í•˜ëŠ” ê¸°ëŠ¥ (ì˜ˆ: í•™ìŠµ ì¤‘ë‹¨, ëª¨ë¸ ì €ì¥ ë“±)

ğŸ“Œ EarlyStopping

    monitor='val_loss':
    ê²€ì¦ ì†ì‹¤(val_loss)ì„ ê¸°ì¤€ìœ¼ë¡œ ëª¨ë‹ˆí„°ë§

    patience=5:
    ê²€ì¦ ì†ì‹¤ì´ 5ë²ˆ ì—°ì† ê°œì„ ë˜ì§€ ì•Šìœ¼ë©´ í•™ìŠµ ì¤‘ë‹¨

    restore_best_weights=True:
    ê°€ì¥ ì„±ëŠ¥ì´ ì¢‹ì•˜ë˜ ì‹œì ì˜ ê°€ì¤‘ì¹˜ë¡œ ë³µì›

ğŸ“Œ ModelCheckpoint

    save_best_only=True:
    ê²€ì¦ ì„±ëŠ¥ì´ ì¢‹ì•„ì§ˆ ë•Œë§Œ ëª¨ë¸ì„ ì €ì¥ â†’ best_model.h5ë¡œ ì €ì¥ë¨

















# ì¹´ë©”ë¼ í™•ì¸

```python
import cv2
import numpy as np
import tensorflow as tf
import json

# ëª¨ë¸ê³¼ í´ë˜ìŠ¤ ì´ë¦„ ë¡œë“œ
model = tf.keras.models.load_model('best_model.h5')

with open('class_names.json', 'r') as f:
    class_names = json.load(f)

def preprocess(frame):
    img = cv2.resize(frame, (96, 96))
    img = img.astype('float32') / 255.0
    return np.expand_dims(img, axis=0)

cap = cv2.VideoCapture(2)
if not cap.isOpened():
    print("ì¹´ë©”ë¼ë¥¼ ì—´ ìˆ˜ ì—†ìŠµë‹ˆë‹¤.")
    exit()

print("ì ¤ë¦¬ ë¶„ë¥˜ ì‹œì‘! (Q í‚¤ë¥¼ ëˆ„ë¥´ë©´ ì¢…ë£Œ)")
while True:
    ret, frame = cap.read()
    if not ret:
        break

    input_img = preprocess(frame)
    pred = model.predict(input_img)
    label = class_names[np.argmax(pred)]

    # ì˜ˆì¸¡ ê²°ê³¼ í™”ë©´ì— ì¶œë ¥
    cv2.putText(frame, f'Prediction: {label}', (10, 30),
                cv2.FONT_HERSHEY_SIMPLEX, 1, (0, 255, 0), 2)
    cv2.imshow('Haribo Classifier', frame)

    if cv2.waitKey(1) & 0xFF == ord('q'):
        break

cap.release()
cv2.destroyAllWindows()
```

## ê²°ê³¼

![text](<../../../assets/img/ARM/AI/CNN/haribo/image copy 9.png>) ![text](<../../../assets/img/ARM/AI/CNN/haribo/image copy 10.png>) ![text](<../../../assets/img/ARM/AI/CNN/haribo/image copy 11.png>) ![text](<../../../assets/img/ARM/AI/CNN/haribo/image copy 12.png>) ![text](<../../../assets/img/ARM/AI/CNN/haribo/image copy 13.png>) ![text](<../../../assets/img/ARM/AI/CNN/haribo/image copy 14.png>) ![text](<../../../assets/img/ARM/AI/CNN/haribo/image copy 15.png>) ![text](<../../../assets/img/ARM/AI/CNN/haribo/image copy 46.png>)

í•™ìŠµí•œ Best_model.h5ë¥¼ ê¸°ì¤€ìœ¼ë¡œ í™•ì¸í•œê²°ê³¼ ê°ì²´ë¥¼ ì˜ ì¸ì‹í•˜ëŠ” ëª¨ìŠµì„ í™•ì¸í•  ìˆ˜ ìˆë‹¤. 

ì¶”ê°€ë¡œ ì§„í–‰í•œ DropOut(0.5 -> 0.3), rotation_range = 10 -> 45 ì˜ ê²½ìš° Best_model_2.h5ë¡œ ì§„í–‰í•˜ì˜€ì§€ë§Œ, DropOutì´ ê³¼ë„í•˜ê²Œ ì ìš© ë° ì¦ê°• ê°•ë„ê°€ ì¦ê°€í•¨ì— ë”°ë¼ íŠ€ëŠ” ê²½í–¥ì´ ì¦ê°€í–ˆìŠµë‹ˆë‹¤. 

ë”°ë¼ì„œ DropOut = 0.4, rotation_range = 90ìœ¼ë¡œ ì„¤ì •í•œ ê²½ìš°
ì•ˆì •ì ì¸ í•™ìŠµ ê³¡ì„ ê³¼ í•¨ê»˜ ì¢‹ì€ ì¼ë°˜í™” ì„±ëŠ¥ì„ ê°€ì§ˆ ìˆ˜ ìˆì—ˆê³ , ê°€ì¥ ì¢‹ì€ ì¸ì‹ë¥ ì„ ë³´ì˜€ìŠµë‹ˆë‹¤.



### ê³ ì°°

ì´ë²ˆ í”„ë¡œì íŠ¸ì—ì„œëŠ” ì „ì²´ì ì¸ ë°ì´í„°ì…‹ì˜ ì–‘ì´ ë¶€ì¡±í•´ ë‹¤ì–‘í•œ ê°ë„ë‚˜ í˜•íƒœì— ëŒ€í•œ ì¶©ë¶„í•œ í•™ìŠµì´ ì–´ë ¤ì› ìŠµë‹ˆë‹¤. 

íŠ¹íˆ ê°ì²´ì˜ ìœ„ì¹˜ë‚˜ ë°©í–¥ì´ ë‹¬ë¼ì§ˆ ê²½ìš° ì¸ì‹ ì •í™•ë„ê°€ ë‹¤ì†Œ ë‚®ê²Œ ë‚˜íƒ€ë‚¬ìŠµë‹ˆë‹¤.

í•˜ì§€ë§Œ ë°ì´í„° ì¦ê°• ê¸°ë²•ì„ ì ìš©í•¨ìœ¼ë¡œì¨ ì´ëŸ¬í•œ í•œê³„ë¥¼ ì–´ëŠ ì •ë„ ë³´ì™„í•  ìˆ˜ ìˆì—ˆê³ , ê²°ê³¼ì ìœ¼ë¡œ ì œí•œëœ ë°ì´í„°ì…‹ì—ì„œë„ ë¹„êµì  ë†’ì€ ì •í™•ë„ë¡œ ì§€ì •í•œ ê°ì²´ë¥¼ ë¶„ë¥˜í•  ìˆ˜ ìˆì—ˆìŠµë‹ˆë‹¤. 

ì´ëŠ” ë°ì´í„° ì¦ê°•ì´ ëª¨ë¸ì˜ ì¼ë°˜í™” ì„±ëŠ¥ì„ í–¥ìƒì‹œí‚¤ëŠ” ë° íš¨ê³¼ì ì´ë¼ëŠ” ì ì„ ë³´ì—¬ì¤ë‹ˆë‹¤.