---
title: "Skin_Check" 
date: "2025-07-01"
thumbnail: "../../../assets/img/ARM/AI/skin_doctor/image.png"
---

# <Google Colab í•™ìŠµ Code>

```python
# SKIN_Project.ipynb
import matplotlib.pyplot as plt
import numpy as np
import os
from tensorflow.keras.preprocessing.image import ImageDataGenerator
from tensorflow.keras.applications import MobileNetV2
from tensorflow.keras import models, layers
from tensorflow.keras.optimizers import Adam
from tensorflow.keras.callbacks import EarlyStopping, ModelCheckpoint

# âœ… Google Drive ë§ˆìš´íŠ¸
from google.colab import drive
drive.mount('/content/drive')

# âœ… ê²½ë¡œ ì„¤ì •
dataset_path = '/content/drive/MyDrive/SKIN/dataset_skin'  # ë„ˆê°€ ì˜¬ë¦° ê²½ë¡œë¡œ ìˆ˜ì •
model_save_path = '/content/drive/MyDrive/SKIN/skin_model.h5'  # ì›í•˜ëŠ” ì €ì¥ ê²½ë¡œ

# âœ… ë°ì´í„° ì¦ê°• ì„¤ì • (rotation_rangeëŠ” 15ë„ë§Œ ì¤Œ)
datagen = ImageDataGenerator(
    rescale=1./255,
    validation_split=0.2,
    rotation_range=90,
    width_shift_range=0.3,
    height_shift_range=0.3,
    shear_range=0.3,
    zoom_range=0.3,
    brightness_range=[0.6, 1.4],
    horizontal_flip=True,
    fill_mode='nearest'
)

# âœ… ë°ì´í„° ë¡œë”©
train_generator = datagen.flow_from_directory(
    dataset_path,
    target_size=(96, 96),  # MobileNetV2ëŠ” ìµœì†Œ 96x96ë¶€í„° ê°€ëŠ¥
    batch_size=32,
    class_mode='categorical',
    subset='training',
    shuffle=True
)

val_generator = datagen.flow_from_directory(
    dataset_path,
    target_size=(96, 96),
    batch_size=32,
    class_mode='categorical',
    subset='validation',
    shuffle=True
)

# âœ… í´ë˜ìŠ¤ ì´ë¦„ ìë™ ì¶”ì¶œ
class_names = list(train_generator.class_indices.keys())
print("í´ë˜ìŠ¤ ì¸ë±ìŠ¤:", train_generator.class_indices)

# âœ… MobileNetV2 ê¸°ë°˜ ëª¨ë¸ êµ¬ì„±
base_model = MobileNetV2(input_shape=(96, 96, 3), include_top=False, weights='imagenet')
base_model.trainable = False

model = models.Sequential([
    base_model,
    layers.GlobalAveragePooling2D(),
    layers.Dense(128, activation='relu'),
    layers.Dropout(0.5),
    layers.Dense(len(class_names), activation='softmax')
])

model.compile(optimizer=Adam(learning_rate=1e-4),
              loss='categorical_crossentropy',
              metrics=['accuracy'])

# âœ… ì½œë°± ì„¤ì •
early_stop = EarlyStopping(monitor='val_loss', patience=5, restore_best_weights=True)
checkpoint = ModelCheckpoint('best_model.h5', save_best_only=True)

# âœ… í•™ìŠµ ì‹¤í–‰
history = model.fit(
    train_generator,
    validation_data=val_generator,
    epochs=50,
    callbacks=[early_stop, checkpoint],
    verbose=2
)

# âœ… ê²°ê³¼ ì‹œê°í™”
acc = history.history['accuracy']
val_acc = history.history['val_accuracy']
loss = history.history['loss']
val_loss = history.history['val_loss']
epochs = range(len(acc))

plt.plot(epochs, acc, 'bo', label='Training acc')
plt.plot(epochs, val_acc, 'b', label='Validation acc')
plt.title('Training and Validation Accuracy')
plt.legend()
plt.show()

plt.plot(epochs, loss, 'bo', label='Training loss')
plt.plot(epochs, val_loss, 'b', label='Validation loss')
plt.title('Training and Validation Loss')
plt.legend()
plt.show()

# âœ… í•™ìŠµ ì´ë¯¸ì§€ ì˜ˆì‹œ
x_batch, y_batch = next(train_generator)
plt.figure(figsize=(10, 10))
for i in range(25):
    plt.subplot(5, 5, i + 1)
    plt.xticks([]); plt.yticks([]); plt.grid(False)
    plt.imshow(x_batch[i])
    label_idx = np.argmax(y_batch[i])
    plt.xlabel(class_names[label_idx])
plt.tight_layout()
plt.show()

# âœ… ëª¨ë¸ ì €ì¥ (.h5 íŒŒì¼)
model.save(model_save_path)
print(f"ëª¨ë¸ì´ ì €ì¥ë˜ì—ˆìŠµë‹ˆë‹¤: {model_save_path}")
```

## Result
![alt text](<../../../assets/img/ARM/AI/skin/ìŠ¤í¬ë¦°ìƒ· 2025-07-04 204712.png>)

## Ubuntu Terminal Dir
```json
// skin_names.json
[
  "ê¸°ì €ì„¸í¬ì•”",
  "ë³´ì›¬ë³‘",
  "í‘œí”¼ë‚­ì¢…",
  "ë¹„ë¦½ì¢…",
  "ì •ìƒí”¼ë¶€",
  "í™”ë†ì„± ìœ¡ì•„ì¢…",
  "í¸í‰ì„¸í¬ì•”",
  "ì‚¬ë§ˆê·€"
]
```
## predict_cam.py
``` python
# predict_cam.py
import cv2
import numpy as np
import tensorflow as tf
import json
from PIL import ImageFont, ImageDraw, Image

# í´ë˜ìŠ¤ ì´ë¦„ (í•œê¸€) ë¶ˆëŸ¬ì˜¤ê¸°
with open("skin_names.json", "r") as f:
    class_names = json.load(f)

# ëª¨ë¸ ë¡œë“œ
model = tf.keras.models.load_model("skin_model.h5")

# í•œê¸€ í°íŠ¸ ê²½ë¡œ (Ubuntuì—ì„œ ì‚¬ìš© ê°€ëŠ¥)
FONT_PATH = "/usr/share/fonts/truetype/nanum/NanumGothic.ttf"
font = ImageFont.truetype(FONT_PATH, 32)

# ì›¹ìº  ì‹œì‘
cap = cv2.VideoCapture(2)
print("Press 'q' to quit.")

while True:
    ret, frame = cap.read()
    if not ret:
        break

    # ì˜ˆì¸¡ì„ ìœ„í•œ ì „ì²˜ë¦¬
    img = cv2.resize(frame, (96, 96))
    img_array = np.expand_dims(img / 255.0, axis=0)
    pred = model.predict(img_array)[0]
    label = class_names[np.argmax(pred)]
    confidence = np.max(pred)

    # ì›ë³¸ í”„ë ˆì„ì„ Pillow ì´ë¯¸ì§€ë¡œ ë³€í™˜
    frame_rgb = cv2.cvtColor(frame, cv2.COLOR_BGR2RGB)
    pil_img = Image.fromarray(frame_rgb)
    draw = ImageDraw.Draw(pil_img)
    text = f"{label} ({confidence*100:.1f}%)"

    # í…ìŠ¤íŠ¸ ì¶œë ¥
    draw.text((10, 30), text, font=font, fill=(0, 255, 0))  # ì´ˆë¡ìƒ‰

    # ë‹¤ì‹œ OpenCV í˜•ì‹ìœ¼ë¡œ ë³€í™˜í•˜ì—¬ ì¶œë ¥
    frame_bgr = cv2.cvtColor(np.array(pil_img), cv2.COLOR_RGB2BGR)
    cv2.imshow("Skin Classifier", frame_bgr)

    if cv2.waitKey(1) & 0xFF == ord("q"):
        break

cap.release()
cv2.destroyAllWindows()
```





# AI ê¸°ë°˜ í”¼ë¶€ë³‘ ì§„ë‹¨ ì‹œìŠ¤í…œ

> **ğŸ“… ì‘ì„±ì¼**: 2025-07-06  
> **ğŸ‘¥ íŒ€**: 11ì¡° â€‘ *ì—„ì°¬í•˜, ì„ì¬í™, ê¹€ë¯¼ê·œ, ì‹ ìƒí•™*  
> **ğŸ« ê³¼ì •**: ì„œìš¸ìƒê³µíšŒì˜ì†Œ â€‘ AI ì‹œìŠ¤í…œ ë°˜ë„ì²´ ì„¤ê³„ 2ê¸°

---

## ğŸ“‹ ê°œìš”

ë³¸ í”„ë¡œì íŠ¸ëŠ” **ì¹´ë©”ë¼ ê¸°ë°˜**ìœ¼ë¡œ ì‚¬ìš©ìì˜ í”¼ë¶€ ìƒíƒœë¥¼ ì¸ì‹í•˜ê³ , **ë”¥ëŸ¬ë‹ ëª¨ë¸**ì„ í†µí•´ í”¼ë¶€ ì§ˆí™˜ì„ ìë™ ì§„ë‹¨í•˜ëŠ” **ì¸ê³µì§€ëŠ¥ ìŠ¤í‚¨ì¼€ì–´ ì‹œìŠ¤í…œ**ì„ êµ¬ì¶•í•˜ëŠ” ê²ƒì„ ëª©í‘œë¡œ í•©ë‹ˆë‹¤.

- ì‹¤ì‹œê°„ ì¹´ë©”ë¼ ì˜ìƒ ë¶„ì„ â¡ï¸ í”¼ë¶€ ë³‘ë³€ íƒì§€ ë° ë¶„ë¥˜(MNv2 ëª¨ë¸)
- **ì§ˆí™˜ classê°€ ì¼ì • ì‹œê°„ ë™ì•ˆ ìœ ì§€**ë  ë•Œë§Œ ì§„ë‹¨ í™•ì • â†’ ì˜ëª»ëœ íŒë‹¨ ìµœì†Œí™”
- ì§„ë‹¨ ê²°ê³¼ì— ë”°ë¼ **Gemma3 : 1b LLM** ìœ¼ë¡œ ê´€ë¦¬Â·ì¹˜ë£Œ ê°€ì´ë“œ ì œê³µ
- í–¥í›„ **ìŠ¤ë§ˆíŠ¸ í‚¤ì˜¤ìŠ¤í¬Â·ëª¨ë°”ì¼ ì•±** ë“± ë‹¤ì–‘í•œ í˜•íƒœë¡œ í™•ì¥ ê°€ëŠ¥

---

## ğŸ¯ ëª©í‘œ

1. **ì‹¤ì‹œê°„ AI ì§„ë‹¨ ì‹œìŠ¤í…œ** êµ¬ì¶• (ì¹´ë©”ë¼ + MNv2)
2. **í´ë˜ìŠ¤ ìœ ì§€ ì‹œê°„ ê¸°ë°˜** ì‹ ë¢°ë„ í–¥ìƒ ë¡œì§ ì ìš©
3. **ì§ˆí™˜ë³„ ê´€ë¦¬Â·ì¹˜ë£Œ íŒ** ìë™ ì•ˆë‚´ (Gemma3 : 1b)

---

## ğŸ’¡ ê¸°ëŒ€ íš¨ê³¼

- **ì¡°ê¸° ë°œê²¬**ìœ¼ë¡œ í”¼ë¶€ ì§ˆí™˜ ì•…í™” ë°©ì§€
- **ë³‘ì› ë°©ë¬¸ ì „ ìê°€ ì§„ë‹¨**Â·ê´€ë¦¬ ê°€ëŠ¥
- ì •í™•í•œ ë¶„ë¥˜ë¡œ **ë¶ˆí•„ìš”í•œ ì œí’ˆ ì†Œë¹„ ì ˆê°** ë° íš¨ìœ¨ì  í”¼ë¶€ ê´€ë¦¬

---

## ğŸ“Š ê¸°íš ë°°ê²½ & ì‹œì¥ ì¡°ì‚¬

- í”¼ë¶€ì§ˆí™˜ì€ **ì „ êµ­ë¯¼ 3ëª… ì¤‘ 1ëª…**ì´ ê²½í—˜
- ë°”ìœ í˜„ëŒ€ì¸ì€ ë¯¼ê°„ìš”ë²•Â·ê²€ìƒ‰ ì˜ì¡´ â†’ **ì˜¤ì§„Â·ì•…í™” ìœ„í—˜** ì¦ê°€
- ê¸°ì¡´ AI í”¼ë¶€ ì†”ë£¨ì…˜ì€ **ì „ë¬¸ ì¥ë¹„ ë˜ëŠ” ê³ í•´ìƒë„ ì´ë¯¸ì§€** í•„ìš” â†’ ì ‘ê·¼ì„± ë‚®ìŒ
- í•„ìš” ì¡°ê±´: **ì €ì‚¬ì–‘ ì¹´ë©”ë¼ + ê²½ëŸ‰ ëª¨ë¸**ë¡œ ëˆ„êµ¬ë‚˜ ì‚¬ìš© ê°€ëŠ¥í•œ ì„œë¹„ìŠ¤

---

## ğŸ”§ ì‹œìŠ¤í…œ êµ¬ì„± & ì£¼ìš” ê¸°ëŠ¥

### ì§„ë‹¨ í”„ë¡œì„¸ìŠ¤

```mermaid
flowchart TD
    A[ì¹´ë©”ë¼ ì‹¤í–‰] --> B[ì‹¤ì‹œê°„ ì˜ìƒ ë¶„ì„]
    B --> C[í”¼ë¶€ ì§ˆí™˜ íƒì§€]
    C --> D{ì§ˆí™˜ class 3ì´ˆ ìœ ì§€?}
    D -- Yes --> E[ì§„ë‹¨ í™•ì •]
    D -- No --> C
    E --> F[Gemma3:1bë¡œ ê´€ë¦¬Â·ì¹˜ë£Œ ì•ˆë‚´]
    F --> G[ê²°ê³¼Â·ê°€ì´ë“œ ì¶œë ¥]
```

| ë‹¨ê³„ | ì„¸ë¶€ ë‚´ìš© |
|------|-----------|
| 1ï¸âƒ£ ì¹´ë©”ë¼ ì‹¤í–‰ | ì‚¬ìš©ìëŠ” í”¼ë¶€ë¥¼ ì¹´ë©”ë¼ ì¤‘ì•™ì— ìœ„ì¹˜ |
| 2ï¸âƒ£ íƒì§€Â·ë¶„ë¥˜ | MNv2 ëª¨ë¸ë¡œ í”„ë ˆì„ë§ˆë‹¤ ì§ˆí™˜ class ì˜ˆì¸¡ |
| 3ï¸âƒ£ ì‹ ë¢°ë„ ê²€ì¦ | ë™ì¼ class **3 ì´ˆ** ìœ ì§€ ì‹œ ì§„ë‹¨ í™•ì • |
| 4ï¸âƒ£ LLM ì•ˆë‚´ | Gemma3 : 1bê°€ **5ë‹¨ê³„ ê´€ë¦¬Â·ì¹˜ë£Œ ê°€ì´ë“œ** ì œê³µ |
| 5ï¸âƒ£ ê²°ê³¼ ì¶œë ¥ | ì§ˆí™˜ëª…Â·ì„¤ëª…Â·ê´€ë¦¬ë²•Â·ë³‘ì› ê¶Œì¥ ì—¬ë¶€ í‘œì‹œ |

---

## ğŸ§  í•™ìŠµ ë°ì´í„° & ëª¨ë¸ êµ¬ì¡°

### ë°ì´í„°ì…‹
- **AI Hub â€“ í”¼ë¶€ë³„ ì¢…ì–‘** ì´ë¯¸ì§€

### ëª¨ë¸
- **ë¶„ë¥˜ê¸°**: CNN (MobileNetV2 ì „ì´í•™ìŠµ)  
- **LLM**: Gemma3 : 1b (ì§ˆí™˜ ì„¤ëª…Â·ê´€ë¦¬ ê°€ì´ë“œ ìƒì„±)

### í‰ê°€ ì§€í‘œ
- **Accuracy** (ì •í™•ë„)

### ê²½ëŸ‰í™” & ìµœì í™”
- ëª¨ë¸ì„ **ONNX** ë³€í™˜ â†’ ONNX Runtime ìµœì í™” (mem-pattern ë“±)  
- EdgeÂ·ëª¨ë°”ì¼ ê¸°ê¸°ì—ì„œë„ **ì‹¤ì‹œê°„ ì¶”ë¡ ** ê°€ëŠ¥  
- ë©”ëª¨ë¦¬Â·ì—°ì‚°ëŸ‰ â†“, ì •í™•ë„ ìœ ì§€ â†’ **ë¹ ë¥´ê³  ì‹ ë¢°ì„± ë†’ì€ ì§„ë‹¨**

---

## ğŸ› ï¸ ì‚¬ìš© ê¸°ìˆ  & ê°œë°œ í™˜ê²½

| êµ¬ë¶„ | ê¸°ìˆ  |
|------|------|
| **ì–¸ì–´** | Python |
| **í”„ë ˆì„ì›Œí¬** | TensorFlow Â· OpenCV Â· TensorFlow Lite Â· ONNX Runtime |
| **ëª¨ë¸** | Gemma3 : 1b Â· MobileNetV2 |
| **ì¸í„°í˜ì´ìŠ¤** | ì›¹ Â· í‚¤ì˜¤ìŠ¤í¬ Â· ëª¨ë°”ì¼ ì•± (í™•ì¥ ì˜ˆì •) |

---

## ğŸš€ í–¥í›„ ê³„íš

- ğŸ¥ **ë³‘ì› ì—°ê³„**: ì „ìì˜ë¬´ê¸°ë¡(EMR) ì‹œìŠ¤í…œê³¼ ì—°ë™
- ğŸ’Š **ì•½êµ­ í‚¤ì˜¤ìŠ¤í¬**: ì¦‰ì„ ì§„ë‹¨ + OTC ì œí’ˆ ì¶”ì²œ
- ğŸ“± **ëª¨ë°”ì¼ ì•±**: ê°œì¸ ë§ì¶¤í˜• í”¼ë¶€ ê´€ë¦¬ í”Œë˜ë„ˆ
- ğŸ  **í™ˆì¼€ì–´ IoT**: ìŠ¤ë§ˆíŠ¸ ê±°ìš¸Â·ì¡°ëª… ì—°ê³„
- ğŸ”¬ **ì¶”ê°€ ì§ˆí™˜ í™•ëŒ€**: ë¶„ë¥˜ class 7 â†’ 20+ ë‹¨ê³„ì  í™•ì¥

---

> **â“’ 2025 Team 11.** ë³¸ ë¬¸ì„œëŠ” í”„ë¡œì íŠ¸ ì´í•´ë¥¼ ë•ê¸° ìœ„í•œ ìš”ì•½ë³¸ì…ë‹ˆë‹¤. ìƒì—…ì  ì´ìš©Â·ë°°í¬ ì‹œ ì‚¬ì „ í—ˆê°€ê°€ í•„ìš”í•©ë‹ˆë‹¤. 



























# README
# ğŸ¥ ONNX ê¸°ë°˜ í”¼ë¶€ ì§ˆí™˜ ì§„ë‹¨ ì‹œìŠ¤í…œ

H5 ëª¨ë¸ì„ 8ë¹„íŠ¸ ì–‘ìí™”í•˜ê³  ONNX/TFLiteë¡œ ë³€í™˜í•˜ì—¬ ë” ë¹ ë¥´ê³  íš¨ìœ¨ì ì¸ í”¼ë¶€ ì§ˆí™˜ ì§„ë‹¨ ì‹œìŠ¤í…œì…ë‹ˆë‹¤.

## ğŸ“‹ ì‹œìŠ¤í…œ êµ¬ì„±

```
ğŸ“ onnx_skin_diagnosis/
â”œâ”€â”€ convert_h5_to_onnx.py          # âœ… H5 â†’ ONNX ë³€í™˜ ìŠ¤í¬ë¦½íŠ¸
â”œâ”€â”€ camera_h5_diagnosis.py         # ğŸ“± H5 ëª¨ë¸ ì¹´ë©”ë¼ ì§„ë‹¨ í”„ë¡œê·¸ë¨
â”œâ”€â”€ camera_onnx_diagnosis.py       # ğŸ“± ê¸°ë³¸ ONNX ì¹´ë©”ë¼ ì§„ë‹¨ í”„ë¡œê·¸ë¨
â”œâ”€â”€ camera_onnx_optimized.py       # ğŸš€ ìµœì í™”ëœ ONNX ì¹´ë©”ë¼ ì§„ë‹¨ í”„ë¡œê·¸ë¨
â”œâ”€â”€ README.md                      # ğŸ“– í”„ë¡œì íŠ¸ ì„¤ëª…ì„œ
â”œâ”€â”€ README_WINDOW.md               # ğŸ“– Windows ë²„ì „ ì„¤ëª…ì„œ
â”œâ”€â”€ OPTIMIZED_option.md            # ğŸ“– ìµœì í™” ì˜µì…˜ ì„¤ëª…ì„œ
â”œâ”€â”€ requirements.txt               # ğŸ“‹ í•„ìš”í•œ íŒ¨í‚¤ì§€ ëª©ë¡
â”œâ”€â”€ ğŸ“ model/                      # ğŸ§  ëª¨ë¸ ì €ì¥ì†Œ
â”‚   â”œâ”€â”€ skin_model.h5              # ì›ë³¸ Keras ëª¨ë¸ (11.4MB)
â”‚   â”œâ”€â”€ skin_model.onnx            # ë³€í™˜ëœ ONNX ëª¨ë¸ (9.6MB)
â”‚   â”œâ”€â”€ skin_model_quantized.tflite # ì–‘ìí™” TFLite ëª¨ë¸ (2.9MB)
â”‚   â”œâ”€â”€ skin_model_quantized_dynamic.onnx # ë™ì  ì–‘ìí™” ONNX (2.6MB)
â”‚   â””â”€â”€ skin_model_quantized_static.onnx  # ì •ì  ì–‘ìí™” ONNX (2.6MB)
â””â”€â”€ ğŸ“ captures/                   # ğŸ“¸ ì§„ë‹¨ ì´ë¯¸ì§€ ì €ì¥ í´ë”
```

## ğŸš€ ì„¤ì¹˜ ë° ì„¤ì •



<details>
<summary> # window </summary>
<div markdown="1">

### 1. í•„ìš”í•œ íŒ¨í‚¤ì§€ ì„¤ì¹˜

```bash
# ê¸°ë³¸ íŒ¨í‚¤ì§€
pip install -r requirements.txt

## ğŸ“‚ ì‚¬ìš© ë°©ë²•

### 1ë‹¨ê³„: ëª¨ë¸ ë³€í™˜

ë¨¼ì € H5 ëª¨ë¸ì„ ONNX/TFLiteë¡œ ë³€í™˜í•©ë‹ˆë‹¤:

```bash
python3 convert_h5_to_onnx.py
```

**ë³€í™˜ ê²°ê³¼:**
- âœ… `skin_model.onnx` - ONNX ëª¨ë¸ ìƒì„±
- âœ… `skin_model_quantized.tflite` - 8ë¹„íŠ¸ ì–‘ìí™” TFLite ëª¨ë¸ ìƒì„±
- âœ… `skin_model_quantized_dynamic.onxx` - 8ë¹„íŠ¸ ë™ì  ì–‘ìí™” onxx ëª¨ë¸ ìƒì„±
- âœ… `skin_model_static_dynamic.onxx` - 8ë¹„íŠ¸ ì •ì  ì–‘ìí™” onxx ëª¨ë¸ ìƒì„±
 

### 2ë‹¨ê³„: ì§„ë‹¨ í”„ë¡œê·¸ë¨ ì‹¤í–‰

```bash
# ollama ì‹¤í–‰ (í„°ë¯¸ë„ í•˜ë‚˜ ë” ì—´ì–´ì„œ ì§„í–‰í–‰)
ollama run gemma3:1b

```

```bash
# h5 ê¸°ë³¸
python camera_h5_diagnosis.py

# onxx ê¸°ë³¸
python camera_onnx_diagnosis.py

# onxx runtime ì ìš©
python camera_onnx_optimized.py

```
</div>
</details>


<details>
<summary> # Linux </summary>
<div markdown="1">


### 1. í•„ìš”í•œ íŒ¨í‚¤ì§€ ì„¤ì¹˜

```bash
# ê¸°ë³¸ íŒ¨í‚¤ì§€ ì„¤ì¹˜
pip install -r requirements.txt

# Pillow ìµœì‹  ë²„ì „ ì—…ê·¸ë ˆì´ë“œ (í…ìŠ¤íŠ¸ ë Œë”ë§ ì˜¤ë¥˜ ë°©ì§€ìš©)
pip install --upgrade pillow

# ë¦¬ëˆ…ìŠ¤(Ubuntu) í™˜ê²½ì—ì„œ í•œê¸€ í°íŠ¸ê°€ ê¹¨ì§ˆ ê²½ìš° ì•„ë˜ ëª…ë ¹ì–´ë¡œ ë‚˜ëˆ”ê¸€ê¼´ ì„¤ì¹˜
sudo apt update
sudo apt install fonts-nanum

ğŸ’¡fonts-nanumì€ í•œê¸€ì„ ê¹¨ì§€ì§€ ì•Šê²Œ í‘œì‹œí•˜ê¸° ìœ„í•´ í•„ìš”í•©ë‹ˆë‹¤. ì„¤ì¹˜ í›„ ì½”ë“œì—ì„œ ë‹¤ìŒê³¼ ê°™ì´ ê²½ë¡œë¥¼ ì„¤ì •í•˜ì„¸ìš”:
font_path = "/usr/share/fonts/truetype/nanum/NanumGothic.ttf"

# Ollama ì„¤ì¹˜ (Snap ê¸°ë°˜)
sudo snap install ollama

## ğŸ“‚ ì‚¬ìš© ë°©ë²•

### 1ë‹¨ê³„: ëª¨ë¸ ë³€í™˜

ë¨¼ì € H5 ëª¨ë¸ì„ ONNX/TFLiteë¡œ ë³€í™˜í•©ë‹ˆë‹¤:


```bash
python3 convert_h5_to_onnx.py
```


**ë³€í™˜ ê²°ê³¼:**
- âœ… `skin_model.onnx` - ONNX ëª¨ë¸ ìƒì„±
- âœ… `skin_model_quantized.tflite` - 8ë¹„íŠ¸ ì–‘ìí™” TFLite ëª¨ë¸ ìƒì„±
- âœ… `skin_model_quantized_dynamic.onxx` - 8ë¹„íŠ¸ ë™ì  ì–‘ìí™” onxx ëª¨ë¸ ìƒì„±
- âœ… `skin_model_static_dynamic.onxx` - 8ë¹„íŠ¸ ì •ì  ì–‘ìí™” onxx ëª¨ë¸ ìƒì„±
 


### 2ë‹¨ê³„: ì§„ë‹¨ í”„ë¡œê·¸ë¨ ì‹¤í–‰

```bash
# ollama ì‹¤í–‰ (í„°ë¯¸ë„ í•˜ë‚˜ ë” ì—´ì–´ì„œ ì§„í–‰í–‰)
ollama run gemma3:1b

```

```bash
# h5 ê¸°ë³¸
python3 camera_h5_diagnosis.py

# onxx ê¸°ë³¸
python3 camera_onnx_diagnosis.py

# onxx runtime ì ìš©
python3 camera_onnx_optimized.py

```

</div>
</details>




## ğŸ¯ ëª¨ë¸ ìš°ì„ ìˆœìœ„

í”„ë¡œê·¸ë¨ì€ ë‹¤ìŒ ìˆœì„œë¡œ ëª¨ë¸ì„ ë¡œë“œí•©ë‹ˆë‹¤:

1. **ONNX ëª¨ë¸** (ìµœìš°ì„ ) - ê°€ì¥ ë¹ ë¥¸ ì¶”ë¡  ì†ë„
2. **TFLite ëª¨ë¸** (8ë¹„íŠ¸ ì–‘ìí™”) - ì‘ì€ íŒŒì¼ í¬ê¸°, ë¹ ë¥¸ ì†ë„
3. **ì›ë³¸ H5 ëª¨ë¸** (ë°±ì—…) - ë³€í™˜ ì‹¤íŒ¨ ì‹œ ì‚¬ìš©

## ğŸ“Š ì§„ë‹¨ í´ë˜ìŠ¤ (7ê°œ)

1. **ê¸°ì €ì„¸í¬ì•”** - ê°€ì¥ í”í•œ í”¼ë¶€ì•”
2. **í‘œí”¼ë‚­ì¢…** - ì–‘ì„± ë‚­ì¢…
3. **í˜ˆê´€ì¢…** - í˜ˆê´€ ì¦ì‹ ë³‘ë³€ 
4. **ë¹„ë¦½ì¢…** - ì‘ì€ ê°ì§ˆ ì£¼ë¨¸ë‹ˆ
5. **ì •ìƒí”¼ë¶€** - ê±´ê°•í•œ í”¼ë¶€
6. **í¸í‰ì„¸í¬ì•”** - ë‘ ë²ˆì§¸ í”í•œ í”¼ë¶€ì•”
7. **ì‚¬ë§ˆê·€** - HPV ê°ì—¼

## ğŸ® ì¡°ì‘ ë°©ë²•

### ì‹¤ì‹œê°„ ëª¨ë“œ
- **ì¹´ë©”ë¼ í™”ë©´**: ì‹¤ì‹œê°„ ì˜ˆì¸¡ ê²°ê³¼ í‘œì‹œ
- **ëª¨ë¸ ì •ë³´**: ì‚¬ìš© ì¤‘ì¸ ëª¨ë¸ íƒ€ì… (ONNX/TFLite/H5) í‘œì‹œ

### ì§„ë‹¨ ëª¨ë“œ
- **'c' í‚¤**: 5ì´ˆê°„ ì—°ì† ì´¬ì˜í•˜ì—¬ ì •í™•í•œ ì§„ë‹¨ ì‹œì‘
- **ì§„ë‹¨ ê³¼ì •**: 5ë²ˆ ì´¬ì˜ â†’ ê²°ê³¼ ì¼ì¹˜ í™•ì¸ â†’ ìµœì¢… ì§„ë‹¨
- **AI ì¡°ì–¸**: Ollama Gemma3ë¥¼ í†µí•œ ê°œì¸ë§ì¶¤ ê±´ê°• ì¡°ì–¸

### ê¸°íƒ€
- **'q' í‚¤**: í”„ë¡œê·¸ë¨ ì¢…ë£Œ

## ğŸ”§ ì„±ëŠ¥ ìµœì í™”

### ëª¨ë¸ í¬ê¸° ë¹„êµ
- **ì›ë³¸ H5**: ~50MB
- **ONNX**: ~45MB (10% ê°ì†Œ)
- **TFLite ì–‘ìí™”**: ~12MB (75% ê°ì†Œ)

### ì¶”ë¡  ì†ë„
- **ONNX**: ê°€ì¥ ë¹ ë¦„ (CPU ìµœì í™”)
- **TFLite**: ë¹ ë¦„ (ë©”ëª¨ë¦¬ íš¨ìœ¨ì )
- **H5**: ë³´í†µ (TensorFlow ì˜¤ë²„í—¤ë“œ)

## ğŸ› ï¸ ë¬¸ì œ í•´ê²°

### ëª¨ë¸ ë¡œë“œ ì‹¤íŒ¨
```bash
âŒ ONNX ëª¨ë¸ ë¡œë“œ ì‹¤íŒ¨: ...
âŒ TFLite ëª¨ë¸ ë¡œë“œ ì‹¤íŒ¨: ...
âœ… ì›ë³¸ H5 ëª¨ë¸ì„ ì‚¬ìš©í•©ë‹ˆë‹¤.
```
â†’ ë³€í™˜ ê³¼ì •ì„ ë‹¤ì‹œ ì‹¤í–‰í•˜ì„¸ìš”.

### ì¹´ë©”ë¼ ì ‘ê·¼ ì‹¤íŒ¨
```bash
âŒ ì¹´ë©”ë¼ë¥¼ ì—´ ìˆ˜ ì—†ìŠµë‹ˆë‹¤.
```
â†’ ë‹¤ë¥¸ í”„ë¡œê·¸ë¨ì´ ì¹´ë©”ë¼ë¥¼ ì‚¬ìš© ì¤‘ì¸ì§€ í™•ì¸í•˜ì„¸ìš”.

### Ollama ì—°ê²° ì‹¤íŒ¨
```bash
Ollama ëª¨ë¸ì„ í˜¸ì¶œí•˜ëŠ” ì¤‘ ì˜¤ë¥˜ê°€ ë°œìƒí–ˆìŠµë‹ˆë‹¤...
```
â†’ `ollama run gemma3` ëª…ë ¹ìœ¼ë¡œ ëª¨ë¸ì„ ì‹¤í–‰í•˜ì„¸ìš”.

## ğŸ“ˆ ì¶”ê°€ ê¸°ëŠ¥

### ì´ë¯¸ì§€ ì €ì¥
- ì§„ë‹¨ ì‹œ ëª¨ë“  ìº¡ì²˜ ì´ë¯¸ì§€ê°€ `captures/` í´ë”ì— ì €ì¥ë©ë‹ˆë‹¤
- íŒŒì¼ëª…: `capture_YYYYMMDD_HHMMSS_N.png`

### AI ê±´ê°• ì¡°ì–¸
- Ollama Gemma3 ëª¨ë¸ì„ í†µí•œ ì‹¤ì‹œê°„ ê±´ê°• ì¡°ì–¸ ì œê³µ
- ê°„ê²°í•˜ê³  ì‹¤ìš©ì ì¸ ì •ë³´ ì œê³µ (200ì ë‚´ì™¸)

## âš ï¸ ì£¼ì˜ì‚¬í•­

1. **ì˜í•™ì  ì¡°ì–¸ ì•„ë‹˜**: ì´ ì‹œìŠ¤í…œì€ ì°¸ê³ ìš©ì´ë©°, ì •í™•í•œ ì§„ë‹¨ì€ ì „ë¬¸ì˜ì™€ ìƒë‹´í•˜ì„¸ìš”.
2. **ì¡°ëª… ì¡°ê±´**: ì¶©ë¶„í•œ ì¡°ëª…ì—ì„œ ì‚¬ìš©í•˜ì„¸ìš”.
3. **ì¹´ë©”ë¼ ìœ„ì¹˜**: ì§„ë‹¨ ë¶€ìœ„ë¥¼ í™”ë©´ ì¤‘ì•™ì— ìœ„ì¹˜ì‹œí‚¤ì„¸ìš”.
4. **ì •í™•ì„±**: 5ë²ˆ ì—°ì† ì´¬ì˜ì—ì„œ ê°™ì€ ê²°ê³¼ê°€ ë‚˜ì™€ì•¼ ì‹ ë¢°í•  ìˆ˜ ìˆìŠµë‹ˆë‹¤.

## ğŸ“ ê¸°ìˆ  ì§€ì›

ë¬¸ì œê°€ ë°œìƒí•˜ë©´ ë‹¤ìŒ ì‚¬í•­ì„ í™•ì¸í•´ì£¼ì„¸ìš”:

1. **Python ë²„ì „**: 3.8 ì´ìƒ ê¶Œì¥
2. **íŒ¨í‚¤ì§€ ë²„ì „**: ìµœì‹  ë²„ì „ ì‚¬ìš© ê¶Œì¥
3. **ëª¨ë¸ íŒŒì¼**: ë³€í™˜ëœ ëª¨ë¸ íŒŒì¼ì´ ì •ìƒì ìœ¼ë¡œ ìƒì„±ë˜ì—ˆëŠ”ì§€ í™•ì¸
4. **í•˜ë“œì›¨ì–´**: ì¶©ë¶„í•œ RAMê³¼ CPU ì„±ëŠ¥ í•„ìš”

---

**Â© 2024 ONNX ê¸°ë°˜ í”¼ë¶€ ì§ˆí™˜ ì§„ë‹¨ ì‹œìŠ¤í…œ** 






















# Optimized Option
# ğŸš€ ìµœì í™”ëœ ONNX Runtime ê¸°ë°˜ í”¼ë¶€ ì§ˆí™˜ ì§„ë‹¨ ì‹œìŠ¤í…œ

ONNX Runtimeì˜ ê³ ê¸‰ ìµœì í™” ê¸°ëŠ¥ì„ ëª¨ë‘ í™œìš©í•œ ì´ˆê³ ì„±ëŠ¥ í”¼ë¶€ ì§ˆí™˜ ì§„ë‹¨ ì‹œìŠ¤í…œì…ë‹ˆë‹¤.

## ğŸ¯ ì„±ëŠ¥ ìµœì í™” ê¸°ëŠ¥

### 1. ğŸ”§ ONNX Runtime ìµœì í™”
- **Graph Optimization**: ëª¨ë¸ ê·¸ë˜í”„ ìµœì í™” (`ORT_ENABLE_ALL`)
- **Memory Pattern**: ë©”ëª¨ë¦¬ íŒ¨í„´ ìµœì í™”
- **CPU Memory Arena**: CPU ë©”ëª¨ë¦¬ ì•„ë ˆë‚˜ í™œì„±í™”
- **Multi-threading**: ë¬¼ë¦¬ CPU ì½”ì–´ ìˆ˜ì— ë§ì¶˜ ìŠ¤ë ˆë“œ ìµœì í™”

### 2. ğŸ® GPU ê°€ì† ì§€ì›
- **DirectML**: Windows GPU ê°€ì† (AMD/Intel/NVIDIA)
- **CUDA**: NVIDIA GPU ê°€ì†
- **OpenVINO**: Intel GPU/VPU ê°€ì†
- **ìë™ Provider ì„ íƒ**: ìµœì ì˜ ì‹¤í–‰ í™˜ê²½ ìë™ ì„ íƒ

### 3. ğŸ“Š ë™ì  ì–‘ìí™”
- **ìë™ ì–‘ìí™”**: ì‹¤í–‰ ì‹œ ìë™ìœ¼ë¡œ 8ë¹„íŠ¸ ì–‘ìí™” ëª¨ë¸ ìƒì„±
- **ê°€ì¤‘ì¹˜ ì••ì¶•**: QUInt8 ê°€ì¤‘ì¹˜ ì••ì¶•ìœ¼ë¡œ íŒŒì¼ í¬ê¸° 50-70% ê°ì†Œ
- **ì„±ëŠ¥ í–¥ìƒ**: ì¶”ë¡  ì†ë„ 20-40% í–¥ìƒ

### 4. ğŸ”„ ë¹„ë™ê¸° ì²˜ë¦¬
- **ë¹„ë™ê¸° ì˜ˆì¸¡**: ë°±ê·¸ë¼ìš´ë“œ ìŠ¤ë ˆë“œì—ì„œ ì˜ˆì¸¡ ì‹¤í–‰
- **ë…¼ë¸”ë¡œí‚¹ UI**: ì‹¤ì‹œê°„ ì¹´ë©”ë¼ ë·° ëŠê¹€ ì—†ìŒ
- **í ê¸°ë°˜ ì²˜ë¦¬**: ì…ë ¥/ì¶œë ¥ íë¥¼ í†µí•œ íš¨ìœ¨ì  ì²˜ë¦¬

### 5. ğŸ“ˆ ì‹¤ì‹œê°„ ì„±ëŠ¥ ëª¨ë‹ˆí„°ë§
- **FPS í‘œì‹œ**: ì‹¤ì‹œê°„ í”„ë ˆì„ ë ˆì´íŠ¸ í‘œì‹œ
- **Provider ì •ë³´**: ì‚¬ìš© ì¤‘ì¸ ì‹¤í–‰ ì œê³µì í‘œì‹œ
- **ë²¤ì¹˜ë§ˆí‚¹**: 'b' í‚¤ë¡œ ì‹¤ì‹œê°„ ì„±ëŠ¥ ì¸¡ì •

## ğŸ“‹ ì‹œìŠ¤í…œ êµ¬ì„±

```
onnx_skin_diagnosis/
â”œâ”€â”€ convert_h5_to_onnx.py           # H5 â†’ ONNX/TFLite ë³€í™˜
â”œâ”€â”€ camera_onnx_diagnosis.py        # ê¸°ë³¸ ONNX ì§„ë‹¨ í”„ë¡œê·¸ë¨
â”œâ”€â”€ camera_onnx_optimized.py        # ğŸš€ ìµœì í™”ëœ ì§„ë‹¨ í”„ë¡œê·¸ë¨
â”œâ”€â”€ requirements.txt                # ê¸°ë³¸ íŒ¨í‚¤ì§€
â”œâ”€â”€ requirements_optimized.txt      # ìµœì í™” ë²„ì „ íŒ¨í‚¤ì§€
â”œâ”€â”€ README.md                       # ê¸°ë³¸ ì„¤ëª…ì„œ
â”œâ”€â”€ README_OPTIMIZED.md            # ì´ íŒŒì¼
â””â”€â”€ captures/                       # ì§„ë‹¨ ì´ë¯¸ì§€ ì €ì¥ í´ë”
```

## ğŸš€ ì„¤ì¹˜ ë° ì„¤ì •

### 1. ìµœì í™”ëœ íŒ¨í‚¤ì§€ ì„¤ì¹˜

```bash
# ê¸°ë³¸ íŒ¨í‚¤ì§€
pip install -r requirements_optimized.txt

# GPU ê°€ì† (ì„ íƒì‚¬í•­)
pip install onnxruntime-gpu        # CUDA ì§€ì›
pip install onnxruntime-directml   # DirectML ì§€ì› (Windows)
pip install onnxruntime-openvino   # OpenVINO ì§€ì› (Intel)
```

### 2. ì‹œìŠ¤í…œ ìš”êµ¬ì‚¬í•­ í™•ì¸

```bash
# ì‚¬ìš© ê°€ëŠ¥í•œ ONNX Providers í™•ì¸
python -c "import onnxruntime as ort; print(ort.get_available_providers())"

# ì‹œìŠ¤í…œ ì •ë³´ í™•ì¸
python -c "import psutil; print(f'CPU: {psutil.cpu_count()} cores, RAM: {psutil.virtual_memory().total/1024**3:.1f}GB')"
```

## ğŸ“‚ ì‚¬ìš© ë°©ë²•

### 1ë‹¨ê³„: ëª¨ë¸ ë³€í™˜ ë° ìµœì í™”

```bash
python convert_h5_to_onnx.py
```

### 2ë‹¨ê³„: ìµœì í™”ëœ ì§„ë‹¨ í”„ë¡œê·¸ë¨ ì‹¤í–‰

```bash
python camera_onnx_optimized.py
```

## ğŸ® ê³ ê¸‰ ì¡°ì‘ ë°©ë²•

### ì‹¤ì‹œê°„ ì„±ëŠ¥ ëª¨ë‹ˆí„°ë§
- **FPS í‘œì‹œ**: í™”ë©´ ì¢Œìƒë‹¨ì— ì‹¤ì‹œê°„ í”„ë ˆì„ ë ˆì´íŠ¸ í‘œì‹œ
- **Provider ì •ë³´**: ì‚¬ìš© ì¤‘ì¸ ì‹¤í–‰ ì œê³µì í‘œì‹œ
- **ì‹œìŠ¤í…œ ì •ë³´**: ì‹œì‘ ì‹œ CPU/ë©”ëª¨ë¦¬/Provider ì •ë³´ ì¶œë ¥

### í‚¤ë³´ë“œ ë‹¨ì¶•í‚¤
- **'c' í‚¤**: 5ì´ˆê°„ ì—°ì† ì§„ë‹¨ ì‹¤í–‰
- **'b' í‚¤**: ì‹¤ì‹œê°„ ë²¤ì¹˜ë§ˆí‚¹ ì‹¤í–‰ (100íšŒ ì¶”ë¡ )
- **'q' í‚¤**: í”„ë¡œê·¸ë¨ ì¢…ë£Œ

### ì„±ëŠ¥ ë²¤ì¹˜ë§ˆí‚¹
í”„ë¡œê·¸ë¨ ì‹¤í–‰ ì‹œ ìë™ìœ¼ë¡œ ì„±ëŠ¥ ë²¤ì¹˜ë§ˆí‚¹ì´ ì‹¤í–‰ë©ë‹ˆë‹¤:
```
ğŸƒ ì„±ëŠ¥ ë²¤ì¹˜ë§ˆí‚¹ ì‹œì‘ (100íšŒ ì‹¤í–‰)...
   âš¡ í‰ê·  ì¶”ë¡  ì‹œê°„: 15.2ms
   ğŸ¯ ì´ˆë‹¹ í”„ë ˆì„: 65.8 FPS
```

## ğŸ”§ ì„±ëŠ¥ ìµœì í™” ê²°ê³¼

### ëª¨ë¸ í¬ê¸° ë¹„êµ
| ëª¨ë¸ íƒ€ì… | í¬ê¸° | ì••ì¶•ë¥  |
|-----------|------|--------|
| ì›ë³¸ H5 | 50MB | - |
| ê¸°ë³¸ ONNX | 45MB | 10% ê°ì†Œ |
| ë™ì  ì–‘ìí™” ONNX | 25MB | 50% ê°ì†Œ |
| TFLite ì–‘ìí™” | 12MB | 76% ê°ì†Œ |

### ì¶”ë¡  ì„±ëŠ¥ ë¹„êµ (CPU ê¸°ì¤€)
| ëª¨ë¸ íƒ€ì… | ì¶”ë¡  ì‹œê°„ | FPS | ì„±ëŠ¥ í–¥ìƒ |
|-----------|-----------|-----|-----------|
| ì›ë³¸ H5 | 45ms | 22 FPS | - |
| ê¸°ë³¸ ONNX | 30ms | 33 FPS | 50% í–¥ìƒ |
| ìµœì í™” ONNX | 20ms | 50 FPS | 125% í–¥ìƒ |
| ë™ì  ì–‘ìí™” | 15ms | 67 FPS | 200% í–¥ìƒ |

### GPU ê°€ì† ì„±ëŠ¥ (DirectML ê¸°ì¤€)
| ëª¨ë¸ íƒ€ì… | ì¶”ë¡  ì‹œê°„ | FPS | ì„±ëŠ¥ í–¥ìƒ |
|-----------|-----------|-----|-----------|
| CPU ìµœì í™” | 15ms | 67 FPS | - |
| GPU ê°€ì† | 8ms | 125 FPS | 87% í–¥ìƒ |

## ğŸ’¡ ìµœì í™” íŒ

### 1. GPU ê°€ì† ì„¤ì •
```python
# GPU ìš°ì„ ìˆœìœ„ ì„¤ì •
providers = ['DmlExecutionProvider', 'CUDAExecutionProvider', 'CPUExecutionProvider']
```

### 2. ë©”ëª¨ë¦¬ ìµœì í™”
```python
# ë©”ëª¨ë¦¬ íŒ¨í„´ ìµœì í™”
session_options.enable_mem_pattern = True
session_options.enable_cpu_mem_arena = True
```

### 3. ìŠ¤ë ˆë“œ ìµœì í™”
```python
# CPU ì½”ì–´ ìˆ˜ì— ë§ì¶˜ ìŠ¤ë ˆë“œ ì„¤ì •
cpu_count = psutil.cpu_count(logical=False)
session_options.intra_op_num_threads = cpu_count
session_options.inter_op_num_threads = cpu_count
```

## ğŸ” ë¬¸ì œ í•´ê²°

### GPU ê°€ì† ë¬¸ì œ
```bash
# DirectML ì„¤ì¹˜ í™•ì¸
pip show onnxruntime-directml

# CUDA ì„¤ì¹˜ í™•ì¸
pip show onnxruntime-gpu
nvidia-smi  # NVIDIA GPU ìƒíƒœ í™•ì¸
```

### ì„±ëŠ¥ ë¬¸ì œ ì§„ë‹¨
```bash
# ì‹œìŠ¤í…œ ë¦¬ì†ŒìŠ¤ í™•ì¸
python -c "import psutil; print(f'CPU: {psutil.cpu_percent()}%, RAM: {psutil.virtual_memory().percent}%')"

# ë²¤ì¹˜ë§ˆí‚¹ ì‹¤í–‰ ('b' í‚¤ ë˜ëŠ” í”„ë¡œê·¸ë¨ ì‹œì‘ ì‹œ ìë™ ì‹¤í–‰)
```

### ë©”ëª¨ë¦¬ ë¶€ì¡± ë¬¸ì œ
```python
# ë°°ì¹˜ í¬ê¸° ì¡°ì • (ê¸°ë³¸ê°’: 1)
# í í¬ê¸° ì¡°ì • (ê¸°ë³¸ê°’: 2)
input_queue = queue.Queue(maxsize=1)  # ë©”ëª¨ë¦¬ ì‚¬ìš©ëŸ‰ ê°ì†Œ
```

## ğŸ¯ ì„±ëŠ¥ ëª©í‘œ

### ìµœì í™” ëª©í‘œ
- **ì¶”ë¡  ì†ë„**: 15ms ì´í•˜ (67+ FPS)
- **ë©”ëª¨ë¦¬ ì‚¬ìš©ëŸ‰**: 500MB ì´í•˜
- **ëª¨ë¸ í¬ê¸°**: 25MB ì´í•˜
- **GPU ê°€ì†**: 8ms ì´í•˜ (125+ FPS)

### ì‹¤ì œ ì„±ëŠ¥ (í…ŒìŠ¤íŠ¸ í™˜ê²½)
- **CPU**: Intel i7-12700H
- **GPU**: NVIDIA RTX 3060 / AMD Radeon Graphics
- **RAM**: 16GB DDR4
- **ì„±ëŠ¥**: 8-15ms ì¶”ë¡  ì‹œê°„, 67-125 FPS

## âš¡ ì¶”ê°€ ìµœì í™” ì˜µì…˜

### 1. ëª¨ë¸ ìµœì í™”
```bash
# ì •ì  ì–‘ìí™” (ë” ë†’ì€ ì••ì¶•ë¥ )
python -m onnxruntime.quantization.preprocess --input model.onnx --output model_optimized.onnx

# ëª¨ë¸ í”„ë£¨ë‹ (ê°€ì¤‘ì¹˜ ì œê±°)
python -m onnxruntime.transformers.models.bert.convert_to_onnx --model_path model.onnx --output pruned_model.onnx
```

### 2. í•˜ë“œì›¨ì–´ ê°€ì†
```bash
# Intel Neural Compute Stick 2
pip install openvino-dev

# Coral Edge TPU
pip install pycoral tflite-runtime

# Apple Silicon (M1/M2)
pip install onnxruntime-silicon
```

### 3. í´ë¼ìš°ë“œ ê°€ì†
```bash
# Azure Machine Learning
pip install azureml-core

# AWS SageMaker
pip install sagemaker
```

## ğŸ“Š ëª¨ë‹ˆí„°ë§ ë° í”„ë¡œíŒŒì¼ë§

### ì‹¤ì‹œê°„ ëª¨ë‹ˆí„°ë§
- **FPS**: ì‹¤ì‹œê°„ í”„ë ˆì„ ë ˆì´íŠ¸
- **ë©”ëª¨ë¦¬ ì‚¬ìš©ëŸ‰**: ì‹œìŠ¤í…œ ë©”ëª¨ë¦¬ ëª¨ë‹ˆí„°ë§
- **Provider ìƒíƒœ**: ì‚¬ìš© ì¤‘ì¸ ì‹¤í–‰ ì œê³µì

### ì„±ëŠ¥ í”„ë¡œíŒŒì¼ë§
```bash
# ìƒì„¸ í”„ë¡œíŒŒì¼ë§
python -m cProfile -o profile.stats camera_onnx_optimized.py

# í”„ë¡œíŒŒì¼ ë¶„ì„
python -c "import pstats; p = pstats.Stats('profile.stats'); p.sort_stats('cumulative').print_stats(10)"
```

---

**ğŸš€ ìµœì í™”ëœ ONNX Runtimeìœ¼ë¡œ ìµœê³  ì„±ëŠ¥ì˜ í”¼ë¶€ ì§ˆí™˜ ì§„ë‹¨ì„ ê²½í—˜í•˜ì„¸ìš”!** 






































# requirements.txt
```bash
# ìµœì í™”ëœ ONNX ê¸°ë°˜ í”¼ë¶€ ì§ˆí™˜ ì§„ë‹¨ ì‹œìŠ¤í…œ í•„ìˆ˜ íŒ¨í‚¤ì§€

# ë¨¸ì‹ ëŸ¬ë‹ ë° ë”¥ëŸ¬ë‹
tensorflow>=2.10.0
numpy>=1.21.0

# ONNX ê´€ë ¨ (ìµœì í™” ë²„ì „)
onnxruntime>=1.16.0
onnxruntime-gpu>=1.16.0  # GPU ì§€ì› (CUDA/DirectML)
tf2onnx>=1.14.0
onnx>=1.14.0

# ì»´í“¨í„° ë¹„ì „
opencv-python>=4.7.0
Pillow>=9.0.0

# ì‹œìŠ¤í…œ ëª¨ë‹ˆí„°ë§ ë° ìµœì í™”
psutil>=5.9.0

# AI ê±´ê°• ì¡°ì–¸ (ì„ íƒì‚¬í•­)
ollama>=0.1.0

# ì„±ëŠ¥ ìµœì í™” (ì„ íƒì‚¬í•­)
# onnxruntime-openvino  # Intel OpenVINO ì§€ì›
# onnxruntime-directml  # DirectML ì§€ì› (Windows) 
```







## camera_h5_diagnosis.py
```python
import numpy as np
import cv2
from PIL import ImageFont, ImageDraw, Image
import time
import os
import ollama
import onnxruntime as ort

# --- ì„¤ì • ---
CAPTURE_INTERVAL = 1  # ìº¡ì²˜ ê°„ê²© (ì´ˆ)
CAPTURE_COUNT = 5     # ìº¡ì²˜ íšŸìˆ˜
CAPTURE_FOLDER = "captures" # ìº¡ì²˜ ì´ë¯¸ì§€ ì €ì¥ í´ë”
OLLAMA_MODEL = "gemma3:1b" # ì‚¬ìš©í•  Ollama ëª¨ë¸

DISPLAY_UPDATE_INTERVAL_MS = 400 # í™”ë©´ì— í‘œì‹œë˜ëŠ” ì˜ˆì¸¡ ê²°ê³¼ ì—…ë°ì´íŠ¸ ì£¼ê¸° (ë°€ë¦¬ì´ˆ)

# ëª¨ë¸ ê²½ë¡œ ì„¤ì •
ONNX_MODEL_PATH = "./model/skin_model.onnx"
TFLITE_MODEL_PATH = "./model/skin_model_quantized.tflite"

# --- í´ë˜ìŠ¤ ë° ëª¨ë¸ ì„¤ì • ---
# í´ë˜ìŠ¤ëª… (7ê°œ í´ë˜ìŠ¤)
class_names_kr = [
    'ê¸°ì €ì„¸í¬ì•”',
    'í‘œí”¼ë‚­ì¢…',
    'í˜ˆê´€ì¢…',
    'ë¹„ë¦½ì¢…',
    'ì •ìƒí”¼ë¶€',
    'í¸í‰ì„¸í¬ì•”',
    'ì‚¬ë§ˆê·€'
]

# --- ONNX ëª¨ë¸ í´ë˜ìŠ¤ ---
class ONNXModel:
    def __init__(self, model_path):
        """ONNX ëª¨ë¸ ë¡œë“œ"""
        self.session = ort.InferenceSession(model_path)
        self.input_name = self.session.get_inputs()[0].name
        self.output_name = self.session.get_outputs()[0].name
        print(f"ONNX ëª¨ë¸ ë¡œë“œ ì„±ê³µ: {model_path}")
    
    def predict(self, input_data):
        """ONNX ëª¨ë¸ ì˜ˆì¸¡"""
        result = self.session.run([self.output_name], {self.input_name: input_data})
        return result[0]

# --- TFLite ëª¨ë¸ í´ë˜ìŠ¤ ---
class TFLiteModel:
    def __init__(self, model_path):
        """TFLite ëª¨ë¸ ë¡œë“œ"""
        import tensorflow as tf
        self.interpreter = tf.lite.Interpreter(model_path=model_path)
        self.interpreter.allocate_tensors()
        
        # ì…ë ¥ ë° ì¶œë ¥ í…ì„œ ì •ë³´
        self.input_details = self.interpreter.get_input_details()
        self.output_details = self.interpreter.get_output_details()
        print(f"TFLite ëª¨ë¸ ë¡œë“œ ì„±ê³µ: {model_path}")
    
    def predict(self, input_data):
        """TFLite ëª¨ë¸ ì˜ˆì¸¡"""
        # ì…ë ¥ ë°ì´í„° ì„¤ì •
        self.interpreter.set_tensor(self.input_details[0]['index'], input_data)
        
        # ì¶”ë¡  ì‹¤í–‰
        self.interpreter.invoke()
        
        # ì¶œë ¥ ë°ì´í„° ê°€ì ¸ì˜¤ê¸°
        output_data = self.interpreter.get_tensor(self.output_details[0]['index'])
        return output_data


def get_solution_from_gemma(disease_name):
    """
    ë¡œì»¬ Ollamaì˜ Gemma3 ëª¨ë¸ì—ê²Œ í”¼ë¶€ ì§ˆí™˜ì— ëŒ€í•œ ê°„ë‹¨í•œ ê°€ì´ë“œ ìš”ì²­.
    ì‘ë‹µì€ ì‚¬ìš©ìê°€ ì´í•´í•˜ê¸° ì‰½ê²Œ 5ë‹¨ê³„ë¡œ ìš”ì•½ë˜ë©°, 200ì ë‚´ì™¸ë¡œ ì œí•œë¨.
    """

    prompt = f"""
ë‹¹ì‹ ì€ í”¼ë¶€ ê±´ê°• ì „ë¬¸ AI ì–´ì‹œìŠ¤í„´íŠ¸ì…ë‹ˆë‹¤. ì•„ë˜ í”¼ë¶€ ì§ˆí™˜ì— ëŒ€í•´ 200ì ë‚´ì™¸ë¡œ ê°„ê²°í•˜ê²Œ ì•ˆë‚´í•´ì£¼ì„¸ìš”.

í”¼ë¶€ ì§ˆí™˜ëª…: {disease_name}

ì•„ë˜ í˜•ì‹ì— ë”°ë¼ í•œêµ­ì–´ë¡œ ì •í™•í•˜ê³  ê°„ë‹¨ëª…ë£Œí•˜ê²Œ ì‘ì„±í•˜ì„¸ìš”:

1. ì§ˆí™˜ ì„¤ëª…: ì¼ë°˜ì¸ì´ ì´í•´í•  ìˆ˜ ìˆë„ë¡ ê°„ë‹¨íˆ
2. ì¦‰ì‹œ ì¡°ì¹˜ì‚¬í•­: ì‘ê¸‰ì„± ì—¬ë¶€ í¬í•¨
3. ê°€ì • ê´€ë¦¬ ë°©ë²•: ì†ì‰½ê²Œ ì‹¤ì²œ ê°€ëŠ¥í•œ íŒ
4. ì „ë¬¸ ì¹˜ë£Œ ë°©ë²•: ë³‘ì›ì—ì„œ ë°›ì„ ìˆ˜ ìˆëŠ” ì¹˜ë£Œ
5. ì£¼ì˜ì‚¬í•­: ì¬ë°œ, ê°ì—¼, ìê°€ ì¹˜ë£Œ ê²½ê³  ë“±


ê° í•­ëª©ì€ ì¤„ë°”ê¿ˆìœ¼ë¡œ êµ¬ë¶„í•˜ì—¬ ì œì‹œí•˜ì„¸ìš”.
ë‹µë³€ì€ 200ì ë‚´ì™¸ë¡œ ê°„ê²°í•˜ê²Œ ì‘ì„±í•´ì£¼ì„¸ìš”.
    """.strip()

    print(f"\n[{OLLAMA_MODEL} ëª¨ë¸ì—ê²Œ ì¡°ì–¸ì„ ìš”ì²­í•©ë‹ˆë‹¤... ì ì‹œë§Œ ê¸°ë‹¤ë ¤ ì£¼ì„¸ìš”.]")

    try:
        response = ollama.chat(
            model=OLLAMA_MODEL,
            messages=[{'role': 'user', 'content': prompt}]
        )
        return response['message']['content'].strip()

    except Exception as e:
        return f"[ì˜¤ë¥˜] Ollama ëª¨ë¸ì„ í˜¸ì¶œí•˜ëŠ” ì¤‘ ë¬¸ì œê°€ ë°œìƒí–ˆìŠµë‹ˆë‹¤: {e}\nOllama ì„œë²„ê°€ ì‹¤í–‰ ì¤‘ì¸ì§€ í™•ì¸í•˜ì„¸ìš”."


# --- ëª¨ë¸ ì´ˆê¸°í™” í•¨ìˆ˜ ---
def initialize_model():
    """ì‚¬ìš© ê°€ëŠ¥í•œ ëª¨ë¸ì„ ì´ˆê¸°í™”í•©ë‹ˆë‹¤."""
    model = None
    model_type = None
    
    # 1. ONNX ëª¨ë¸ ì‹œë„
    if os.path.exists(ONNX_MODEL_PATH):
        try:
            model = ONNXModel(ONNX_MODEL_PATH)
            model_type = "ONNX"
            print("ONNX ëª¨ë¸ì„ ì‚¬ìš©í•©ë‹ˆë‹¤.")
        except Exception as e:
            print(f"ONNX ëª¨ë¸ ë¡œë“œ ì‹¤íŒ¨: {e}")
    
    # 2. TFLite ëª¨ë¸ ì‹œë„ (ONNX ì‹¤íŒ¨ ì‹œ)
    if model is None and os.path.exists(TFLITE_MODEL_PATH):
        try:
            model = TFLiteModel(TFLITE_MODEL_PATH)
            model_type = "TFLite"
            print("TFLite ëª¨ë¸ì„ ì‚¬ìš©í•©ë‹ˆë‹¤.")
        except Exception as e:
            print(f"TFLite ëª¨ë¸ ë¡œë“œ ì‹¤íŒ¨: {e}")
    
    # 3. ì›ë³¸ H5 ëª¨ë¸ ì‹œë„ (ë‘˜ ë‹¤ ì‹¤íŒ¨ ì‹œ)
    if model is None:
        try:
            import tensorflow as tf
            from tensorflow import keras
            h5_model_path = "C:/Users/kccistc/project/onnx_skin_diagnosis/model/skin_model.h5"
            model = keras.models.load_model(h5_model_path)
            model_type = "H5"
            print("ì›ë³¸ H5 ëª¨ë¸ì„ ì‚¬ìš©í•©ë‹ˆë‹¤.")
        except Exception as e:
            print(f"H5 ëª¨ë¸ ë¡œë“œ ì‹¤íŒ¨: {e}")
    
    return model, model_type
# --- TTS ---
from gtts import gTTS
import os

def speak_korean_gtts(text):
    try:
        tts = gTTS(text=text, lang='ko', slow=False)
        original = "tts_output_original.mp3"
        faster = "tts_output_fast.mp3"

        tts.save(original)

        # ğŸ› ï¸ ffmpegë¡œ ì†ë„ 1.5ë°° ë¹ ë¥´ê²Œ ë³€í™˜ (tempo=1.5)
        os.system(f"ffmpeg -y -i {original} -filter:a 'atempo=1.5' {faster}")

        # ğŸš ì¬ìƒ
        os.system(f"mpg123 {faster}")

        # ğŸ§¹ ì •ë¦¬
        os.remove(original)
        os.remove(faster)
        print(f"[ğŸ§¹] mp3 íŒŒì¼ ìë™ ì‚­ì œ ì™„ë£Œ..")

    except Exception as e:
        print(f"[TTS ì˜¤ë¥˜] {e}")


# --- ë©”ì¸ ë¡œì§ ---
def main():
    print("ONNX Skin Diagnosis System")
    print("=" * 50)
    
    # ìº¡ì²˜ í´ë” ìƒì„±
    if not os.path.exists(CAPTURE_FOLDER):
        os.makedirs(CAPTURE_FOLDER)
    
    # ëª¨ë¸ ì´ˆê¸°í™”
    model, model_type = initialize_model()
    if model is None:
        print("ì‚¬ìš© ê°€ëŠ¥í•œ ëª¨ë¸ì´ ì—†ìŠµë‹ˆë‹¤.")
        print("ë¨¼ì € convert_h5_to_onnx.pyë¥¼ ì‹¤í–‰í•˜ì—¬ ëª¨ë¸ì„ ë³€í™˜í•˜ì„¸ìš”.")
        return
    
    print(f"ì‚¬ìš© ì¤‘ì¸ ëª¨ë¸: {model_type}")
    
    # í°íŠ¸ ì„¤ì •
    font_path = "/usr/share/fonts/truetype/noto/NotoSansCJK-Regular.ttc"
    try:
        font = ImageFont.truetype(font_path, 20)
    except IOError:
        print(f"ì˜¤ë¥˜: í°íŠ¸ íŒŒì¼ì„ ì°¾ì„ ìˆ˜ ì—†ìŠµë‹ˆë‹¤: {font_path}. ê¸°ë³¸ í°íŠ¸ë¥¼ ì‚¬ìš©í•©ë‹ˆë‹¤.")
        font = ImageFont.load_default()

    # í™”ë©´ í‘œì‹œ ì—…ë°ì´íŠ¸ë¥¼ ìœ„í•œ ë³€ìˆ˜
    last_display_update_time = time.time()
    current_display_label = ""

    # ì¹´ë©”ë¼ ì„¤ì •
    cap = cv2.VideoCapture(1) # ì™¸ë¶€ ì›¹ìº 
    if not cap.isOpened():
        cap = cv2.VideoCapture(0) # ë‚´ì¥ ì›¹ìº 
        if not cap.isOpened():
            print("ì¹´ë©”ë¼ë¥¼ ì—´ ìˆ˜ ì—†ìŠµë‹ˆë‹¤.")
            return

    print("ì¹´ë©”ë¼ê°€ ì¤€ë¹„ë˜ì—ˆìŠµë‹ˆë‹¤.")
    print("í™”ë©´ì„ ë³´ë©° ì§„ë‹¨í•  ë¶€ìœ„ë¥¼ ì¤‘ì•™ì— ìœ„ì¹˜ì‹œí‚¤ì„¸ìš”.")
    print("í‚¤ë³´ë“œ 'c'ë¥¼ ëˆ„ë¥´ë©´ 5ì´ˆê°„ ì—°ì†ìœ¼ë¡œ ì´¬ì˜í•˜ì—¬ ì§„ë‹¨í•©ë‹ˆë‹¤.")
    print("í‚¤ë³´ë“œ 'q'ë¥¼ ëˆ„ë¥´ë©´ í”„ë¡œê·¸ë¨ì„ ì¢…ë£Œí•©ë‹ˆë‹¤.")

    while True:
        ret, frame = cap.read()
        if not ret:
            print("ì˜¤ë¥˜: ì¹´ë©”ë¼ì—ì„œ í”„ë ˆì„ì„ ì½ì„ ìˆ˜ ì—†ìŠµë‹ˆë‹¤.")
            break

        # ì¤‘ì•™ 1:1 ì˜ì—­ crop
        h, w, _ = frame.shape
        min_dim = min(h, w)
        start_x = (w - min_dim) // 2
        start_y = (h - min_dim) // 2
        crop_frame = frame[start_y:start_y+min_dim, start_x:start_x+min_dim]

        # --- ì‹¤ì‹œê°„ ì˜ˆì¸¡ ---
        # ì´ë¯¸ì§€ ì „ì²˜ë¦¬
        img_array = cv2.resize(crop_frame, (96, 96))
        img_array = np.expand_dims(img_array, axis=0)  # ë°°ì¹˜ ì°¨ì› ì¶”ê°€
        img_array = img_array.astype(np.float32) / 255.0  # ì •ê·œí™”

        # ëª¨ë¸ íƒ€ì…ì— ë”°ë¥¸ ì˜ˆì¸¡
        if model_type == "H5":
            predictions = model.predict(img_array, verbose=0)
        else:
            predictions = model.predict(img_array)
        
        predicted_class_idx = np.argmax(predictions[0])
        confidence = predictions[0][predicted_class_idx]

        # ê²°ê³¼ í…ìŠ¤íŠ¸ ìƒì„±
        current_label = f"{class_names_kr[predicted_class_idx]} ({confidence*100:.1f}%)"
        
        # í™”ë©´ í‘œì‹œ ì—…ë°ì´íŠ¸ ì£¼ê¸° ì œì–´
        current_time = time.time()
        if (current_time - last_display_update_time) * 1000 >= DISPLAY_UPDATE_INTERVAL_MS:
            current_display_label = current_label
            last_display_update_time = current_time

        # í™”ë©´ì— í‘œì‹œ (Pillow ì‚¬ìš©)
        img_pil = Image.fromarray(cv2.cvtColor(crop_frame, cv2.COLOR_BGR2RGB))
        draw = ImageDraw.Draw(img_pil)
        draw.text((10, 10), f"ì‹¤ì‹œê°„ ì˜ˆì¸¡ ({model_type}):", font=font, fill=(0, 255, 0))
        draw.text((10, 35), current_display_label, font=font, fill=(0, 255, 0))
        display_frame = cv2.cvtColor(np.array(img_pil), cv2.COLOR_RGB2BGR)

        cv2.imshow('ONNX Skin Disease Diagnosis', display_frame)

        key = cv2.waitKey(1) & 0xFF

        # --- 'c' í‚¤ë¥¼ ëˆŒëŸ¬ ì—°ì† ìº¡ì²˜ ë° ì§„ë‹¨ ---
        if key == ord('c'):
            # í™”ë©´ì„ ê²€ê²Œ ë§Œë“¤ê³  "ì˜ì‚¬ì˜ ë‹µë³€ ì¤€ë¹„ì¤‘..." ë©”ì‹œì§€ í‘œì‹œ
            black_screen = np.zeros_like(display_frame)
            
            # Pillowë¥¼ ì‚¬ìš©í•˜ì—¬ í…ìŠ¤íŠ¸ ì¶”ê°€
            img_pil_black = Image.fromarray(cv2.cvtColor(black_screen, cv2.COLOR_BGR2RGB))
            draw_black = ImageDraw.Draw(img_pil_black)
            
            text = "ì˜ì‚¬ì˜ ë‹µë³€ ì¤€ë¹„ì¤‘..."
            
            # í…ìŠ¤íŠ¸ í¬ê¸° ê³„ì‚°
            try:
                # Pillow 10.0.0 ì´ìƒ
                text_bbox = draw_black.textbbox((0, 0), text, font=font)
                text_width = text_bbox[2] - text_bbox[0]
                text_height = text_bbox[3] - text_bbox[1]
            except AttributeError:
                # ì´ì „ ë²„ì „ì˜ Pillow
                text_width, text_height = draw_black.textsize(text, font=font)

            text_x = (black_screen.shape[1] - text_width) // 2
            text_y = (black_screen.shape[0] - text_height) // 2
            
            draw_black.text((text_x, text_y), text, font=font, fill=(255, 255, 255))
            
            # OpenCV í˜•ì‹ìœ¼ë¡œ ë‹¤ì‹œ ë³€í™˜í•˜ì—¬ í‘œì‹œ
            black_screen_with_text = cv2.cvtColor(np.array(img_pil_black), cv2.COLOR_RGB2BGR)
            cv2.imshow('ONNX Skin Disease Diagnosis', black_screen_with_text)
            cv2.waitKey(1) # í™”ë©´ì„ ì¦‰ì‹œ ì—…ë°ì´íŠ¸

            print("\n" + "="*40)
            print(f"ì§„ë‹¨ì„ ì‹œì‘í•©ë‹ˆë‹¤. {CAPTURE_COUNT}ì´ˆ ë™ì•ˆ {CAPTURE_COUNT}ë²ˆ ì´¬ì˜í•©ë‹ˆë‹¤.")
            print("="*40)
            
            captured_classes = []
            
            for i in range(CAPTURE_COUNT):
                time.sleep(CAPTURE_INTERVAL)
                
                # í˜„ì¬ í”„ë ˆì„(crop_frame)ìœ¼ë¡œ ì˜ˆì¸¡
                current_img_array = cv2.resize(crop_frame, (96, 96))
                current_img_array = np.expand_dims(current_img_array, axis=0)
                current_img_array = current_img_array.astype(np.float32) / 255.0

                # ëª¨ë¸ íƒ€ì…ì— ë”°ë¥¸ ì˜ˆì¸¡
                if model_type == "H5":
                    current_predictions = model.predict(current_img_array, verbose=0)
                else:
                    current_predictions = model.predict(current_img_array)
                
                current_predicted_idx = np.argmax(current_predictions[0])
                
                predicted_name = class_names_kr[current_predicted_idx]
                captured_classes.append(predicted_name)
                
                # ìº¡ì²˜ ì´ë¯¸ì§€ ì €ì¥
                timestamp = time.strftime("%Y%m%d_%H%M%S")
                capture_path = os.path.join(CAPTURE_FOLDER, f"capture_{timestamp}_{i+1}.png")
                cv2.imwrite(capture_path, crop_frame)
                
                print(f"ì´¬ì˜ {i+1}/5... ì˜ˆì¸¡: {predicted_name} (ì´ë¯¸ì§€ ì €ì¥: {capture_path})")

            # --- ìµœì¢… ì§„ë‹¨ ---
            print("\n" + "-"*40)
            if len(set(captured_classes)) == 1:
                final_diagnosis = captured_classes[0]
                print(f"ìµœì¢… ì§„ë‹¨ ê²°ê³¼: **{final_diagnosis}**")
                print(f"ì‚¬ìš© ëª¨ë¸: {model_type}")
                print("-"*40)
                
                # Gemma3 í•´ê²°ì±… ìš”ì²­
                solution = get_solution_from_gemma(final_diagnosis)
                print("\n[Ollama Gemma3ì˜ ê±´ê°• ì¡°ì–¸]")
                print(solution)
                print("\n(ì£¼ì˜: ì´ ì •ë³´ëŠ” ì°¸ê³ ìš©ì´ë©°, ì •í™•í•œ ì§„ë‹¨ê³¼ ì¹˜ë£Œë¥¼ ìœ„í•´ ë°˜ë“œì‹œ ì „ë¬¸ ì˜ë£Œê¸°ê´€ì„ ë°©ë¬¸í•˜ì„¸ìš”.)")
                speak_korean_gtts(solution) # TTS ìŒì„± ì¶œë ¥
                
            else:
                print("ì§„ë‹¨ ì‹¤íŒ¨: ì˜ˆì¸¡ ê²°ê³¼ê°€ ì¼ì¹˜í•˜ì§€ ì•ŠìŠµë‹ˆë‹¤.")
                print(f"ì§€ë‚œ {CAPTURE_COUNT}ë²ˆì˜ ì˜ˆì¸¡: {captured_classes}")
            
            print("="*40)
            print("\në‹¤ì‹œ ì§„ë‹¨í•˜ë ¤ë©´ 'c'ë¥¼, ì¢…ë£Œí•˜ë ¤ë©´ 'q'ë¥¼ ëˆ„ë¥´ì„¸ìš”.")

        # --- 'q' í‚¤ë¥¼ ëˆŒëŸ¬ ì¢…ë£Œ ---
        elif key == ord('q'):
            print("í”„ë¡œê·¸ë¨ì„ ì¢…ë£Œí•©ë‹ˆë‹¤.")
            break

    cap.release()
    cv2.destroyAllWindows()

if __name__ == "__main__":
    main()
```



## camera_onnx_diagnosis.py
```python
import numpy as np
import cv2
from PIL import ImageFont, ImageDraw, Image
import time
import os
import ollama
import onnxruntime as ort

# --- ì„¤ì • ---
CAPTURE_INTERVAL = 1  # ìº¡ì²˜ ê°„ê²© (ì´ˆ)
CAPTURE_COUNT = 5     # ìº¡ì²˜ íšŸìˆ˜
CAPTURE_FOLDER = "captures" # ìº¡ì²˜ ì´ë¯¸ì§€ ì €ì¥ í´ë”
OLLAMA_MODEL = "gemma3:1b" # ì‚¬ìš©í•  Ollama ëª¨ë¸

# ëª¨ë¸ ê²½ë¡œ ì„¤ì •
ONNX_MODEL_PATH = "./model/skin_model.onnx"
TFLITE_MODEL_PATH = "./model/skin_model_quantized.tflite"

# --- í´ë˜ìŠ¤ ë° ëª¨ë¸ ì„¤ì • ---
# í´ë˜ìŠ¤ëª… (7ê°œ í´ë˜ìŠ¤)
class_names_kr = [
    'ê¸°ì €ì„¸í¬ì•”',
    'í‘œí”¼ë‚­ì¢…',
    'í˜ˆê´€ì¢…',
    'ë¹„ë¦½ì¢…',
    'ì •ìƒí”¼ë¶€',
    'í¸í‰ì„¸í¬ì•”',
    'ì‚¬ë§ˆê·€'
]

# --- ONNX ëª¨ë¸ í´ë˜ìŠ¤ ---
class ONNXModel:
    def __init__(self, model_path):
        """ONNX ëª¨ë¸ ë¡œë“œ"""
        self.session = ort.InferenceSession(model_path)
        self.input_name = self.session.get_inputs()[0].name
        self.output_name = self.session.get_outputs()[0].name
        print(f"ONNX ëª¨ë¸ ë¡œë“œ ì„±ê³µ: {model_path}")
    
    def predict(self, input_data):
        """ONNX ëª¨ë¸ ì˜ˆì¸¡"""
        result = self.session.run([self.output_name], {self.input_name: input_data})
        return result[0]

# --- TFLite ëª¨ë¸ í´ë˜ìŠ¤ ---
class TFLiteModel:
    def __init__(self, model_path):
        """TFLite ëª¨ë¸ ë¡œë“œ"""
        import tensorflow as tf
        self.interpreter = tf.lite.Interpreter(model_path=model_path)
        self.interpreter.allocate_tensors()
        
        # ì…ë ¥ ë° ì¶œë ¥ í…ì„œ ì •ë³´
        self.input_details = self.interpreter.get_input_details()
        self.output_details = self.interpreter.get_output_details()
        print(f"TFLite ëª¨ë¸ ë¡œë“œ ì„±ê³µ: {model_path}")
    
    def predict(self, input_data):
        """TFLite ëª¨ë¸ ì˜ˆì¸¡"""
        # ì…ë ¥ ë°ì´í„° ì„¤ì •
        self.interpreter.set_tensor(self.input_details[0]['index'], input_data)
        
        # ì¶”ë¡  ì‹¤í–‰
        self.interpreter.invoke()
        
        # ì¶œë ¥ ë°ì´í„° ê°€ì ¸ì˜¤ê¸°
        output_data = self.interpreter.get_tensor(self.output_details[0]['index'])
        return output_data


def get_solution_from_gemma(disease_name):
    """
    ë¡œì»¬ Ollamaì˜ Gemma3 ëª¨ë¸ì—ê²Œ í”¼ë¶€ ì§ˆí™˜ì— ëŒ€í•œ ê°„ë‹¨í•œ ê°€ì´ë“œ ìš”ì²­.
    ì‘ë‹µì€ ì‚¬ìš©ìê°€ ì´í•´í•˜ê¸° ì‰½ê²Œ 5ë‹¨ê³„ë¡œ ìš”ì•½ë˜ë©°, 200ì ë‚´ì™¸ë¡œ ì œí•œë¨.
    """

    prompt = f"""
ë‹¹ì‹ ì€ í”¼ë¶€ ê±´ê°• ì „ë¬¸ AI ì–´ì‹œìŠ¤í„´íŠ¸ì…ë‹ˆë‹¤. ì•„ë˜ í”¼ë¶€ ì§ˆí™˜ì— ëŒ€í•´ 200ì ë‚´ì™¸ë¡œ ê°„ê²°í•˜ê²Œ ì•ˆë‚´í•´ì£¼ì„¸ìš”.

í”¼ë¶€ ì§ˆí™˜ëª…: {disease_name}

ì•„ë˜ í˜•ì‹ì— ë”°ë¼ í•œêµ­ì–´ë¡œ ì •í™•í•˜ê³  ê°„ë‹¨ëª…ë£Œí•˜ê²Œ ì‘ì„±í•˜ì„¸ìš”:

1. ì§ˆí™˜ ì„¤ëª…: ì¼ë°˜ì¸ì´ ì´í•´í•  ìˆ˜ ìˆë„ë¡ ê°„ë‹¨íˆ
2. ì¦‰ì‹œ ì¡°ì¹˜ì‚¬í•­: ì‘ê¸‰ì„± ì—¬ë¶€ í¬í•¨
3. ê°€ì • ê´€ë¦¬ ë°©ë²•: ì†ì‰½ê²Œ ì‹¤ì²œ ê°€ëŠ¥í•œ íŒ
4. ì „ë¬¸ ì¹˜ë£Œ ë°©ë²•: ë³‘ì›ì—ì„œ ë°›ì„ ìˆ˜ ìˆëŠ” ì¹˜ë£Œ
5. ì£¼ì˜ì‚¬í•­: ì¬ë°œ, ê°ì—¼, ìê°€ ì¹˜ë£Œ ê²½ê³  ë“±


ê° í•­ëª©ì€ ì¤„ë°”ê¿ˆìœ¼ë¡œ êµ¬ë¶„í•˜ì—¬ ì œì‹œí•˜ì„¸ìš”.
ë‹µë³€ì€ 200ì ë‚´ì™¸ë¡œ ê°„ê²°í•˜ê²Œ ì‘ì„±í•´ì£¼ì„¸ìš”.
    """.strip()
        
    print(f"\n[{OLLAMA_MODEL} ëª¨ë¸ì—ê²Œ ì¡°ì–¸ì„ ìš”ì²­í•©ë‹ˆë‹¤... ì ì‹œë§Œ ê¸°ë‹¤ë ¤ ì£¼ì„¸ìš”.]")

    try:
        response = ollama.chat(
            model=OLLAMA_MODEL,
            messages=[{'role': 'user', 'content': prompt}]
        )
        return response['message']['content'].strip()

    except Exception as e:
        return f"[ì˜¤ë¥˜] Ollama ëª¨ë¸ì„ í˜¸ì¶œí•˜ëŠ” ì¤‘ ë¬¸ì œê°€ ë°œìƒí–ˆìŠµë‹ˆë‹¤: {e}\nOllama ì„œë²„ê°€ ì‹¤í–‰ ì¤‘ì¸ì§€ í™•ì¸í•˜ì„¸ìš”."


# --- ëª¨ë¸ ì´ˆê¸°í™” í•¨ìˆ˜ ---
def initialize_model():
    """ì‚¬ìš© ê°€ëŠ¥í•œ ëª¨ë¸ì„ ì´ˆê¸°í™”í•©ë‹ˆë‹¤."""
    model = None
    model_type = None
    
    # 1. ONNX ëª¨ë¸ ì‹œë„
    if os.path.exists(ONNX_MODEL_PATH):
        try:
            model = ONNXModel(ONNX_MODEL_PATH)
            model_type = "ONNX"
            print("ONNX ëª¨ë¸ì„ ì‚¬ìš©í•©ë‹ˆë‹¤.")
        except Exception as e:
            print(f"ONNX ëª¨ë¸ ë¡œë“œ ì‹¤íŒ¨: {e}")
    
    # 2. TFLite ëª¨ë¸ ì‹œë„ (ONNX ì‹¤íŒ¨ ì‹œ)
    if model is None and os.path.exists(TFLITE_MODEL_PATH):
        try:
            model = TFLiteModel(TFLITE_MODEL_PATH)
            model_type = "TFLite"
            print("TFLite ëª¨ë¸ì„ ì‚¬ìš©í•©ë‹ˆë‹¤.")
        except Exception as e:
            print(f"TFLite ëª¨ë¸ ë¡œë“œ ì‹¤íŒ¨: {e}")
    
    # 3. ì›ë³¸ H5 ëª¨ë¸ ì‹œë„ (ë‘˜ ë‹¤ ì‹¤íŒ¨ ì‹œ)
    if model is None:
        try:
            import tensorflow as tf
            from tensorflow import keras
            h5_model_path = "./model/skin_model.h5"
            model = keras.models.load_model(h5_model_path)
            model_type = "H5"
            print("ì›ë³¸ H5 ëª¨ë¸ì„ ì‚¬ìš©í•©ë‹ˆë‹¤.")
        except Exception as e:
            print(f"H5 ëª¨ë¸ ë¡œë“œ ì‹¤íŒ¨: {e}")
    
    return model, model_type
# --- TTS ---
from gtts import gTTS
import os

def speak_korean_gtts(text):
    try:
        tts = gTTS(text=text, lang='ko', slow=False)
        original = "tts_output_original.mp3"
        faster = "tts_output_fast.mp3"

        tts.save(original)

        # ğŸ› ï¸ ffmpegë¡œ ì†ë„ 1.5ë°° ë¹ ë¥´ê²Œ ë³€í™˜ (tempo=1.5)
        os.system(f"ffmpeg -y -i {original} -filter:a 'atempo=1.5' {faster}")

        # ğŸš ì¬ìƒ
        os.system(f"mpg123 {faster}")

        # ğŸ§¹ ì •ë¦¬
        os.remove(original)
        os.remove(faster)
        print(f"[ğŸ§¹] mp3 íŒŒì¼ ìë™ ì‚­ì œ ì™„ë£Œ..")

    except Exception as e:
        print(f"[TTS ì˜¤ë¥˜] {e}")

# --- ë©”ì¸ ë¡œì§ ---
def main():
    print("ONNX Skin Diagnosis System")
    print("=" * 50)
    
    # ìº¡ì²˜ í´ë” ìƒì„±
    if not os.path.exists(CAPTURE_FOLDER):
        os.makedirs(CAPTURE_FOLDER)
    
    # ëª¨ë¸ ì´ˆê¸°í™”
    model, model_type = initialize_model()
    if model is None:
        print("ì‚¬ìš© ê°€ëŠ¥í•œ ëª¨ë¸ì´ ì—†ìŠµë‹ˆë‹¤.")
        print("ë¨¼ì € convert_h5_to_onnx.pyë¥¼ ì‹¤í–‰í•˜ì—¬ ëª¨ë¸ì„ ë³€í™˜í•˜ì„¸ìš”.")
        return
    
    print(f"ì‚¬ìš© ì¤‘ì¸ ëª¨ë¸: {model_type}")
    
    # í°íŠ¸ ì„¤ì •
    font_path = "/usr/share/fonts/truetype/noto/NotoSansCJK-Regular.ttc"
    try:
        font = ImageFont.truetype(font_path, 20)
    except IOError:
        print(f"ì˜¤ë¥˜: í°íŠ¸ íŒŒì¼ì„ ì°¾ì„ ìˆ˜ ì—†ìŠµë‹ˆë‹¤: {font_path}. ê¸°ë³¸ í°íŠ¸ë¥¼ ì‚¬ìš©í•©ë‹ˆë‹¤.")
        font = ImageFont.load_default()

    # ì¹´ë©”ë¼ ì„¤ì •
    cap = cv2.VideoCapture(1) # ì™¸ë¶€ ì›¹ìº 
    if not cap.isOpened():
        cap = cv2.VideoCapture(0) # ë‚´ì¥ ì›¹ìº 
        if not cap.isOpened():
            print("ì¹´ë©”ë¼ë¥¼ ì—´ ìˆ˜ ì—†ìŠµë‹ˆë‹¤.")
            return

    print("ì¹´ë©”ë¼ê°€ ì¤€ë¹„ë˜ì—ˆìŠµë‹ˆë‹¤.")
    print("í™”ë©´ì„ ë³´ë©° ì§„ë‹¨í•  ë¶€ìœ„ë¥¼ ì¤‘ì•™ì— ìœ„ì¹˜ì‹œí‚¤ì„¸ìš”.")
    print("í‚¤ë³´ë“œ 'c'ë¥¼ ëˆ„ë¥´ë©´ 5ì´ˆê°„ ì—°ì†ìœ¼ë¡œ ì´¬ì˜í•˜ì—¬ ì§„ë‹¨í•©ë‹ˆë‹¤.")
    print("í‚¤ë³´ë“œ 'q'ë¥¼ ëˆ„ë¥´ë©´ í”„ë¡œê·¸ë¨ì„ ì¢…ë£Œí•©ë‹ˆë‹¤.")

    while True:
        ret, frame = cap.read()
        if not ret:
            print("ì˜¤ë¥˜: ì¹´ë©”ë¼ì—ì„œ í”„ë ˆì„ì„ ì½ì„ ìˆ˜ ì—†ìŠµë‹ˆë‹¤.")
            break

        # ì¤‘ì•™ 1:1 ì˜ì—­ crop
        h, w, _ = frame.shape
        min_dim = min(h, w)
        start_x = (w - min_dim) // 2
        start_y = (h - min_dim) // 2
        crop_frame = frame[start_y:start_y+min_dim, start_x:start_x+min_dim]

        # --- ì‹¤ì‹œê°„ ì˜ˆì¸¡ ---
        # ì´ë¯¸ì§€ ì „ì²˜ë¦¬
        img_array = cv2.resize(crop_frame, (96, 96))
        img_array = np.expand_dims(img_array, axis=0)  # ë°°ì¹˜ ì°¨ì› ì¶”ê°€
        img_array = img_array.astype(np.float32) / 255.0  # ì •ê·œí™”

        # ëª¨ë¸ íƒ€ì…ì— ë”°ë¥¸ ì˜ˆì¸¡
        if model_type == "H5":
            predictions = model.predict(img_array, verbose=0)
        else:
            predictions = model.predict(img_array)
        
        predicted_class_idx = np.argmax(predictions[0])
        confidence = predictions[0][predicted_class_idx]

        # ê²°ê³¼ í…ìŠ¤íŠ¸ ìƒì„±
        current_label = f"{class_names_kr[predicted_class_idx]} ({confidence*100:.1f}%)"
        
        # í™”ë©´ í‘œì‹œ ì—…ë°ì´íŠ¸ ì£¼ê¸° ì œì–´
        current_time = time.time()
        last_display_update_time = 0
        DISPLAY_UPDATE_INTERVAL_MS = 100
        if (current_time - last_display_update_time) * 1000 >= DISPLAY_UPDATE_INTERVAL_MS:
            current_display_label = current_label
            last_display_update_time = current_time

        # í™”ë©´ì— í‘œì‹œ (Pillow ì‚¬ìš©)
        img_pil = Image.fromarray(cv2.cvtColor(crop_frame, cv2.COLOR_BGR2RGB))
        draw = ImageDraw.Draw(img_pil)
        draw.text((10, 10), f"ì‹¤ì‹œê°„ ì˜ˆì¸¡ ({model_type}):", font=font, fill=(0, 255, 0))
        draw.text((10, 35), current_display_label, font=font, fill=(0, 255, 0))
        display_frame = cv2.cvtColor(np.array(img_pil), cv2.COLOR_RGB2BGR)

        cv2.imshow('ONNX Skin Disease Diagnosis', display_frame)

        key = cv2.waitKey(1) & 0xFF

        # --- 'c' í‚¤ë¥¼ ëˆŒëŸ¬ ì—°ì† ìº¡ì²˜ ë° ì§„ë‹¨ ---
        if key == ord('c'):
            # í™”ë©´ì„ ê²€ê²Œ ë§Œë“¤ê³  "ì˜ì‚¬ì˜ ë‹µë³€ ì¤€ë¹„ì¤‘..." ë©”ì‹œì§€ í‘œì‹œ
            black_screen = np.zeros_like(display_frame)
            
            # Pillowë¥¼ ì‚¬ìš©í•˜ì—¬ í…ìŠ¤íŠ¸ ì¶”ê°€
            img_pil_black = Image.fromarray(cv2.cvtColor(black_screen, cv2.COLOR_BGR2RGB))
            draw_black = ImageDraw.Draw(img_pil_black)
            
            text = "ì˜ì‚¬ì˜ ë‹µë³€ ì¤€ë¹„ì¤‘..."
            
            # í…ìŠ¤íŠ¸ í¬ê¸° ê³„ì‚°
            try:
                # Pillow 10.0.0 ì´ìƒ
                text_bbox = draw_black.textbbox((0, 0), text, font=font)
                text_width = text_bbox[2] - text_bbox[0]
                text_height = text_bbox[3] - text_bbox[1]
            except AttributeError:
                # ì´ì „ ë²„ì „ì˜ Pillow
                text_width, text_height = draw_black.textsize(text, font=font)

            text_x = (black_screen.shape[1] - text_width) // 2
            text_y = (black_screen.shape[0] - text_height) // 2
            
            draw_black.text((text_x, text_y), text, font=font, fill=(255, 255, 255))
            
            # OpenCV í˜•ì‹ìœ¼ë¡œ ë‹¤ì‹œ ë³€í™˜í•˜ì—¬ í‘œì‹œ
            black_screen_with_text = cv2.cvtColor(np.array(img_pil_black), cv2.COLOR_RGB2BGR)
            cv2.imshow('ONNX Skin Disease Diagnosis', black_screen_with_text)
            cv2.waitKey(1) # í™”ë©´ì„ ì¦‰ì‹œ ì—…ë°ì´íŠ¸

            print("\n" + "="*40)
            print(f"ì§„ë‹¨ì„ ì‹œì‘í•©ë‹ˆë‹¤. {CAPTURE_COUNT}ì´ˆ ë™ì•ˆ {CAPTURE_COUNT}ë²ˆ ì´¬ì˜í•©ë‹ˆë‹¤.")
            print("="*40)
            
            captured_classes = []
            
            for i in range(CAPTURE_COUNT):
                time.sleep(CAPTURE_INTERVAL)
                
                # í˜„ì¬ í”„ë ˆì„(crop_frame)ìœ¼ë¡œ ì˜ˆì¸¡
                current_img_array = cv2.resize(crop_frame, (96, 96))
                current_img_array = np.expand_dims(current_img_array, axis=0)
                current_img_array = current_img_array.astype(np.float32) / 255.0

                # ëª¨ë¸ íƒ€ì…ì— ë”°ë¥¸ ì˜ˆì¸¡
                if model_type == "H5":
                    current_predictions = model.predict(current_img_array, verbose=0)
                else:
                    current_predictions = model.predict(current_img_array)
                
                current_predicted_idx = np.argmax(current_predictions[0])
                
                predicted_name = class_names_kr[current_predicted_idx]
                captured_classes.append(predicted_name)
                
                # ìº¡ì²˜ ì´ë¯¸ì§€ ì €ì¥
                timestamp = time.strftime("%Y%m%d_%H%M%S")
                capture_path = os.path.join(CAPTURE_FOLDER, f"capture_{timestamp}_{i+1}.png")
                cv2.imwrite(capture_path, crop_frame)
                
                print(f"ì´¬ì˜ {i+1}/5... ì˜ˆì¸¡: {predicted_name} (ì´ë¯¸ì§€ ì €ì¥: {capture_path})")

            # --- ìµœì¢… ì§„ë‹¨ ---
            print("\n" + "-"*40)
            if len(set(captured_classes)) == 1:
                final_diagnosis = captured_classes[0]
                print(f"ìµœì¢… ì§„ë‹¨ ê²°ê³¼: **{final_diagnosis}**")
                print(f"ì‚¬ìš© ëª¨ë¸: {model_type}")
                print("-"*40)
                
                # Gemma3 í•´ê²°ì±… ìš”ì²­
                solution = get_solution_from_gemma(final_diagnosis)
                print("\n[Ollama Gemma3ì˜ ê±´ê°• ì¡°ì–¸]")
                print(solution)
                print("\n(ì£¼ì˜: ì´ ì •ë³´ëŠ” ì°¸ê³ ìš©ì´ë©°, ì •í™•í•œ ì§„ë‹¨ê³¼ ì¹˜ë£Œë¥¼ ìœ„í•´ ë°˜ë“œì‹œ ì „ë¬¸ ì˜ë£Œê¸°ê´€ì„ ë°©ë¬¸í•˜ì„¸ìš”.)")
                speak_korean_gtts(solution) # TTS ìŒì„± ì¶œë ¥
                
            else:
                print("ì§„ë‹¨ ì‹¤íŒ¨: ì˜ˆì¸¡ ê²°ê³¼ê°€ ì¼ì¹˜í•˜ì§€ ì•ŠìŠµë‹ˆë‹¤.")
                print(f"ì§€ë‚œ {CAPTURE_COUNT}ë²ˆì˜ ì˜ˆì¸¡: {captured_classes}")
            
            print("="*40)
            print("\në‹¤ì‹œ ì§„ë‹¨í•˜ë ¤ë©´ 'c'ë¥¼, ì¢…ë£Œí•˜ë ¤ë©´ 'q'ë¥¼ ëˆ„ë¥´ì„¸ìš”.")

        # --- 'q' í‚¤ë¥¼ ëˆŒëŸ¬ ì¢…ë£Œ ---
        elif key == ord('q'):
            print("í”„ë¡œê·¸ë¨ì„ ì¢…ë£Œí•©ë‹ˆë‹¤.")
            break

    cap.release()
    cv2.destroyAllWindows()

if __name__ == "__main__":
    main() 
```


## camera_onnx_optimized.py
```python
import numpy as np
import os, platform
# Wayland í™˜ê²½ì—ì„œ Qt í”Œë«í¼ í”ŒëŸ¬ê·¸ì¸ ì˜¤ë¥˜ë¥¼ ë°©ì§€í•˜ê¸° ìœ„í•´ Linuxì—ì„œëŠ” ê¸°ë³¸ì ìœ¼ë¡œ XCB ì‚¬ìš©
if platform.system() == "Linux" and os.environ.get("QT_QPA_PLATFORM", "") == "":
    os.environ["QT_QPA_PLATFORM"] = "xcb"
import cv2
from PIL import ImageFont, ImageDraw, Image
import time
import os
import ollama
import onnxruntime as ort
from onnxruntime.quantization import quantize_dynamic, QuantType
import psutil
import threading
import queue

# --- ì„¤ì • ---
CAPTURE_INTERVAL = 1  # ìº¡ì²˜ ê°„ê²© (ì´ˆ)
CAPTURE_COUNT = 5     # ìº¡ì²˜ íšŸìˆ˜
CAPTURE_FOLDER = "captures" # ìº¡ì²˜ ì´ë¯¸ì§€ ì €ì¥ í´ë”
OLLAMA_MODEL = "gemma3:1b" # ì‚¬ìš©í•  Ollama ëª¨ë¸

# ì¹´ë©”ë¼ ì„¤ì • (ë¼ì¦ˆë² ë¦¬ íŒŒì´ 5 ìµœì í™”ë¥¼ ìœ„í•´ ì¡°ì • ê°€ëŠ¥)
CAMERA_WIDTH = 640
CAMERA_HEIGHT = 480
CAMERA_FPS = 15

PREDICTION_SMOOTHING_WINDOW_SIZE = 5 # ì˜ˆì¸¡ ê²°ê³¼ ìŠ¤ë¬´ë”©ì„ ìœ„í•œ í”„ë ˆì„ ìˆ˜ (5~10 ì •ë„ ê¶Œì¥)
DISPLAY_UPDATE_INTERVAL_MS = 400 # í™”ë©´ì— í‘œì‹œë˜ëŠ” ì˜ˆì¸¡ ê²°ê³¼ ì—…ë°ì´íŠ¸ ì£¼ê¸° (ë°€ë¦¬ì´ˆ)

# ëª¨ë¸ ê²½ë¡œ ì„¤ì •

ONNX_MODEL_PATH = "./model/skin_model.onnx"
ONNX_OPTIMIZED_PATH = "./model/skin_model_quantized.onnx" # ìì˜ì ìœ¼ë¡œ ë°”ê¿”ì„œ ìµœì í™” ëª¨ë¸ ê²½ë¡œ ì„¤ì •
ONNX_QUANTIZED_PATH = "./model/skin_model_quantized.onnx"
TFLITE_MODEL_PATH = "./model/skin_model_quantized.tflite"

# --- OSë³„ ì„¤ì • í•¨ìˆ˜ ---
def get_system_font_path():
    """OSë³„ ì‹œìŠ¤í…œ í°íŠ¸ ê²½ë¡œ ë°˜í™˜"""
    system = platform.system()
    
    if system == "Windows":
        return "C:/Windows/Fonts/malgun.ttf"
    elif system == "Linux":
        # Ubuntu/Debian ê³„ì—´
        linux_fonts = [
            "/usr/share/fonts/truetype/nanum/NanumGothic.ttf",
            "/usr/share/fonts/truetype/dejavu/DejaVuSans.ttf",
            "/usr/share/fonts/truetype/liberation/LiberationSans-Regular.ttf",
            "/usr/share/fonts/TTF/NanumGothic.ttf",  # Arch Linux
            "/System/Library/Fonts/Helvetica.ttc"    # macOS backup
        ]
        for font in linux_fonts:
            if os.path.exists(font):
                return font
    elif system == "Darwin":  # macOS
        return "/System/Library/Fonts/AppleGothic.ttf"
    
    # ê¸°ë³¸ê°’ (í°íŠ¸ê°€ ì—†ëŠ” ê²½ìš°)
    return None

def get_backup_model_path():
    """ë°±ì—… H5 ëª¨ë¸ ê²½ë¡œ ë°˜í™˜ (OS ë¬´ê´€)"""
    possible_paths = [
        "./model/jaehong_skin_model.h5",  # ìƒëŒ€ ê²½ë¡œ (ìš°ì„ )
        "../pth/jaehong_skin_model.h5",   # ìƒìœ„ í´ë”
        "./jaehong_skin_model.h5",       # í˜„ì¬ í´ë”
        "C:/Users/kccistc/project/pth/jaehong_skin_model.h5",  # Windows ì ˆëŒ€ ê²½ë¡œ
        "/home/kccistc/project/pth/jaehong_skin_model.h5"       # Linux ì ˆëŒ€ ê²½ë¡œ
    ]
    
    for path in possible_paths:
        if os.path.exists(path):
            return path
    return None

# --- í´ë˜ìŠ¤ ë° ëª¨ë¸ ì„¤ì • ---
# í´ë˜ìŠ¤ëª… (7ê°œ í´ë˜ìŠ¤)
class_names_kr = [
    'ê¸°ì €ì„¸í¬ì•”',
    'í‘œí”¼ë‚­ì¢…',
    'í˜ˆê´€ì¢…',
    'ë¹„ë¦½ì¢…',
    'ì •ìƒí”¼ë¶€',
    'í¸í‰ì„¸í¬ì•”',
    'ì‚¬ë§ˆê·€'
]

# --- ìµœì í™”ëœ ONNX ëª¨ë¸ í´ë˜ìŠ¤ ---
class OptimizedONNXModel:
    def __init__(self, model_path, optimization_level="all", use_gpu=False):
        """
        ìµœì í™”ëœ ONNX ëª¨ë¸ ë¡œë“œ
        
        Args:
            model_path: ëª¨ë¸ ê²½ë¡œ
            optimization_level: ìµœì í™” ë ˆë²¨ ("disable", "basic", "extended", "all")
            use_gpu: GPU ì‚¬ìš© ì—¬ë¶€
        """
        self.model_path = model_path
        self.optimization_level = optimization_level
        self.use_gpu = use_gpu
        
        # ì„¸ì…˜ ì˜µì…˜ ì„¤ì •
        self.session_options = ort.SessionOptions()
        
        # ìµœì í™” ë ˆë²¨ ì„¤ì •
        if optimization_level == "disable":
            self.session_options.graph_optimization_level = ort.GraphOptimizationLevel.ORT_DISABLE_ALL
        elif optimization_level == "basic":
            self.session_options.graph_optimization_level = ort.GraphOptimizationLevel.ORT_ENABLE_BASIC
        elif optimization_level == "extended":
            self.session_options.graph_optimization_level = ort.GraphOptimizationLevel.ORT_ENABLE_EXTENDED
        else:  # "all"
            self.session_options.graph_optimization_level = ort.GraphOptimizationLevel.ORT_ENABLE_ALL
        
        # ë³‘ë ¬ ì²˜ë¦¬ ì„¤ì •
        cpu_count = psutil.cpu_count(logical=False)
        self.session_options.intra_op_num_threads = cpu_count
        self.session_options.inter_op_num_threads = cpu_count
        
        # ë©”ëª¨ë¦¬ íŒ¨í„´ ìµœì í™”
        self.session_options.enable_mem_pattern = True
        self.session_options.enable_cpu_mem_arena = True
        
        # ì‹¤í–‰ ì œê³µì ì„¤ì •
        providers = self._get_providers()
        
        try:
            # ONNX ì„¸ì…˜ ìƒì„±
            self.session = ort.InferenceSession(
                model_path, 
                sess_options=self.session_options,
                providers=providers
            )
            
            # ì…ì¶œë ¥ ì •ë³´
            self.input_name = self.session.get_inputs()[0].name
            self.output_name = self.session.get_outputs()[0].name
            
            # ëª¨ë¸ ì •ë³´ ì¶œë ¥
            print(f"âœ… ìµœì í™”ëœ ONNX ëª¨ë¸ ë¡œë“œ ì„±ê³µ: {model_path}")
            print(f"   ğŸ”§ ìµœì í™” ë ˆë²¨: {optimization_level}")
            print(f"   ğŸ§µ Intra-op threads: {self.session_options.intra_op_num_threads}")
            print(f"   ğŸ§µ Inter-op threads: {self.session_options.inter_op_num_threads}")
            print(f"   ğŸ’» ì‚¬ìš© ì¤‘ì¸ Providers: {self.session.get_providers()}")
            
        except Exception as e:
            print(f"âŒ ìµœì í™”ëœ ONNX ëª¨ë¸ ë¡œë“œ ì‹¤íŒ¨: {e}")
            raise
    
    def _get_providers(self):
        """ì‚¬ìš© ê°€ëŠ¥í•œ ì‹¤í–‰ ì œê³µì ë°˜í™˜"""
        providers = []
        available_providers = ort.get_available_providers()
        system = platform.system()
        
        # GPU ì‚¬ìš© ì‹œë„
        if self.use_gpu:
            # DirectML (Windowsë§Œ)
            if system == "Windows" and 'DmlExecutionProvider' in available_providers:
                providers.append('DmlExecutionProvider')
                print("ğŸ® DirectML Provider ì‚¬ìš© (Windows GPU)")
            
            # CUDA (NVIDIA - ëª¨ë“  OS)
            if 'CUDAExecutionProvider' in available_providers:
                providers.append('CUDAExecutionProvider')
                print("ğŸš€ CUDA Provider ì‚¬ìš© (NVIDIA GPU)")
            
            # ROCm (AMD - Linux)
            if system == "Linux" and 'ROCMExecutionProvider' in available_providers:
                providers.append('ROCMExecutionProvider')
                print("ğŸ”¥ ROCm Provider ì‚¬ìš© (AMD GPU)")
            
            # OpenVINO (Intel - ëª¨ë“  OS)
            if 'OpenVINOExecutionProvider' in available_providers:
                providers.append('OpenVINOExecutionProvider')
                print("âš¡ OpenVINO Provider ì‚¬ìš© (Intel GPU)")
            
            # TensorRT (NVIDIA - Linux ì£¼ë¡œ)
            if 'TensorrtExecutionProvider' in available_providers:
                providers.append('TensorrtExecutionProvider')
                print("ğŸï¸ TensorRT Provider ì‚¬ìš© (NVIDIA GPU)")
        
        # CPUëŠ” í•­ìƒ ë°±ì—…ìœ¼ë¡œ ì¶”ê°€
        providers.append('CPUExecutionProvider')
        
        return providers
    
    def predict(self, input_data):
        """ìµœì í™”ëœ ì˜ˆì¸¡"""
        try:
            result = self.session.run([self.output_name], {self.input_name: input_data})
            return result[0]
        except Exception as e:
            print(f"âŒ ì˜ˆì¸¡ ì‹¤íŒ¨: {e}")
            return None

# --- ì„±ëŠ¥ ë²¤ì¹˜ë§ˆí‚¹ í•¨ìˆ˜ ---
def benchmark_model(model, test_data, num_runs=100):
    """ëª¨ë¸ ì„±ëŠ¥ ë²¤ì¹˜ë§ˆí‚¹"""
    print(f"ğŸƒ ì„±ëŠ¥ ë²¤ì¹˜ë§ˆí‚¹ ì‹œì‘ ({num_runs}íšŒ ì‹¤í–‰)...")
    
    # ì›Œë°ì—…
    for _ in range(10):
        model.predict(test_data)
    
    # ì‹¤ì œ ë²¤ì¹˜ë§ˆí‚¹
    start_time = time.time()
    for _ in range(num_runs):
        model.predict(test_data)
    end_time = time.time()
    
    avg_time = (end_time - start_time) / num_runs * 1000  # ms
    fps = 1 / (avg_time / 1000)
    
    print(f"   âš¡ í‰ê·  ì¶”ë¡  ì‹œê°„: {avg_time:.2f}ms")
    print(f"   ğŸ¯ ì´ˆë‹¹ í”„ë ˆì„: {fps:.1f} FPS")
    
    return avg_time, fps

# --- ë¹„ë™ê¸° ì˜ˆì¸¡ í´ë˜ìŠ¤ ---
class AsyncPredictor:
    def __init__(self, model):
        self.model = model
        self.input_queue = queue.Queue(maxsize=2)
        self.output_queue = queue.Queue(maxsize=2)
        self.prediction_thread = threading.Thread(target=self._prediction_worker)
        self.prediction_thread.daemon = True
        self.prediction_thread.start()
        self.last_prediction = None
    
    def _prediction_worker(self):
        """ë°±ê·¸ë¼ìš´ë“œ ì˜ˆì¸¡ ì‘ì—…ì"""
        while True:
            try:
                input_data = self.input_queue.get(timeout=0.1)
                result = self.model.predict(input_data)
                
                # ê²°ê³¼ íê°€ ê°€ë“ ì°¬ ê²½ìš° ì´ì „ ê²°ê³¼ ì œê±°
                if self.output_queue.full():
                    try:
                        self.output_queue.get_nowait()
                    except queue.Empty:
                        pass
                
                self.output_queue.put(result)
                self.input_queue.task_done()
                
            except queue.Empty:
                continue
            except Exception as e:
                print(f"âŒ ë¹„ë™ê¸° ì˜ˆì¸¡ ì˜¤ë¥˜: {e}")
    
    def predict_async(self, input_data):
        """ë¹„ë™ê¸° ì˜ˆì¸¡ ìš”ì²­"""
        # ì…ë ¥ íê°€ ê°€ë“ ì°¬ ê²½ìš° ì´ì „ ìš”ì²­ ì œê±°
        if self.input_queue.full():
            try:
                self.input_queue.get_nowait()
            except queue.Empty:
                pass
        
        try:
            self.input_queue.put_nowait(input_data)
        except queue.Full:
            pass
    
    def get_prediction(self):
        """ì˜ˆì¸¡ ê²°ê³¼ ê°€ì ¸ì˜¤ê¸°"""
        try:
            result = self.output_queue.get_nowait()
            self.last_prediction = result
            return result
        except queue.Empty:
            return self.last_prediction

# --- ëª¨ë¸ ì´ˆê¸°í™” í•¨ìˆ˜ ---
def initialize_optimized_model():
    """ìµœì í™”ëœ ëª¨ë¸ ì´ˆê¸°í™”"""
    print("ğŸš€ ìµœì í™”ëœ ONNX ëª¨ë¸ ì´ˆê¸°í™” ì‹œì‘...")
    
    # 1. ì›ë³¸ ONNX ëª¨ë¸ í™•ì¸
    if not os.path.exists(ONNX_MODEL_PATH):
        print(f"âŒ ì›ë³¸ ONNX ëª¨ë¸ì´ ì—†ìŠµë‹ˆë‹¤: {ONNX_MODEL_PATH}")
        print("ğŸ’¡ ë¨¼ì € convert_h5_to_onnx.pyë¥¼ ì‹¤í–‰í•˜ì—¬ ëª¨ë¸ì„ ë³€í™˜í•˜ì„¸ìš”.")
        return None, None
    
    # 2. ìµœì í™”ëœ ëª¨ë¸ ë¡œë“œ ì‹œë„ (ìš°ì„ ìˆœìœ„ëŒ€ë¡œ)
    models_to_try = [
        (ONNX_QUANTIZED_PATH, "ë™ì  ì–‘ìí™” ONNX"),
        (ONNX_MODEL_PATH, "ê¸°ë³¸ ONNX")
    ]
    
    for model_path, description in models_to_try:
        if os.path.exists(model_path):
            try:
                # GPU ì‚¬ìš© ê°€ëŠ¥ ì—¬ë¶€ í™•ì¸
                use_gpu = len([p for p in ort.get_available_providers() 
                              if p in ['CUDAExecutionProvider', 'DmlExecutionProvider', 'OpenVINOExecutionProvider']]) > 0
                
                model = OptimizedONNXModel(
                    model_path, 
                    optimization_level="all",
                    use_gpu=use_gpu
                )
                
                print(f"âœ… {description} ëª¨ë¸ ë¡œë“œ ì„±ê³µ")
                return model, description
                
            except Exception as e:
                print(f"âŒ {description} ëª¨ë¸ ë¡œë“œ ì‹¤íŒ¨: {e}")
                continue
    
    # 4. ë°±ì—…ìœ¼ë¡œ H5 ëª¨ë¸ ì‹œë„
    try:
        import tensorflow as tf
        from tensorflow import keras
        h5_model_path = get_backup_model_path()
        if h5_model_path and os.path.exists(h5_model_path):
            model = keras.models.load_model(h5_model_path)
            print(f"âœ… ë°±ì—… H5 ëª¨ë¸ ë¡œë“œ ì„±ê³µ: {h5_model_path}")
            return model, "H5 ë°±ì—…"
        else:
            print("âŒ ë°±ì—… H5 ëª¨ë¸ íŒŒì¼ì„ ì°¾ì„ ìˆ˜ ì—†ìŠµë‹ˆë‹¤")
    except Exception as e:
        print(f"âŒ ë°±ì—… H5 ëª¨ë¸ ë¡œë“œ ì‹¤íŒ¨: {e}")
    
    return None, None

# --- Ollama Gemma3 í•¨ìˆ˜ ---
# í´ë˜ìŠ¤ ì´ë¦„ (í•œê¸€ â†’ ì˜ì–´ ë³€í™˜ìš©, ë˜ëŠ” UI í‘œê¸°ìš©)
class_names_kr = [
    'ê¸°ì €ì„¸í¬ì•”',
    'í‘œí”¼ë‚­ì¢…',
    'í˜ˆê´€ì¢…',
    'ë¹„ë¦½ì¢…',
    'ì •ìƒí”¼ë¶€',
    'í¸í‰ì„¸í¬ì•”',
    'ì‚¬ë§ˆê·€'
]

def get_solution_from_gemma(disease_name):
    """
    ë¡œì»¬ Ollamaì˜ Gemma3 ëª¨ë¸ì—ê²Œ í”¼ë¶€ ì§ˆí™˜ì— ëŒ€í•œ ê°„ë‹¨í•œ ê°€ì´ë“œ ìš”ì²­.
    ì‘ë‹µì€ ì‚¬ìš©ìê°€ ì´í•´í•˜ê¸° ì‰½ê²Œ 5ë‹¨ê³„ë¡œ ìš”ì•½ë˜ë©°, 200ì ë‚´ì™¸ë¡œ ì œí•œë¨.
    """

    prompt = f"""
ë‹¹ì‹ ì€ í”¼ë¶€ ê±´ê°• ì „ë¬¸ AI ì–´ì‹œìŠ¤í„´íŠ¸ì…ë‹ˆë‹¤. ì•„ë˜ í”¼ë¶€ ì§ˆí™˜ì— ëŒ€í•´ 200ì ë‚´ì™¸ë¡œ ê°„ê²°í•˜ê²Œ ì•ˆë‚´í•´ì£¼ì„¸ìš”.

í”¼ë¶€ ì§ˆí™˜ëª…: {disease_name}

ì•„ë˜ í˜•ì‹ì— ë”°ë¼ í•œêµ­ì–´ë¡œ ì •í™•í•˜ê³  ê°„ë‹¨ëª…ë£Œí•˜ê²Œ ì‘ì„±í•˜ì„¸ìš”:

1. ì§ˆí™˜ ì„¤ëª…: ì¼ë°˜ì¸ì´ ì´í•´í•  ìˆ˜ ìˆë„ë¡ ê°„ë‹¨íˆ
2. ì¦‰ì‹œ ì¡°ì¹˜ì‚¬í•­: ì‘ê¸‰ì„± ì—¬ë¶€ í¬í•¨
3. ê°€ì • ê´€ë¦¬ ë°©ë²•: ì†ì‰½ê²Œ ì‹¤ì²œ ê°€ëŠ¥í•œ íŒ
4. ì „ë¬¸ ì¹˜ë£Œ ë°©ë²•: ë³‘ì›ì—ì„œ ë°›ì„ ìˆ˜ ìˆëŠ” ì¹˜ë£Œ
5. ì£¼ì˜ì‚¬í•­: ì¬ë°œ, ê°ì—¼, ìê°€ ì¹˜ë£Œ ê²½ê³  ë“±

ê° í•­ëª©ì€ ì¤„ë°”ê¿ˆìœ¼ë¡œ êµ¬ë¶„í•˜ì—¬ ì œì‹œí•˜ì„¸ìš”.
ë‹µë³€ì€ 200ì ë‚´ì™¸ë¡œ ê°„ê²°í•˜ê²Œ ì‘ì„±í•´ì£¼ì„¸ìš”.
    """.strip()

    print(f"\n[{OLLAMA_MODEL} ëª¨ë¸ì—ê²Œ ì¡°ì–¸ì„ ìš”ì²­í•©ë‹ˆë‹¤... ì ì‹œë§Œ ê¸°ë‹¤ë ¤ ì£¼ì„¸ìš”.]")

    try:
        response = ollama.chat(
            model=OLLAMA_MODEL,
            messages=[{'role': 'user', 'content': prompt}]
        )
        return response['message']['content'].strip()

    except Exception as e:
        return f"[ì˜¤ë¥˜] Ollama ëª¨ë¸ì„ í˜¸ì¶œí•˜ëŠ” ì¤‘ ë¬¸ì œê°€ ë°œìƒí–ˆìŠµë‹ˆë‹¤: {e}\nOllama ì„œë²„ê°€ ì‹¤í–‰ ì¤‘ì¸ì§€ í™•ì¸í•˜ì„¸ìš”."

# --- TTS ---
from gtts import gTTS
import os

def speak_korean_gtts(text):
    try:
        tts = gTTS(text=text, lang='ko', slow=False)
        original = "tts_output_original.mp3"
        faster = "tts_output_fast.mp3"

        tts.save(original)

        # ğŸ› ï¸ ffmpegë¡œ ì†ë„ 1.5ë°° ë¹ ë¥´ê²Œ ë³€í™˜ (tempo=1.5)
        os.system(f"ffmpeg -y -i {original} -filter:a 'atempo=1.5' {faster}")

        # ğŸš ì¬ìƒ
        os.system(f"mpg123 {faster}")

        # ğŸ§¹ ì •ë¦¬
        os.remove(original)
        os.remove(faster)
        print(f"[ğŸ§¹] mp3 íŒŒì¼ ìë™ ì‚­ì œ ì™„ë£Œ..")

    except Exception as e:
        print(f"[TTS ì˜¤ë¥˜] {e}")

# --- ë©”ì¸ ë¡œì§ ---
def main():
    print("ìµœì í™”ëœ ONNX ê¸°ë°˜ í”¼ë¶€ ì§ˆí™˜ ì§„ë‹¨ ì‹œìŠ¤í…œ")
    print("=" * 55)
    
    # ì‹œìŠ¤í…œ ì •ë³´ ì¶œë ¥
    print(f"ğŸ’» CPU ì½”ì–´: {psutil.cpu_count(logical=False)} ë¬¼ë¦¬ / {psutil.cpu_count(logical=True)} ë…¼ë¦¬")
    print(f"ğŸ§  ì‚¬ìš© ê°€ëŠ¥í•œ ë©”ëª¨ë¦¬: {psutil.virtual_memory().available / (1024**3):.1f} GB")
    print(f"âš¡ ì‚¬ìš© ê°€ëŠ¥í•œ ONNX Providers: {ort.get_available_providers()}")
    print("=" * 55)
    
    # ìº¡ì²˜ í´ë” ìƒì„±
    if not os.path.exists(CAPTURE_FOLDER):
        os.makedirs(CAPTURE_FOLDER)
    
    # ìµœì í™”ëœ ëª¨ë¸ ì´ˆê¸°í™”
    model, model_type = initialize_optimized_model()
    if model is None:
        print("âŒ ì‚¬ìš© ê°€ëŠ¥í•œ ëª¨ë¸ì´ ì—†ìŠµë‹ˆë‹¤.")
        return
    
    print(f"ğŸ“Š ì‚¬ìš© ì¤‘ì¸ ëª¨ë¸: {model_type}")
    
    # ì„±ëŠ¥ ë²¤ì¹˜ë§ˆí‚¹ (ONNX ëª¨ë¸ì˜ ê²½ìš°)
    if model_type and ("ONNX" in model_type or "ìµœì í™”" in model_type):
        test_data = np.random.random((1, 96, 96, 3)).astype(np.float32)
        avg_time, fps = benchmark_model(model, test_data)
        
        # ë¹„ë™ê¸° ì˜ˆì¸¡ê¸° ì´ˆê¸°í™”
        async_predictor = AsyncPredictor(model)
        use_async = True
        print("ğŸ”„ ë¹„ë™ê¸° ì˜ˆì¸¡ ëª¨ë“œ í™œì„±í™”")
    else:
        use_async = False
        print("â³ ë™ê¸° ì˜ˆì¸¡ ëª¨ë“œ ì‚¬ìš©")
    
    # í°íŠ¸ ì„¤ì • (OSë³„ ëŒ€ì‘)
    font_path = get_system_font_path()
    try:
        if font_path and os.path.exists(font_path):
            font = ImageFont.truetype(font_path, 20)
            small_font = ImageFont.truetype(font_path, 14)
            print(f"âœ… í°íŠ¸ ë¡œë“œ ì„±ê³µ: {font_path}")
        else:
            font = ImageFont.load_default()
            small_font = ImageFont.load_default()
            print("âš ï¸ ì‹œìŠ¤í…œ í°íŠ¸ë¥¼ ì°¾ì„ ìˆ˜ ì—†ì–´ ê¸°ë³¸ í°íŠ¸ë¥¼ ì‚¬ìš©í•©ë‹ˆë‹¤")
    except IOError:
        font = ImageFont.load_default()
        small_font = ImageFont.load_default()
        print("âš ï¸ í°íŠ¸ ë¡œë“œ ì‹¤íŒ¨, ê¸°ë³¸ í°íŠ¸ë¥¼ ì‚¬ìš©í•©ë‹ˆë‹¤")

    # --- ì¹´ë©”ë¼ ì—´ê¸° --------------------------------------------------
    cap = open_camera()
    if cap is None:
        print("âŒ ì‚¬ìš© ê°€ëŠ¥í•œ ì¹´ë©”ë¼ë¥¼ ì°¾ì§€ ëª»í–ˆìŠµë‹ˆë‹¤.")
        print("ğŸ’¡ ë‹¤ë¥¸ ì•±ì´ ì¹´ë©”ë¼ë¥¼ ì ìœ  ì¤‘ì¸ì§€ ë˜ëŠ” ê¶Œí•œ(video ê·¸ë£¹) ì—¬ë¶€ë¥¼ í™•ì¸í•˜ì„¸ìš”.")
        return

    # --- í•´ìƒë„ / FPS / FOURCC ì„¤ì • (ê°€ëŠ¥í•œ ê²½ìš°ì—ë§Œ) ---------------
    try_set(cap, cv2.CAP_PROP_FOURCC, cv2.VideoWriter_fourcc(*"MJPG"))
    try_set(cap, cv2.CAP_PROP_FRAME_WIDTH, CAMERA_WIDTH)
    try_set(cap, cv2.CAP_PROP_FRAME_HEIGHT, CAMERA_HEIGHT)
    try_set(cap, cv2.CAP_PROP_FPS, CAMERA_FPS)

    # ìœ„ try_set ë‹¨ê³„ì—ì„œ ì´ë¯¸ í•´ìƒë„Â·FPS ì„¤ì •ì„ ì‹œë„í–ˆìœ¼ë¯€ë¡œ
    # ì¶”ê°€ cap.set í˜¸ì¶œì„ ì œê±°í•˜ì—¬ ì¼ë¶€ ì¹´ë©”ë¼ì—ì„œ í”„ë ˆì„ì´ 0Ã—0ìœ¼ë¡œ
    # ë³€í•˜ëŠ” ë¬¸ì œë¥¼ ë°©ì§€í•©ë‹ˆë‹¤.

    print("ğŸ“· ì¹´ë©”ë¼ê°€ ì¤€ë¹„ë˜ì—ˆìŠµë‹ˆë‹¤.")
    print("í™”ë©´ì„ ë³´ë©° ì§„ë‹¨í•  ë¶€ìœ„ë¥¼ ì¤‘ì•™ì— ìœ„ì¹˜ì‹œí‚¤ì„¸ìš”.")
    print("í‚¤ë³´ë“œ 'c'ë¥¼ ëˆ„ë¥´ë©´ 5ì´ˆê°„ ì—°ì†ìœ¼ë¡œ ì´¬ì˜í•˜ì—¬ ì§„ë‹¨í•©ë‹ˆë‹¤.")
    print("í‚¤ë³´ë“œ 'q'ë¥¼ ëˆ„ë¥´ë©´ í”„ë¡œê·¸ë¨ì„ ì¢…ë£Œí•©ë‹ˆë‹¤.")
    print("í‚¤ë³´ë“œ 'b'ë¥¼ ëˆ„ë¥´ë©´ ë²¤ì¹˜ë§ˆí‚¹ì„ ë‹¤ì‹œ ì‹¤í–‰í•©ë‹ˆë‹¤.")

    # ----------------- OpenCV ì°½ ì„¤ì • -----------------
    window_name = "ONNX Skin Disease Diagnosis"
    cv2.namedWindow(window_name, cv2.WINDOW_AUTOSIZE)
    # --------------------------------------------------

    # ì„±ëŠ¥ ì¸¡ì • ë³€ìˆ˜
    frame_count = 0
    fps_start_time = time.time()
    current_fps = 0.0 # FPS ê°’ì„ ì €ì¥í•  ë³€ìˆ˜ ì´ˆê¸°í™”
    last_display_update_time = time.time() # ë§ˆì§€ë§‰ ë””ìŠ¤í”Œë ˆì´ ì—…ë°ì´íŠ¸ ì‹œê°„
    current_display_label = ""

    # ì˜ˆì¸¡ ìŠ¤ë¬´ë”©ì„ ìœ„í•œ ë¦¬ìŠ¤íŠ¸
    recent_predictions = []
    
    while True:
        ret, frame = cap.read()
        if not ret:
            print("ì˜¤ë¥˜: ì¹´ë©”ë¼ì—ì„œ í”„ë ˆì„ì„ ì½ì„ ìˆ˜ ì—†ìŠµë‹ˆë‹¤.")
            break

        # ì¤‘ì•™ 1:1 ì˜ì—­ crop
        h, w, _ = frame.shape
        min_dim = min(h, w)
        start_x = (w - min_dim) // 2
        start_y = (h - min_dim) // 2
        crop_frame = frame[start_y:start_y+min_dim, start_x:start_x+min_dim]

        # ì´ë¯¸ì§€ ì „ì²˜ë¦¬
        img_array = cv2.resize(crop_frame, (96, 96))
        img_array = np.expand_dims(img_array, axis=0)
        img_array = img_array.astype(np.float32) / 255.0

        # ì˜ˆì¸¡ ìˆ˜í–‰
        if use_async:
            # ë¹„ë™ê¸° ì˜ˆì¸¡
            async_predictor.predict_async(img_array)
            predictions = async_predictor.get_prediction()
            
            if predictions is not None:
                current_predicted_class_idx = np.argmax(predictions[0])
                current_confidence = predictions[0][current_predicted_class_idx]
            else:
                current_predicted_class_idx = 0
                current_confidence = 0.0
        else:
            # ë™ê¸° ì˜ˆì¸¡
            if model_type and ("ONNX" in model_type or "ìµœì í™”" in model_type):
                predictions = model.predict(img_array)
                if predictions is not None:
                    current_predicted_class_idx = np.argmax(predictions[0])
                    current_confidence = predictions[0][current_predicted_class_idx]
                else:
                    current_predicted_class_idx = 0
                    current_confidence = 0.0
            else:
                predictions = model.predict(img_array, verbose=0)
                current_predicted_class_idx = np.argmax(predictions[0])
                current_confidence = predictions[0][current_predicted_class_idx]

        # ì˜ˆì¸¡ ê²°ê³¼ ìŠ¤ë¬´ë”©
        recent_predictions.append((current_predicted_class_idx, current_confidence))
        if len(recent_predictions) > PREDICTION_SMOOTHING_WINDOW_SIZE:
            recent_predictions.pop(0) # ê°€ì¥ ì˜¤ë˜ëœ ì˜ˆì¸¡ ì œê±°

        # ìŠ¤ë¬´ë”©ëœ ì˜ˆì¸¡ ê²°ê³¼ ê³„ì‚°
        if recent_predictions:
            # ê° í´ë˜ìŠ¤ë³„ë¡œ ë“±ì¥ íšŸìˆ˜ ê³„ì‚°
            class_counts = {}
            for idx, _ in recent_predictions:
                class_counts[idx] = class_counts.get(idx, 0) + 1
            
            # ê°€ì¥ ë§ì´ ë“±ì¥í•œ í´ë˜ìŠ¤ ì„ íƒ
            smoothed_predicted_class_idx = max(class_counts, key=class_counts.get)
            
            # í•´ë‹¹ í´ë˜ìŠ¤ì˜ í‰ê·  ì‹ ë¢°ë„ ê³„ì‚°
            smoothed_confidence_sum = sum([conf for idx, conf in recent_predictions if idx == smoothed_predicted_class_idx])
            smoothed_confidence_count = class_counts[smoothed_predicted_class_idx]
            smoothed_confidence = smoothed_confidence_sum / smoothed_confidence_count
        else:
            smoothed_predicted_class_idx = 0
            smoothed_confidence = 0.0

        # í™”ë©´ í‘œì‹œ ì—…ë°ì´íŠ¸ ì£¼ê¸° ì œì–´
        current_time = time.time()
        if (current_time - last_display_update_time) * 1000 >= DISPLAY_UPDATE_INTERVAL_MS:
            current_display_label = f"{class_names_kr[smoothed_predicted_class_idx]} ({smoothed_confidence*100:.1f}%)"
            last_display_update_time = current_time

        # FPS ê³„ì‚°
        frame_count += 1
        if frame_count % 30 == 0:
            fps_end_time = time.time()
            current_fps = 30 / (fps_end_time - fps_start_time)
            fps_start_time = fps_end_time

        # í™”ë©´ì— í‘œì‹œ
        img_pil = Image.fromarray(cv2.cvtColor(crop_frame, cv2.COLOR_BGR2RGB))
        draw = ImageDraw.Draw(img_pil)
        
        # ë©”ì¸ ì •ë³´
        draw.text((10, 10), f"ğŸ”¬ ì‹¤ì‹œê°„ ì˜ˆì¸¡ ({model_type}):", font=font, fill=(0, 255, 0))
        draw.text((10, 35), current_display_label, font=font, fill=(0, 255, 0))
        
        # ì„±ëŠ¥ ì •ë³´ (FPSëŠ” í•­ìƒ í‘œì‹œ)
        draw.text((10, 65), f"âš¡ FPS: {current_fps:.1f}", font=small_font, fill=(255, 255, 0))
        
        # ì‚¬ìš© ì¤‘ì¸ Provider ì •ë³´ (ONNX ëª¨ë¸ì˜ ê²½ìš°)
        if hasattr(model, 'session'):
            provider_info = model.session.get_providers()[0]
            draw.text((10, 85), f"ğŸ’» Provider: {provider_info.replace('ExecutionProvider', '')}", font=small_font, fill=(255, 255, 0))
        
        display_frame = cv2.cvtColor(np.array(img_pil), cv2.COLOR_RGB2BGR)
        cv2.imshow(window_name, display_frame)

        key = cv2.waitKey(1) & 0xFF

        # 'b' í‚¤ë¡œ ë²¤ì¹˜ë§ˆí‚¹ ì‹¤í–‰
        if key == ord('b') and ("ONNX" in model_type or "ìµœì í™”" in model_type):
            print("\n" + "="*50)
            print("ğŸƒ ì‹¤ì‹œê°„ ë²¤ì¹˜ë§ˆí‚¹ ì‹¤í–‰")
            print("="*50)
            avg_time, fps = benchmark_model(model, img_array)

        # 'c' í‚¤ë¡œ ì§„ë‹¨ ì‹¤í–‰
        elif key == ord('c'):
            # í™”ë©´ì„ ê²€ê²Œ ë§Œë“¤ê³  "ì˜ì‚¬ì˜ ë‹µë³€ ì¤€ë¹„ì¤‘..." ë©”ì‹œì§€ í‘œì‹œ
            black_screen = np.zeros_like(display_frame)
            
            # Pillowë¥¼ ì‚¬ìš©í•˜ì—¬ í…ìŠ¤íŠ¸ ì¶”ê°€
            img_pil_black = Image.fromarray(cv2.cvtColor(black_screen, cv2.COLOR_BGR2RGB))
            draw_black = ImageDraw.Draw(img_pil_black)
            
            text = "ì˜ì‚¬ì˜ ë‹µë³€ ì¤€ë¹„ì¤‘..."
            
            # í…ìŠ¤íŠ¸ í¬ê¸° ê³„ì‚°
            try:
                # Pillow 10.0.0 ì´ìƒ
                text_bbox = draw_black.textbbox((0, 0), text, font=font)
                text_width = text_bbox[2] - text_bbox[0]
                text_height = text_bbox[3] - text_bbox[1]
            except AttributeError:
                # ì´ì „ ë²„ì „ì˜ Pillow
                text_width, text_height = draw_black.textsize(text, font=font)

            text_x = (black_screen.shape[1] - text_width) // 2
            text_y = (black_screen.shape[0] - text_height) // 2
            
            draw_black.text((text_x, text_y), text, font=font, fill=(255, 255, 255))
            
            # OpenCV í˜•ì‹ìœ¼ë¡œ ë‹¤ì‹œ ë³€í™˜í•˜ì—¬ í‘œì‹œ
            black_screen_with_text = cv2.cvtColor(np.array(img_pil_black), cv2.COLOR_RGB2BGR)
            cv2.imshow(window_name, black_screen_with_text)
            cv2.waitKey(1) # í™”ë©´ì„ ì¦‰ì‹œ ì—…ë°ì´íŠ¸

            # ì§„ë‹¨ ë¡œì§ (ê¸°ì¡´ê³¼ ë™ì¼)
            print("\n" + "="*40)
            print(f"ì§„ë‹¨ì„ ì‹œì‘í•©ë‹ˆë‹¤. {CAPTURE_COUNT}ì´ˆ ë™ì•ˆ {CAPTURE_COUNT}ë²ˆ ì´¬ì˜í•©ë‹ˆë‹¤.")
            print("="*40)
            
            captured_classes = []
            
            for i in range(CAPTURE_COUNT):
                time.sleep(CAPTURE_INTERVAL)
                
                # í˜„ì¬ í”„ë ˆì„ìœ¼ë¡œ ì˜ˆì¸¡
                if "ONNX" in model_type or "ìµœì í™”" in model_type:
                    current_predictions = model.predict(img_array)
                    if current_predictions is not None:
                        current_predicted_idx = np.argmax(current_predictions[0])
                    else:
                        current_predicted_idx = 0
                else:
                    current_predictions = model.predict(img_array, verbose=0)
                    current_predicted_idx = np.argmax(current_predictions[0])
                
                predicted_name = class_names_kr[current_predicted_idx]
                captured_classes.append(predicted_name)
                
                # ìº¡ì²˜ ì´ë¯¸ì§€ ì €ì¥
                timestamp = time.strftime("%Y%m%d_%H%M%S")
                capture_path = os.path.join(CAPTURE_FOLDER, f"capture_{timestamp}_{i+1}.png")
                cv2.imwrite(capture_path, crop_frame)
                
                print(f"ì´¬ì˜ {i+1}/5... ì˜ˆì¸¡: {predicted_name}")

            # ìµœì¢… ì§„ë‹¨
            print("\n" + "-"*40)
            if len(set(captured_classes)) == 1:
                final_diagnosis = captured_classes[0]
                print(f"ìµœì¢… ì§„ë‹¨ ê²°ê³¼: **{final_diagnosis}**")
                print(f"ì‚¬ìš© ëª¨ë¸: {model_type}")
                print("-"*40)
                
                # Gemma3 í•´ê²°ì±… ìš”ì²­
                solution = get_solution_from_gemma(final_diagnosis)
                print("\n[Ollama Gemma3ì˜ ê±´ê°• ì¡°ì–¸]")
                print(solution)
                print("\n(ì£¼ì˜: ì´ ì •ë³´ëŠ” ì°¸ê³ ìš©ì´ë©°, ì •í™•í•œ ì§„ë‹¨ê³¼ ì¹˜ë£Œë¥¼ ìœ„í•´ ë°˜ë“œì‹œ ì „ë¬¸ ì˜ë£Œê¸°ê´€ì„ ë°©ë¬¸í•˜ì„¸ìš”.)")
                speak_korean_gtts(solution) # TTS ìŒì„± ì¶œë ¥
                
            else:
                print("ì§„ë‹¨ ì‹¤íŒ¨: ì˜ˆì¸¡ ê²°ê³¼ê°€ ì¼ì¹˜í•˜ì§€ ì•ŠìŠµë‹ˆë‹¤.")
                print(f"ì§€ë‚œ {CAPTURE_COUNT}ë²ˆì˜ ì˜ˆì¸¡: {captured_classes}")
            
            print("="*40)
            print("\në‹¤ì‹œ ì§„ë‹¨í•˜ë ¤ë©´ 'c'ë¥¼, ë²¤ì¹˜ë§ˆí‚¹ì€ 'b'ë¥¼, ì¢…ë£Œí•˜ë ¤ë©´ 'q'ë¥¼ ëˆ„ë¥´ì„¸ìš”.")

        # 'q' í‚¤ë¡œ ì¢…ë£Œ
        elif key == ord('q'):
            print("í”„ë¡œê·¸ë¨ì„ ì¢…ë£Œí•©ë‹ˆë‹¤.")
            break

    cap.release()
    cv2.destroyAllWindows()

# --- ì¹´ë©”ë¼ í—¬í¼ í•¨ìˆ˜ -------------------------------------------------

def open_camera(indices=(0, 1, 2)):
    """ì—¬ëŸ¬ ì¸ë±ìŠ¤ë¥¼ ìˆœíšŒí•˜ë©° ì •ìƒ í”„ë ˆì„ì„ ë°˜í™˜í•˜ëŠ” ì¹´ë©”ë¼ ê°ì²´ë¥¼ ì°¾ëŠ”ë‹¤."""
    for idx in indices:
        cap = cv2.VideoCapture(idx)
        if not cap.isOpened():
            continue

        ok, frame = cap.read()
        if ok and frame is not None and frame.size > 0:
            print(f"âœ… ì¹´ë©”ë¼ {idx}ë²ˆ ì •ìƒ ë™ì‘ (ê¸°ë³¸ ì„¤ì •)")
            return cap

        # ì •ìƒ í”„ë ˆì„ì´ ì•„ë‹ˆë©´ í•´ì œ í›„ ë‹¤ìŒ ì¸ë±ìŠ¤ ì‹œë„
        cap.release()
    return None

def try_set(cap, prop, value):
    """ì¹´ë©”ë¼ ì†ì„± ì„¤ì • ì‹œë„ í›„ ì‹¤íŒ¨í•˜ë©´ ì›ë³µ."""
    old_val = cap.get(prop)
    cap.set(prop, value)
    ok, frame = cap.read()
    if not ok or frame is None or frame.size == 0:
        cap.set(prop, old_val)
        print(f"âš ï¸ ì†ì„± ì„¤ì • ì‹¤íŒ¨ â†’ ì›ë³µ: {prop}={value}")
        return False
    return True

if __name__ == "__main__":
    main() 
```