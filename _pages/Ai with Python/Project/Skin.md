---
title: "Skin_Check" 
date: "2025-07-01"
thumbnail: "../../../assets/img/ARM/AI/skin/images.png"
---

# <Google Colab í•™ìŠµ Code>

```python
# SKIN_Project.ipynb
import matplotlib.pyplot as plt
import numpy as np
import os
from tensorflow.keras.preprocessing.image import ImageDataGenerator
from tensorflow.keras.applications import MobileNetV2
from tensorflow.keras import models, layers
from tensorflow.keras.optimizers import Adam
from tensorflow.keras.callbacks import EarlyStopping, ModelCheckpoint

# âœ… Google Drive ë§ˆìš´íŠ¸
from google.colab import drive
drive.mount('/content/drive')

# âœ… ê²½ë¡œ ì„¤ì •
dataset_path = '/content/drive/MyDrive/SKIN/dataset_skin'  # ë„ˆê°€ ì˜¬ë¦° ê²½ë¡œë¡œ ìˆ˜ì •
model_save_path = '/content/drive/MyDrive/SKIN/skin_model.h5'  # ì›í•˜ëŠ” ì €ì¥ ê²½ë¡œ

# âœ… ë°ì´í„° ì¦ê°• ì„¤ì • (rotation_rangeëŠ” 15ë„ë§Œ ì¤Œ)
datagen = ImageDataGenerator(
    rescale=1./255,
    validation_split=0.2,
    rotation_range=90,
    width_shift_range=0.3,
    height_shift_range=0.3,
    shear_range=0.3,
    zoom_range=0.3,
    brightness_range=[0.6, 1.4],
    horizontal_flip=True,
    fill_mode='nearest'
)

# âœ… ë°ì´í„° ë¡œë”©
train_generator = datagen.flow_from_directory(
    dataset_path,
    target_size=(96, 96),  # MobileNetV2ëŠ” ìµœì†Œ 96x96ë¶€í„° ê°€ëŠ¥
    batch_size=32,
    class_mode='categorical',
    subset='training',
    shuffle=True
)

val_generator = datagen.flow_from_directory(
    dataset_path,
    target_size=(96, 96),
    batch_size=32,
    class_mode='categorical',
    subset='validation',
    shuffle=True
)

# âœ… í´ë˜ìŠ¤ ì´ë¦„ ìë™ ì¶”ì¶œ
class_names = list(train_generator.class_indices.keys())
print("í´ë˜ìŠ¤ ì¸ë±ìŠ¤:", train_generator.class_indices)

# âœ… MobileNetV2 ê¸°ë°˜ ëª¨ë¸ êµ¬ì„±
base_model = MobileNetV2(input_shape=(96, 96, 3), include_top=False, weights='imagenet')
base_model.trainable = False

model = models.Sequential([
    base_model,
    layers.GlobalAveragePooling2D(),
    layers.Dense(128, activation='relu'),
    layers.Dropout(0.5),
    layers.Dense(len(class_names), activation='softmax')
])

model.compile(optimizer=Adam(learning_rate=1e-4),
              loss='categorical_crossentropy',
              metrics=['accuracy'])

# âœ… ì½œë°± ì„¤ì •
early_stop = EarlyStopping(monitor='val_loss', patience=5, restore_best_weights=True)
checkpoint = ModelCheckpoint('best_model.h5', save_best_only=True)

# âœ… í•™ìŠµ ì‹¤í–‰
history = model.fit(
    train_generator,
    validation_data=val_generator,
    epochs=50,
    callbacks=[early_stop, checkpoint],
    verbose=2
)

# âœ… ê²°ê³¼ ì‹œê°í™”
acc = history.history['accuracy']
val_acc = history.history['val_accuracy']
loss = history.history['loss']
val_loss = history.history['val_loss']
epochs = range(len(acc))

plt.plot(epochs, acc, 'bo', label='Training acc')
plt.plot(epochs, val_acc, 'b', label='Validation acc')
plt.title('Training and Validation Accuracy')
plt.legend()
plt.show()

plt.plot(epochs, loss, 'bo', label='Training loss')
plt.plot(epochs, val_loss, 'b', label='Validation loss')
plt.title('Training and Validation Loss')
plt.legend()
plt.show()

# âœ… í•™ìŠµ ì´ë¯¸ì§€ ì˜ˆì‹œ
x_batch, y_batch = next(train_generator)
plt.figure(figsize=(10, 10))
for i in range(25):
    plt.subplot(5, 5, i + 1)
    plt.xticks([]); plt.yticks([]); plt.grid(False)
    plt.imshow(x_batch[i])
    label_idx = np.argmax(y_batch[i])
    plt.xlabel(class_names[label_idx])
plt.tight_layout()
plt.show()

# âœ… ëª¨ë¸ ì €ì¥ (.h5 íŒŒì¼)
model.save(model_save_path)
print(f"ëª¨ë¸ì´ ì €ì¥ë˜ì—ˆìŠµë‹ˆë‹¤: {model_save_path}")
```

## Result
![alt text](<../../../assets/img/ARM/AI/skin/ìŠ¤í¬ë¦°ìƒ· 2025-07-04 204712.png>)

## Ubuntu Terminal Dir
```json
// skin_names.json
[
  "ê¸°ì €ì„¸í¬ì•”",
  "ë³´ì›¬ë³‘",
  "í‘œí”¼ë‚­ì¢…",
  "ë¹„ë¦½ì¢…",
  "ì •ìƒí”¼ë¶€",
  "í™”ë†ì„± ìœ¡ì•„ì¢…",
  "í¸í‰ì„¸í¬ì•”",
  "ì‚¬ë§ˆê·€"
]
```
## predict_cam.py
``` python
# predict_cam.py
import cv2
import numpy as np
import tensorflow as tf
import json
from PIL import ImageFont, ImageDraw, Image

# í´ë˜ìŠ¤ ì´ë¦„ (í•œê¸€) ë¶ˆëŸ¬ì˜¤ê¸°
with open("skin_names.json", "r") as f:
    class_names = json.load(f)

# ëª¨ë¸ ë¡œë“œ
model = tf.keras.models.load_model("skin_model.h5")

# í•œê¸€ í°íŠ¸ ê²½ë¡œ (Ubuntuì—ì„œ ì‚¬ìš© ê°€ëŠ¥)
FONT_PATH = "/usr/share/fonts/truetype/nanum/NanumGothic.ttf"
font = ImageFont.truetype(FONT_PATH, 32)

# ì›¹ìº  ì‹œì‘
cap = cv2.VideoCapture(2)
print("Press 'q' to quit.")

while True:
    ret, frame = cap.read()
    if not ret:
        break

    # ì˜ˆì¸¡ì„ ìœ„í•œ ì „ì²˜ë¦¬
    img = cv2.resize(frame, (96, 96))
    img_array = np.expand_dims(img / 255.0, axis=0)
    pred = model.predict(img_array)[0]
    label = class_names[np.argmax(pred)]
    confidence = np.max(pred)

    # ì›ë³¸ í”„ë ˆì„ì„ Pillow ì´ë¯¸ì§€ë¡œ ë³€í™˜
    frame_rgb = cv2.cvtColor(frame, cv2.COLOR_BGR2RGB)
    pil_img = Image.fromarray(frame_rgb)
    draw = ImageDraw.Draw(pil_img)
    text = f"{label} ({confidence*100:.1f}%)"

    # í…ìŠ¤íŠ¸ ì¶œë ¥
    draw.text((10, 30), text, font=font, fill=(0, 255, 0))  # ì´ˆë¡ìƒ‰

    # ë‹¤ì‹œ OpenCV í˜•ì‹ìœ¼ë¡œ ë³€í™˜í•˜ì—¬ ì¶œë ¥
    frame_bgr = cv2.cvtColor(np.array(pil_img), cv2.COLOR_RGB2BGR)
    cv2.imshow("Skin Classifier", frame_bgr)

    if cv2.waitKey(1) & 0xFF == ord("q"):
        break

cap.release()
cv2.destroyAllWindows()
```

# ë¼ì¦ˆë² ë¦¬íŒŒì´ í”¼ë¶€ ì§ˆí™˜ ì§„ë‹¨ ë° ìƒë‹´ ì‹œìŠ¤í…œ

ì‹¤ì‹œê°„ ì¹´ë©”ë¼ ì˜ìƒìœ¼ë¡œ 15ì¢…ì˜ í”¼ë¶€ ì§ˆí™˜ì„ ì˜ˆì¸¡í•˜ê³ , Ollamaì™€ Gemma3 ëª¨ë¸ì„ í†µí•´ ê´€ë ¨ ì •ë³´ë¥¼ ì œê³µí•˜ëŠ” ì‹œìŠ¤í…œì…ë‹ˆë‹¤.

## í´ë” êµ¬ì¡°
```
skin_diagnosis_pi/
â”œâ”€â”€ app.py
â”œâ”€â”€ requirements.txt
â”œâ”€â”€ model/
â”‚   â””â”€â”€ best_skin_disease_model.pth
â”œâ”€â”€ font/
â”‚   â””â”€â”€ NanumGothic.ttf
â””â”€â”€ README.md
```

## ë¼ì¦ˆë² ë¦¬íŒŒì´ ì„¤ì • ë‹¨ê³„

### 1. í”„ë¡œì íŠ¸ í´ë” ë³µì‚¬
- ì´ `skin_diagnosis_pi` í´ë” ì „ì²´ë¥¼ ë¼ì¦ˆë² ë¦¬íŒŒì´ì˜ ì›í•˜ëŠ” ìœ„ì¹˜ë¡œ ë³µì‚¬í•©ë‹ˆë‹¤. (ì˜ˆ: `/home/pi/`)

### 2. í•„ìˆ˜ ë¼ì´ë¸ŒëŸ¬ë¦¬ ì„¤ì¹˜
- í„°ë¯¸ë„ì„ ì—´ê³  í”„ë¡œì íŠ¸ í´ë”ë¡œ ì´ë™í•©ë‹ˆë‹¤.
  ```bash
  cd /path/to/skin_diagnosis_pi
  ```
  ê°€ìƒí™˜ê²½ ì´ë™ í›„,
- `requirements.txt` íŒŒì¼ì„ ì´ìš©í•´ ëª¨ë“  íŒŒì´ì¬ ë¼ì´ë¸ŒëŸ¬ë¦¬ë¥¼ ì„¤ì¹˜í•©ë‹ˆë‹¤.
  ```bash
  pip3 install -r requirements.txt
  ```
- **ì¤‘ìš”**: ë¼ì¦ˆë² ë¦¬íŒŒì´ì—ì„œëŠ” `torch`ì™€ `torchvision` ì„¤ì¹˜ê°€ ê¹Œë‹¤ë¡œìš¸ ìˆ˜ ìˆìŠµë‹ˆë‹¤. ìœ„ ëª…ë ¹ì´ ì‹¤íŒ¨í•˜ë©´, ë¼ì¦ˆë² ë¦¬íŒŒì´ OS ë²„ì „ì— ë§ëŠ” íŒŒì¼ì„ ì§ì ‘ ì°¾ì•„ ì„¤ì¹˜í•´ì•¼ í•  ìˆ˜ ìˆìŠµë‹ˆë‹¤. [ì´ê³³](https://github.com/KinaIso/RaspberryPi-Pytorch-builder)ê³¼ ê°™ì€ ì»¤ë®¤ë‹ˆí‹° ë¹Œë“œë¥¼ ì°¸ê³ í•˜ì„¸ìš”.

### 3. Ollama ì„¤ì¹˜ ë° Gemma3 ëª¨ë¸ ë‹¤ìš´ë¡œë“œ
- [Ollama ê³µì‹ í™ˆí˜ì´ì§€](https://ollama.com/download)ë¡œ ì´ë™í•˜ì—¬ ë¼ì¦ˆë² ë¦¬íŒŒì´ìš© Ollamaë¥¼ ì„¤ì¹˜í•©ë‹ˆë‹¤.
ollama run gemma3:1b 
- ì„¤ì¹˜ê°€ ì™„ë£Œë˜ë©´ í„°ë¯¸ë„ì—ì„œ ì•„ë˜ ëª…ë ¹ì„ ì‹¤í–‰í•˜ì—¬ `gemma3` ëª¨ë¸ì„ ë‹¤ìš´ë¡œë“œí•©ë‹ˆë‹¤.
  ```bash
  ollama run gemma3:1b
  ```
- ìœ„ ëª…ë ¹ì„ ì‹¤í–‰í•˜ë©´ ëª¨ë¸ ë‹¤ìš´ë¡œë“œ í›„ `>>>` í”„ë¡¬í”„íŠ¸ê°€ ë‚˜íƒ€ë‚©ë‹ˆë‹¤. ì´ í„°ë¯¸ë„ì€ **ë‹«ì§€ ë§ê³  ê·¸ëŒ€ë¡œ ì¼œë‘ì„¸ìš”.** Ollama ì„œë²„ ì—­í• ì„ í•©ë‹ˆë‹¤.

## ì‹¤í–‰ ë°©ë²•

1.  **ìƒˆë¡œìš´ í„°ë¯¸ë„ ì°½**ì„ ì—½ë‹ˆë‹¤.
2.  í”„ë¡œì íŠ¸ í´ë”ë¡œ ì´ë™í•©ë‹ˆë‹¤.
   ```bash
   cd /path/to/skin_diagnosis_pi
   ```
3.  ë©”ì¸ ìŠ¤í¬ë¦½íŠ¸ë¥¼ ì‹¤í–‰í•©ë‹ˆë‹¤.
   ```bash
   python3 app.py
   ```
4.  ì¹´ë©”ë¼ ì˜ìƒì´ ë‚˜íƒ€ë‚˜ë©´, ì§„ë‹¨í•  ë¶€ìœ„ë¥¼ ì¤‘ì•™ì— ë§ì¶”ê³  í‚¤ë³´ë“œ `c`ë¥¼ ëˆŒëŸ¬ ì§„ë‹¨ì„ ì‹œì‘í•˜ì„¸ìš”.
5.  ì¢…ë£Œí•˜ë ¤ë©´ `q`ë¥¼ ëˆ„ë¦…ë‹ˆë‹¤.


## requirements.txt
```
opencv-python
torch
torchvision
numpy
Pillow
ollama
```

## app.py
```python


import torch
import torch.nn as nn
from torchvision import models, transforms
import cv2
import numpy as np
from PIL import ImageFont, ImageDraw, Image
import time
import os
import ollama

# --- ì„¤ì • ---
CAPTURE_INTERVAL = 1  # ìº¡ì²˜ ê°„ê²© (ì´ˆ)
CAPTURE_COUNT = 5     # ìº¡ì²˜ íšŸìˆ˜
OLLAMA_MODEL = "gemma3:1b" # ì‚¬ìš©í•  Ollama ëª¨ë¸

# --- ê²½ë¡œ ì„¤ì • (ì¤‘ìš”: ìƒëŒ€ ê²½ë¡œ ì‚¬ìš©) ---
# ì´ ìŠ¤í¬ë¦½íŠ¸ íŒŒì¼(app.py)ì´ ìˆëŠ” ë””ë ‰í† ë¦¬ë¥¼ ê¸°ì¤€ìœ¼ë¡œ ê²½ë¡œë¥¼ ì„¤ì •í•©ë‹ˆë‹¤.
BASE_DIR = os.path.dirname(os.path.abspath(__file__))
CAPTURE_FOLDER = os.path.join(BASE_DIR, "captures")
MODEL_PATH = os.path.join(BASE_DIR, "model", "best_skin_disease_model.pth")
FONT_PATH = os.path.join(BASE_DIR, "font", "NanumGothic.ttf")
CAMERA_INDEX = 0 # ë¼ì¦ˆë² ë¦¬íŒŒì´ ì¹´ë©”ë¼ëŠ” ë³´í†µ 0ë²ˆì…ë‹ˆë‹¤.

# --- í´ë˜ìŠ¤ ë° ëª¨ë¸ ì„¤ì • ---
class_names_kr = [
    'ê´‘ì„ ê°í™”ì¦', 'ê¸°ì €ì„¸í¬ì•”', 'ë©œë¼ë‹Œì„¸í¬ëª¨ë°˜', 'ë³´ì›¬ë³‘', 'ë¹„ë¦½ì¢…',
    'ì‚¬ë§ˆê·€', 'ì•…ì„±í‘ìƒ‰ì¢…', 'ì§€ë£¨ê°í™”ì¦', 'í¸í‰ì„¸í¬ì•”', 'í‘œí”¼ë‚­ì¢…',
    'í”¼ë¶€ì„¬ìœ ì¢…', 'í”¼ì§€ìƒ˜ì¦ì‹ì¢…', 'í˜ˆê´€ì¢…', 'í™”ë†ìœ¡ì•„ì¢…', 'í‘ìƒ‰ì '
]

class SkinDiseaseClassifier(nn.Module):
    def __init__(self, num_classes=15):
        super(SkinDiseaseClassifier, self).__init__()
        self.backbone = models.resnet101(weights=None) # weights=Noneìœ¼ë¡œ ëª…ì‹œ
        self.backbone = nn.Sequential(*list(self.backbone.children())[:-1])
        self.avgpool = nn.AdaptiveAvgPool2d((1, 1))
        self.flatten = nn.Flatten()
        self.fc1 = nn.Linear(2048, 1024)
        self.relu1 = nn.ReLU(inplace=True)
        self.dropout1 = nn.Dropout(0.5)
        self.fc2 = nn.Linear(1024, 512)
        self.relu2 = nn.ReLU(inplace=True)
        self.dropout2 = nn.Dropout(0.3)
        self.fc3 = nn.Linear(512, num_classes)

    def forward(self, x):
        x = self.backbone(x)
        x = self.avgpool(x)
        x = self.flatten(x)
        x = self.fc1(x)
        x = self.relu1(x)
        x = self.dropout1(x)
        x = self.fc2(x)
        x = self.relu2(x)
        x = self.dropout2(x)
        x = self.fc3(x)
        return x

# --- Ollama Gemma3 í•¨ìˆ˜ ---
def get_solution_from_gemma(disease_name):
    prompt = f"""ë‹¹ì‹ ì€ í”¼ë¶€ ê±´ê°• ì „ë¬¸ AI ì–´ì‹œìŠ¤í„´íŠ¸ì…ë‹ˆë‹¤.
    ë‹¤ìŒ í”¼ë¶€ ì§ˆí™˜ì— ëŒ€í•´ ì¼ë°˜ì¸ì´ ì´í•´í•˜ê¸° ì‰½ê²Œ ì„¤ëª…í•˜ê³ , ì§‘ì—ì„œ í•  ìˆ˜ ìˆëŠ” ê´€ë¦¬ ë°©ë²•ê³¼ ì „ë¬¸ì ì¸ ì¹˜ë£Œ ë°©ë²•ì„ ë‹¨ê³„ë³„ë¡œ ëª…í™•í•˜ê²Œ êµ¬ë¶„í•´ì„œ í•œêµ­ì–´ë¡œ ì•Œë ¤ì£¼ì„¸ìš”.

    **ì§ˆí™˜ëª…: {disease_name}**
    """
    print(f"\n[{OLLAMA_MODEL} ëª¨ë¸ì—ê²Œ ì¡°ì–¸ì„ ìš”ì²­í•©ë‹ˆë‹¤...]")
    try:
        response = ollama.chat(
            model=OLLAMA_MODEL,
            messages=[{'role': 'user', 'content': prompt}]
        )
        return response['message']['content']
    except Exception as e:
        return f"Ollama ëª¨ë¸ í˜¸ì¶œ ì¤‘ ì˜¤ë¥˜: {e}\nOllamaê°€ ì‹¤í–‰ ì¤‘ì¸ì§€, ëª¨ë¸({OLLAMA_MODEL})ì´ ë‹¤ìš´ë¡œë“œë˜ì—ˆëŠ”ì§€ í™•ì¸í•˜ì„¸ìš”."

# --- ë©”ì¸ ë¡œì§ ---
def main():
    if not os.path.exists(CAPTURE_FOLDER):
        os.makedirs(CAPTURE_FOLDER)

    transform = transforms.Compose([
        transforms.ToPILImage(),
        transforms.Resize((224, 224)),
        transforms.ToTensor(),
        transforms.Normalize(mean=[0.485, 0.456, 0.406], std=[0.229, 0.224, 0.225])
    ])
    
    device = torch.device('cpu')
    classification_model = SkinDiseaseClassifier(num_classes=15)
    
    if not os.path.exists(MODEL_PATH):
        print(f"ì˜¤ë¥˜: ë¶„ë¥˜ ëª¨ë¸ íŒŒì¼ì„ ì°¾ì„ ìˆ˜ ì—†ìŠµë‹ˆë‹¤! ê²½ë¡œë¥¼ í™•ì¸í•˜ì„¸ìš”: {MODEL_PATH}")
        return
    classification_model.load_state_dict(torch.load(MODEL_PATH, map_location=device))
    classification_model.eval()

    try:
        font = ImageFont.truetype(FONT_PATH, 20)
    except IOError:
        print(f"ê²½ê³ : í•œê¸€ í°íŠ¸ íŒŒì¼ì„ ì°¾ì„ ìˆ˜ ì—†ìŠµë‹ˆë‹¤: {FONT_PATH}. ê¸°ë³¸ í°íŠ¸ë¥¼ ì‚¬ìš©í•©ë‹ˆë‹¤.")
        font = ImageFont.load_default()

    cap = cv2.VideoCapture(CAMERA_INDEX)
    if not cap.isOpened():
        print(f"ì˜¤ë¥˜: ì¹´ë©”ë¼({CAMERA_INDEX}ë²ˆ)ë¥¼ ì—´ ìˆ˜ ì—†ìŠµë‹ˆë‹¤.")
        return

    print("\nì¹´ë©”ë¼ ì¤€ë¹„ ì™„ë£Œ. 'c'ë¥¼ ëˆ„ë¥´ë©´ ì§„ë‹¨ ì‹œì‘, 'q'ë¥¼ ëˆ„ë¥´ë©´ ì¢…ë£Œ.")

    while True:
        ret, frame = cap.read()
        if not ret:
            break

        h, w, _ = frame.shape
        min_dim = min(h, w)
        start_x = (w - min_dim) // 2
        start_y = (h - min_dim) // 2
        crop_frame = frame[start_y:start_y+min_dim, start_x:start_x+min_dim]

        input_tensor = transform(crop_frame).unsqueeze(0).to(device)
        with torch.no_grad():
            outputs = classification_model(input_tensor)
            probabilities = torch.softmax(outputs, dim=1)
            predicted_class_idx = torch.argmax(probabilities, dim=1).item()
            confidence = probabilities[0][predicted_class_idx].item()

        label = f"{class_names_kr[predicted_class_idx]} ({confidence*100:.1f}%)"
        
        img_pil = Image.fromarray(cv2.cvtColor(crop_frame, cv2.COLOR_BGR2RGB))
        draw = ImageDraw.Draw(img_pil)
        draw.text((10, 10), "ì‹¤ì‹œê°„ ì˜ˆì¸¡:", font=font, fill=(0, 255, 0))
        draw.text((10, 35), label, font=font, fill=(0, 255, 0))
        display_frame = cv2.cvtColor(np.array(img_pil), cv2.COLOR_RGB2BGR)

        cv2.imshow('Skin Diagnosis (Raspberry Pi)', display_frame)

        key = cv2.waitKey(1) & 0xFF

        if key == ord('c'):
            print("\n" + "="*40 + "\nì§„ë‹¨ì„ ì‹œì‘í•©ë‹ˆë‹¤...")
            captured_classes = []
            for i in range(CAPTURE_COUNT):
                time.sleep(CAPTURE_INTERVAL)
                
                current_input_tensor = transform(crop_frame).unsqueeze(0).to(device)
                with torch.no_grad():
                    current_outputs = classification_model(current_input_tensor)
                    current_predicted_idx = torch.argmax(torch.softmax(current_outputs, dim=1), dim=1).item()
                
                predicted_name = class_names_kr[current_predicted_idx]
                captured_classes.append(predicted_name)
                
                timestamp = time.strftime("%Y%m%d_%H%M%S")
                capture_path = os.path.join(CAPTURE_FOLDER, f"capture_{timestamp}_{i+1}.png")
                cv2.imwrite(capture_path, crop_frame)
                print(f"ì´¬ì˜ {i+1}/{CAPTURE_COUNT}... ì˜ˆì¸¡: {predicted_name}")

            print("\n" + "-"*40)
            if len(set(captured_classes)) == 1:
                final_diagnosis = captured_classes[0]
                print(f"ìµœì¢… ì§„ë‹¨ ê²°ê³¼: **{final_diagnosis}**")
                print("-" * 40)
                
                solution = get_solution_from_gemma(final_diagnosis)
                print("\n[Ollama Gemma3ì˜ ê±´ê°• ì¡°ì–¸]")
                print(solution)
                print("\n(ì£¼ì˜: ì´ ì •ë³´ëŠ” ì°¸ê³ ìš©ì´ë©°, ì •í™•í•œ ì§„ë‹¨ê³¼ ì¹˜ë£Œë¥¼ ìœ„í•´ ë°˜ë“œì‹œ ì „ë¬¸ ì˜ë£Œê¸°ê´€ì„ ë°©ë¬¸í•˜ì„¸ìš”.)")
            else:
                print("ì§„ë‹¨ ì‹¤íŒ¨: ì˜ˆì¸¡ ê²°ê³¼ê°€ ì¼ì¹˜í•˜ì§€ ì•ŠìŠµë‹ˆë‹¤.")
                print(f"ì§€ë‚œ {CAPTURE_COUNT}ë²ˆì˜ ì˜ˆì¸¡: {captured_classes}")
            
            print("="*40 + "\në‹¤ì‹œ ì§„ë‹¨í•˜ë ¤ë©´ 'c'ë¥¼, ì¢…ë£Œí•˜ë ¤ë©´ 'q'ë¥¼ ëˆ„ë¥´ì„¸ìš”.")

        elif key == ord('q'):
            break

    cap.release()
    cv2.destroyAllWindows()

if __name__ == "__main__":
    main()

```



















# ğŸ¥ ONNX ê¸°ë°˜ í”¼ë¶€ ì§ˆí™˜ ì§„ë‹¨ ì‹œìŠ¤í…œ

H5 ëª¨ë¸ì„ 8ë¹„íŠ¸ ì–‘ìí™”í•˜ê³  ONNX/TFLiteë¡œ ë³€í™˜í•˜ì—¬ ë” ë¹ ë¥´ê³  íš¨ìœ¨ì ì¸ í”¼ë¶€ ì§ˆí™˜ ì§„ë‹¨ ì‹œìŠ¤í…œì…ë‹ˆë‹¤.

## ğŸ“‹ ì‹œìŠ¤í…œ êµ¬ì„±

```
ğŸ“ onnx_skin_diagnosis/
â”œâ”€â”€ convert_h5_to_onnx.py          # âœ… H5 â†’ ONNX ë³€í™˜ ìŠ¤í¬ë¦½íŠ¸
â”œâ”€â”€ camera_h5_diagnosis.py         # ğŸ“± H5 ëª¨ë¸ ì¹´ë©”ë¼ ì§„ë‹¨ í”„ë¡œê·¸ë¨
â”œâ”€â”€ camera_onnx_diagnosis.py       # ğŸ“± ê¸°ë³¸ ONNX ì¹´ë©”ë¼ ì§„ë‹¨ í”„ë¡œê·¸ë¨
â”œâ”€â”€ camera_onnx_optimized.py       # ğŸš€ ìµœì í™”ëœ ONNX ì¹´ë©”ë¼ ì§„ë‹¨ í”„ë¡œê·¸ë¨
â”œâ”€â”€ README.md                      # ğŸ“– í”„ë¡œì íŠ¸ ì„¤ëª…ì„œ
â”œâ”€â”€ README_WINDOW.md               # ğŸ“– Windows ë²„ì „ ì„¤ëª…ì„œ
â”œâ”€â”€ OPTIMIZED_option.md            # ğŸ“– ìµœì í™” ì˜µì…˜ ì„¤ëª…ì„œ
â”œâ”€â”€ requirements.txt               # ğŸ“‹ í•„ìš”í•œ íŒ¨í‚¤ì§€ ëª©ë¡
â”œâ”€â”€ ğŸ“ model/                      # ğŸ§  ëª¨ë¸ ì €ì¥ì†Œ
â”‚   â”œâ”€â”€ skin_model.h5              # ì›ë³¸ Keras ëª¨ë¸ (11.4MB)
â”‚   â”œâ”€â”€ skin_model.onnx            # ë³€í™˜ëœ ONNX ëª¨ë¸ (9.6MB)
â”‚   â”œâ”€â”€ skin_model_quantized.tflite # ì–‘ìí™” TFLite ëª¨ë¸ (2.9MB)
â”‚   â”œâ”€â”€ skin_model_quantized_dynamic.onnx # ë™ì  ì–‘ìí™” ONNX (2.6MB)
â”‚   â””â”€â”€ skin_model_quantized_static.onnx  # ì •ì  ì–‘ìí™” ONNX (2.6MB)
â””â”€â”€ ğŸ“ captures/                   # ğŸ“¸ ì§„ë‹¨ ì´ë¯¸ì§€ ì €ì¥ í´ë”
```

## ğŸš€ ì„¤ì¹˜ ë° ì„¤ì •

<details>
<summary> # window </summary>
<div markdown="1">

### 1. í•„ìš”í•œ íŒ¨í‚¤ì§€ ì„¤ì¹˜

```bash
# ê¸°ë³¸ íŒ¨í‚¤ì§€
pip install -r requirements.txt

## ğŸ“‚ ì‚¬ìš© ë°©ë²•

### 1ë‹¨ê³„: ëª¨ë¸ ë³€í™˜

ë¨¼ì € H5 ëª¨ë¸ì„ ONNX/TFLiteë¡œ ë³€í™˜í•©ë‹ˆë‹¤:

```bash
python3 convert_h5_to_onnx.py
```

**ë³€í™˜ ê²°ê³¼:**
- âœ… `skin_model.onnx` - ONNX ëª¨ë¸ ìƒì„±
- âœ… `skin_model_quantized.tflite` - 8ë¹„íŠ¸ ì–‘ìí™” TFLite ëª¨ë¸ ìƒì„±
- âœ… `skin_model_quantized_dynamic.onxx` - 8ë¹„íŠ¸ ë™ì  ì–‘ìí™” onxx ëª¨ë¸ ìƒì„±
- âœ… `skin_model_static_dynamic.onxx` - 8ë¹„íŠ¸ ì •ì  ì–‘ìí™” onxx ëª¨ë¸ ìƒì„±
 

### 2ë‹¨ê³„: ì§„ë‹¨ í”„ë¡œê·¸ë¨ ì‹¤í–‰

```bash
# ollama ì‹¤í–‰ (í„°ë¯¸ë„ í•˜ë‚˜ ë” ì—´ì–´ì„œ ì§„í–‰í–‰)
ollama run gemma3:1b

```

```bash
# h5 ê¸°ë³¸
python camera_h5_diagnosis.py

# onxx ê¸°ë³¸
python camera_onnx_diagnosis.py

# onxx runtime ì ìš©
python camera_onnx_optimized.py

```
</div>
</details>


<details>
<summary> # Linux </summary>
<div markdown="1">


### 1. í•„ìš”í•œ íŒ¨í‚¤ì§€ ì„¤ì¹˜

```bash
# ê¸°ë³¸ íŒ¨í‚¤ì§€ ì„¤ì¹˜
pip install -r requirements.txt

# Pillow ìµœì‹  ë²„ì „ ì—…ê·¸ë ˆì´ë“œ (í…ìŠ¤íŠ¸ ë Œë”ë§ ì˜¤ë¥˜ ë°©ì§€ìš©)
pip install --upgrade pillow

# ë¦¬ëˆ…ìŠ¤(Ubuntu) í™˜ê²½ì—ì„œ í•œê¸€ í°íŠ¸ê°€ ê¹¨ì§ˆ ê²½ìš° ì•„ë˜ ëª…ë ¹ì–´ë¡œ ë‚˜ëˆ”ê¸€ê¼´ ì„¤ì¹˜
sudo apt update
sudo apt install fonts-nanum

ğŸ’¡fonts-nanumì€ í•œê¸€ì„ ê¹¨ì§€ì§€ ì•Šê²Œ í‘œì‹œí•˜ê¸° ìœ„í•´ í•„ìš”í•©ë‹ˆë‹¤. ì„¤ì¹˜ í›„ ì½”ë“œì—ì„œ ë‹¤ìŒê³¼ ê°™ì´ ê²½ë¡œë¥¼ ì„¤ì •í•˜ì„¸ìš”:
font_path = "/usr/share/fonts/truetype/nanum/NanumGothic.ttf"

# Ollama ì„¤ì¹˜ (Snap ê¸°ë°˜)
sudo snap install ollama

## ğŸ“‚ ì‚¬ìš© ë°©ë²•

### 1ë‹¨ê³„: ëª¨ë¸ ë³€í™˜

ë¨¼ì € H5 ëª¨ë¸ì„ ONNX/TFLiteë¡œ ë³€í™˜í•©ë‹ˆë‹¤:


```bash
python3 convert_h5_to_onnx.py
```


**ë³€í™˜ ê²°ê³¼:**
- âœ… `skin_model.onnx` - ONNX ëª¨ë¸ ìƒì„±
- âœ… `skin_model_quantized.tflite` - 8ë¹„íŠ¸ ì–‘ìí™” TFLite ëª¨ë¸ ìƒì„±
- âœ… `skin_model_quantized_dynamic.onxx` - 8ë¹„íŠ¸ ë™ì  ì–‘ìí™” onxx ëª¨ë¸ ìƒì„±
- âœ… `skin_model_static_dynamic.onxx` - 8ë¹„íŠ¸ ì •ì  ì–‘ìí™” onxx ëª¨ë¸ ìƒì„±
 


### 2ë‹¨ê³„: ì§„ë‹¨ í”„ë¡œê·¸ë¨ ì‹¤í–‰

```bash
# ollama ì‹¤í–‰ (í„°ë¯¸ë„ í•˜ë‚˜ ë” ì—´ì–´ì„œ ì§„í–‰í–‰)
ollama run gemma3:1b

```

```bash
# h5 ê¸°ë³¸
python3 camera_h5_diagnosis.py

# onxx ê¸°ë³¸
python3 camera_onnx_diagnosis.py

# onxx runtime ì ìš©
python3 camera_onnx_optimized.py

```

</div>
</details>




## ğŸ¯ ëª¨ë¸ ìš°ì„ ìˆœìœ„

í”„ë¡œê·¸ë¨ì€ ë‹¤ìŒ ìˆœì„œë¡œ ëª¨ë¸ì„ ë¡œë“œí•©ë‹ˆë‹¤:

1. **ONNX ëª¨ë¸** (ìµœìš°ì„ ) - ê°€ì¥ ë¹ ë¥¸ ì¶”ë¡  ì†ë„
2. **TFLite ëª¨ë¸** (8ë¹„íŠ¸ ì–‘ìí™”) - ì‘ì€ íŒŒì¼ í¬ê¸°, ë¹ ë¥¸ ì†ë„
3. **ì›ë³¸ H5 ëª¨ë¸** (ë°±ì—…) - ë³€í™˜ ì‹¤íŒ¨ ì‹œ ì‚¬ìš©

## ğŸ“Š ì§„ë‹¨ í´ë˜ìŠ¤ (7ê°œ)

1. **ê¸°ì €ì„¸í¬ì•”** - ê°€ì¥ í”í•œ í”¼ë¶€ì•”
2. **í‘œí”¼ë‚­ì¢…** - ì–‘ì„± ë‚­ì¢…
3. **í˜ˆê´€ì¢…** - í˜ˆê´€ ì¦ì‹ ë³‘ë³€ 
4. **ë¹„ë¦½ì¢…** - ì‘ì€ ê°ì§ˆ ì£¼ë¨¸ë‹ˆ
5. **ì •ìƒí”¼ë¶€** - ê±´ê°•í•œ í”¼ë¶€
6. **í¸í‰ì„¸í¬ì•”** - ë‘ ë²ˆì§¸ í”í•œ í”¼ë¶€ì•”
7. **ì‚¬ë§ˆê·€** - HPV ê°ì—¼

## ğŸ® ì¡°ì‘ ë°©ë²•

### ì‹¤ì‹œê°„ ëª¨ë“œ
- **ì¹´ë©”ë¼ í™”ë©´**: ì‹¤ì‹œê°„ ì˜ˆì¸¡ ê²°ê³¼ í‘œì‹œ
- **ëª¨ë¸ ì •ë³´**: ì‚¬ìš© ì¤‘ì¸ ëª¨ë¸ íƒ€ì… (ONNX/TFLite/H5) í‘œì‹œ

### ì§„ë‹¨ ëª¨ë“œ
- **'c' í‚¤**: 5ì´ˆê°„ ì—°ì† ì´¬ì˜í•˜ì—¬ ì •í™•í•œ ì§„ë‹¨ ì‹œì‘
- **ì§„ë‹¨ ê³¼ì •**: 5ë²ˆ ì´¬ì˜ â†’ ê²°ê³¼ ì¼ì¹˜ í™•ì¸ â†’ ìµœì¢… ì§„ë‹¨
- **AI ì¡°ì–¸**: Ollama Gemma3ë¥¼ í†µí•œ ê°œì¸ë§ì¶¤ ê±´ê°• ì¡°ì–¸

### ê¸°íƒ€
- **'q' í‚¤**: í”„ë¡œê·¸ë¨ ì¢…ë£Œ

## ğŸ”§ ì„±ëŠ¥ ìµœì í™”

### ëª¨ë¸ í¬ê¸° ë¹„êµ
- **ì›ë³¸ H5**: ~50MB
- **ONNX**: ~45MB (10% ê°ì†Œ)
- **TFLite ì–‘ìí™”**: ~12MB (75% ê°ì†Œ)

### ì¶”ë¡  ì†ë„
- **ONNX**: ê°€ì¥ ë¹ ë¦„ (CPU ìµœì í™”)
- **TFLite**: ë¹ ë¦„ (ë©”ëª¨ë¦¬ íš¨ìœ¨ì )
- **H5**: ë³´í†µ (TensorFlow ì˜¤ë²„í—¤ë“œ)

## ğŸ› ï¸ ë¬¸ì œ í•´ê²°

### ëª¨ë¸ ë¡œë“œ ì‹¤íŒ¨
```bash
âŒ ONNX ëª¨ë¸ ë¡œë“œ ì‹¤íŒ¨: ...
âŒ TFLite ëª¨ë¸ ë¡œë“œ ì‹¤íŒ¨: ...
âœ… ì›ë³¸ H5 ëª¨ë¸ì„ ì‚¬ìš©í•©ë‹ˆë‹¤.
```
â†’ ë³€í™˜ ê³¼ì •ì„ ë‹¤ì‹œ ì‹¤í–‰í•˜ì„¸ìš”.

### ì¹´ë©”ë¼ ì ‘ê·¼ ì‹¤íŒ¨
```bash
âŒ ì¹´ë©”ë¼ë¥¼ ì—´ ìˆ˜ ì—†ìŠµë‹ˆë‹¤.
```
â†’ ë‹¤ë¥¸ í”„ë¡œê·¸ë¨ì´ ì¹´ë©”ë¼ë¥¼ ì‚¬ìš© ì¤‘ì¸ì§€ í™•ì¸í•˜ì„¸ìš”.

### Ollama ì—°ê²° ì‹¤íŒ¨
```bash
Ollama ëª¨ë¸ì„ í˜¸ì¶œí•˜ëŠ” ì¤‘ ì˜¤ë¥˜ê°€ ë°œìƒí–ˆìŠµë‹ˆë‹¤...
```
â†’ `ollama run gemma3` ëª…ë ¹ìœ¼ë¡œ ëª¨ë¸ì„ ì‹¤í–‰í•˜ì„¸ìš”.

## ğŸ“ˆ ì¶”ê°€ ê¸°ëŠ¥

### ì´ë¯¸ì§€ ì €ì¥
- ì§„ë‹¨ ì‹œ ëª¨ë“  ìº¡ì²˜ ì´ë¯¸ì§€ê°€ `captures/` í´ë”ì— ì €ì¥ë©ë‹ˆë‹¤
- íŒŒì¼ëª…: `capture_YYYYMMDD_HHMMSS_N.png`

### AI ê±´ê°• ì¡°ì–¸
- Ollama Gemma3 ëª¨ë¸ì„ í†µí•œ ì‹¤ì‹œê°„ ê±´ê°• ì¡°ì–¸ ì œê³µ
- ê°„ê²°í•˜ê³  ì‹¤ìš©ì ì¸ ì •ë³´ ì œê³µ (200ì ë‚´ì™¸)

## âš ï¸ ì£¼ì˜ì‚¬í•­

1. **ì˜í•™ì  ì¡°ì–¸ ì•„ë‹˜**: ì´ ì‹œìŠ¤í…œì€ ì°¸ê³ ìš©ì´ë©°, ì •í™•í•œ ì§„ë‹¨ì€ ì „ë¬¸ì˜ì™€ ìƒë‹´í•˜ì„¸ìš”.
2. **ì¡°ëª… ì¡°ê±´**: ì¶©ë¶„í•œ ì¡°ëª…ì—ì„œ ì‚¬ìš©í•˜ì„¸ìš”.
3. **ì¹´ë©”ë¼ ìœ„ì¹˜**: ì§„ë‹¨ ë¶€ìœ„ë¥¼ í™”ë©´ ì¤‘ì•™ì— ìœ„ì¹˜ì‹œí‚¤ì„¸ìš”.
4. **ì •í™•ì„±**: 5ë²ˆ ì—°ì† ì´¬ì˜ì—ì„œ ê°™ì€ ê²°ê³¼ê°€ ë‚˜ì™€ì•¼ ì‹ ë¢°í•  ìˆ˜ ìˆìŠµë‹ˆë‹¤.

## ğŸ“ ê¸°ìˆ  ì§€ì›

ë¬¸ì œê°€ ë°œìƒí•˜ë©´ ë‹¤ìŒ ì‚¬í•­ì„ í™•ì¸í•´ì£¼ì„¸ìš”:

1. **Python ë²„ì „**: 3.8 ì´ìƒ ê¶Œì¥
2. **íŒ¨í‚¤ì§€ ë²„ì „**: ìµœì‹  ë²„ì „ ì‚¬ìš© ê¶Œì¥
3. **ëª¨ë¸ íŒŒì¼**: ë³€í™˜ëœ ëª¨ë¸ íŒŒì¼ì´ ì •ìƒì ìœ¼ë¡œ ìƒì„±ë˜ì—ˆëŠ”ì§€ í™•ì¸
4. **í•˜ë“œì›¨ì–´**: ì¶©ë¶„í•œ RAMê³¼ CPU ì„±ëŠ¥ í•„ìš”

---

**Â© 2024 ONNX ê¸°ë°˜ í”¼ë¶€ ì§ˆí™˜ ì§„ë‹¨ ì‹œìŠ¤í…œ** 

## requirements.txt
```
# ìµœì í™”ëœ ONNX ê¸°ë°˜ í”¼ë¶€ ì§ˆí™˜ ì§„ë‹¨ ì‹œìŠ¤í…œ í•„ìˆ˜ íŒ¨í‚¤ì§€

# ë¨¸ì‹ ëŸ¬ë‹ ë° ë”¥ëŸ¬ë‹
tensorflow>=2.10.0
numpy>=1.21.0

# ONNX ê´€ë ¨ (ìµœì í™” ë²„ì „)
onnxruntime>=1.16.0
onnxruntime-gpu>=1.16.0  # GPU ì§€ì› (CUDA/DirectML)
tf2onnx>=1.14.0
onnx>=1.14.0

# ì»´í“¨í„° ë¹„ì „
opencv-python>=4.7.0
Pillow>=9.0.0

# ì‹œìŠ¤í…œ ëª¨ë‹ˆí„°ë§ ë° ìµœì í™”
psutil>=5.9.0

# AI ê±´ê°• ì¡°ì–¸ (ì„ íƒì‚¬í•­)
ollama>=0.1.0

# ì„±ëŠ¥ ìµœì í™” (ì„ íƒì‚¬í•­)
# onnxruntime-openvino  # Intel OpenVINO ì§€ì›
# onnxruntime-directml  # DirectML ì§€ì› (Windows) 
```

## Diagnosis.py
```python
import numpy as np
import cv2
from PIL import ImageFont, ImageDraw, Image
import time
import os
import ollama
import onnxruntime as ort

# --- ì„¤ì • ---
CAPTURE_INTERVAL = 1  # ìº¡ì²˜ ê°„ê²© (ì´ˆ)
CAPTURE_COUNT = 5     # ìº¡ì²˜ íšŸìˆ˜
CAPTURE_FOLDER = "captures" # ìº¡ì²˜ ì´ë¯¸ì§€ ì €ì¥ í´ë”
OLLAMA_MODEL = "gemma3:1b-it-qat" # ì‚¬ìš©í•  Ollama ëª¨ë¸

# ëª¨ë¸ ê²½ë¡œ ì„¤ì •
ONNX_MODEL_PATH = "./model/skin_model.onnx"
TFLITE_MODEL_PATH = "./model/skin_model_quantized.tflite"

# --- í´ë˜ìŠ¤ ë° ëª¨ë¸ ì„¤ì • ---
# í´ë˜ìŠ¤ëª… (7ê°œ í´ë˜ìŠ¤)
class_names_kr = [
    'ê¸°ì €ì„¸í¬ì•”',
    'í‘œí”¼ë‚­ì¢…',
    'í˜ˆê´€ì¢…',
    'ë¹„ë¦½ì¢…',
    'ì •ìƒí”¼ë¶€',
    'í¸í‰ì„¸í¬ì•”',
    'ì‚¬ë§ˆê·€'
]


class ONNXModel:
    def __init__(self, model_path):
        """ONNX ëª¨ë¸ ë¡œë“œ"""
        self.session = ort.InferenceSession(model_path)
        self.input_name = self.session.get_inputs()[0].name
        self.output_name = self.session.get_outputs()[0].name
        print(f"ONNX ëª¨ë¸ ë¡œë“œ ì„±ê³µ: {model_path}")
    
    def predict(self, input_data):
        """ONNX ëª¨ë¸ ì˜ˆì¸¡"""
        result = self.session.run([self.output_name], {self.input_name: input_data})
        return result[0]

# --- TFLite ëª¨ë¸ í´ë˜ìŠ¤ ---
class TFLiteModel:
    def __init__(self, model_path):
        """TFLite ëª¨ë¸ ë¡œë“œ"""
        import tensorflow as tf
        self.interpreter = tf.lite.Interpreter(model_path=model_path)
        self.interpreter.allocate_tensors()
        
        # ì…ë ¥ ë° ì¶œë ¥ í…ì„œ ì •ë³´
        self.input_details = self.interpreter.get_input_details()
        self.output_details = self.interpreter.get_output_details()
        print(f"TFLite ëª¨ë¸ ë¡œë“œ ì„±ê³µ: {model_path}")
    
    def predict(self, input_data):
        """TFLite ëª¨ë¸ ì˜ˆì¸¡"""
        # ì…ë ¥ ë°ì´í„° ì„¤ì •
        self.interpreter.set_tensor(self.input_details[0]['index'], input_data)
        
        # ì¶”ë¡  ì‹¤í–‰
        self.interpreter.invoke()
        
        # ì¶œë ¥ ë°ì´í„° ê°€ì ¸ì˜¤ê¸°
        output_data = self.interpreter.get_tensor(self.output_details[0]['index'])
        return output_data

# --- Ollama Gemma3 í•¨ìˆ˜ ---
def get_solution_from_gemma(disease_name):
    """
    Ollamaë¥¼ í†µí•´ ë¡œì»¬ Gemma3 ëª¨ë¸ì„ í˜¸ì¶œí•˜ì—¬ ì§ˆí™˜ì— ëŒ€í•œ í•´ê²°ì±…ì„ ìƒì„±í•©ë‹ˆë‹¤.
    """
    prompt = f"""ë‹¹ì‹ ì€ í”¼ë¶€ ê±´ê°• ì „ë¬¸ AI ì–´ì‹œìŠ¤í„´íŠ¸ì…ë‹ˆë‹¤.
ë‹¤ìŒ í”¼ë¶€ ì§ˆí™˜ì— ëŒ€í•´ ì¼ë°˜ì¸ì´ ì´í•´í•˜ê¸° ì‰½ê²Œ ì„¤ëª…í•˜ê³ , ì§‘ì—ì„œ í•  ìˆ˜ ìˆëŠ” ê´€ë¦¬ ë°©ë²•ê³¼ ì „ë¬¸ì ì¸ ì¹˜ë£Œ ë°©ë²•ì„ ë‹¨ê³„ë³„ë¡œ ëª…í™•í•˜ê²Œ êµ¬ë¶„í•´ì„œ í•œêµ­ì–´ë¡œ ì•Œë ¤ì£¼ì„¸ìš”.

**ì§ˆí™˜ëª…: {disease_name}**

ë‹¤ìŒ í˜•ì‹ìœ¼ë¡œ ë‹µë³€í•´ì£¼ì„¸ìš”:
1. ì§ˆí™˜ ì„¤ëª… (ê°„ë‹¨ëª…ë£Œí•˜ê²Œ)
2. ì¦‰ì‹œ ì¡°ì¹˜ì‚¬í•­ (ì‘ê¸‰ì„± ì—¬ë¶€)
3. ê°€ì • ê´€ë¦¬ ë°©ë²•
4. ì „ë¬¸ ì¹˜ë£Œ ë°©ë²•
5. ì£¼ì˜ì‚¬í•­

200ì ë‚´ì™¸ë¡œ ê°„ê²°í•˜ê²Œ ë‹µë³€í•´ì£¼ì„¸ìš”."""
    
    print(f"\n[{OLLAMA_MODEL} ëª¨ë¸ì—ê²Œ ì¡°ì–¸ì„ ìš”ì²­í•©ë‹ˆë‹¤... ì ì‹œë§Œ ê¸°ë‹¤ë ¤ ì£¼ì„¸ìš”.]")
    
    try:
        # Ollama ì„œë²„ì™€ í†µì‹ 
        response = ollama.chat(
            model=OLLAMA_MODEL,
            messages=[
                {
                    'role': 'user',
                    'content': prompt,
                },
            ]
        )
        return response['message']['content']
    except Exception as e:
        return f"Ollama ëª¨ë¸ì„ í˜¸ì¶œí•˜ëŠ” ì¤‘ ì˜¤ë¥˜ê°€ ë°œìƒí–ˆìŠµë‹ˆë‹¤: {e}\nOllamaê°€ ì‹¤í–‰ ì¤‘ì¸ì§€ í™•ì¸í•˜ì„¸ìš”. í„°ë¯¸ë„ì—ì„œ 'ollama run {OLLAMA_MODEL}' ëª…ë ¹ì„ ì‹¤í–‰í•´ì•¼ í•  ìˆ˜ ìˆìŠµë‹ˆë‹¤."

# --- ëª¨ë¸ ì´ˆê¸°í™” í•¨ìˆ˜ ---
def initialize_model():
    """ì‚¬ìš© ê°€ëŠ¥í•œ ëª¨ë¸ì„ ì´ˆê¸°í™”í•©ë‹ˆë‹¤."""
    model = None
    model_type = None
    
    # 1. ONNX ëª¨ë¸ ì‹œë„
    if os.path.exists(ONNX_MODEL_PATH):
        try:
            model = ONNXModel(ONNX_MODEL_PATH)
            model_type = "ONNX"
            print("ONNX ëª¨ë¸ì„ ì‚¬ìš©í•©ë‹ˆë‹¤.")
        except Exception as e:
            print(f"ONNX ëª¨ë¸ ë¡œë“œ ì‹¤íŒ¨: {e}")
    
    # 2. TFLite ëª¨ë¸ ì‹œë„ (ONNX ì‹¤íŒ¨ ì‹œ)
    if model is None and os.path.exists(TFLITE_MODEL_PATH):
        try:
            model = TFLiteModel(TFLITE_MODEL_PATH)
            model_type = "TFLite"
            print("TFLite ëª¨ë¸ì„ ì‚¬ìš©í•©ë‹ˆë‹¤.")
        except Exception as e:
            print(f"TFLite ëª¨ë¸ ë¡œë“œ ì‹¤íŒ¨: {e}")
    
    # 3. ì›ë³¸ H5 ëª¨ë¸ ì‹œë„ (ë‘˜ ë‹¤ ì‹¤íŒ¨ ì‹œ)
    if model is None:
        try:
            import tensorflow as tf
            from tensorflow import keras
            h5_model_path = "C:/Users/kccistc/project/pth/jaehong_skin_model.h5"
            model = keras.models.load_model(h5_model_path)
            model_type = "H5"
            print("ì›ë³¸ H5 ëª¨ë¸ì„ ì‚¬ìš©í•©ë‹ˆë‹¤.")
        except Exception as e:
            print(f"H5 ëª¨ë¸ ë¡œë“œ ì‹¤íŒ¨: {e}")
    
    return model, model_type

# --- ë©”ì¸ ë¡œì§ ---
def main():
    print("ONNX Skin Diagnosis System")
    print("=" * 50)
    
    # ìº¡ì²˜ í´ë” ìƒì„±
    if not os.path.exists(CAPTURE_FOLDER):
        os.makedirs(CAPTURE_FOLDER)
    
    # ëª¨ë¸ ì´ˆê¸°í™”
    model, model_type = initialize_model()
    if model is None:
        print("ì‚¬ìš© ê°€ëŠ¥í•œ ëª¨ë¸ì´ ì—†ìŠµë‹ˆë‹¤.")
        print("ë¨¼ì € convert_h5_to_onnx.pyë¥¼ ì‹¤í–‰í•˜ì—¬ ëª¨ë¸ì„ ë³€í™˜í•˜ì„¸ìš”.")
        return
    
    print(f"ì‚¬ìš© ì¤‘ì¸ ëª¨ë¸: {model_type}")
    
    # í°íŠ¸ ì„¤ì •
    font_path = "/usr/share/fonts/truetype/nanum/NanumGothicExtraBold.ttf"
    try:
        font = ImageFont.truetype(font_path, 20)
    except IOError:
        print(f"ì˜¤ë¥˜: í°íŠ¸ íŒŒì¼ì„ ì°¾ì„ ìˆ˜ ì—†ìŠµë‹ˆë‹¤: {font_path}. ê¸°ë³¸ í°íŠ¸ë¥¼ ì‚¬ìš©í•©ë‹ˆë‹¤.")
        font = ImageFont.load_default()

    # ì¹´ë©”ë¼ ì„¤ì •
    cap = cv2.VideoCapture(1) # ì™¸ë¶€ ì›¹ìº 
    if not cap.isOpened():
        cap = cv2.VideoCapture(0) # ë‚´ì¥ ì›¹ìº 
        if not cap.isOpened():
            print("ì¹´ë©”ë¼ë¥¼ ì—´ ìˆ˜ ì—†ìŠµë‹ˆë‹¤.")
            return

    print("ì¹´ë©”ë¼ê°€ ì¤€ë¹„ë˜ì—ˆìŠµë‹ˆë‹¤.")
    print("í™”ë©´ì„ ë³´ë©° ì§„ë‹¨í•  ë¶€ìœ„ë¥¼ ì¤‘ì•™ì— ìœ„ì¹˜ì‹œí‚¤ì„¸ìš”.")
    print("í‚¤ë³´ë“œ 'c'ë¥¼ ëˆ„ë¥´ë©´ 5ì´ˆê°„ ì—°ì†ìœ¼ë¡œ ì´¬ì˜í•˜ì—¬ ì§„ë‹¨í•©ë‹ˆë‹¤.")
    print("í‚¤ë³´ë“œ 'q'ë¥¼ ëˆ„ë¥´ë©´ í”„ë¡œê·¸ë¨ì„ ì¢…ë£Œí•©ë‹ˆë‹¤.")

    while True:
        ret, frame = cap.read()
        if not ret:
            print("ì˜¤ë¥˜: ì¹´ë©”ë¼ì—ì„œ í”„ë ˆì„ì„ ì½ì„ ìˆ˜ ì—†ìŠµë‹ˆë‹¤.")
            break

        # ì¤‘ì•™ 1:1 ì˜ì—­ crop
        h, w, _ = frame.shape
        min_dim = min(h, w)
        start_x = (w - min_dim) // 2
        start_y = (h - min_dim) // 2
        crop_frame = frame[start_y:start_y+min_dim, start_x:start_x+min_dim]

        # --- ì‹¤ì‹œê°„ ì˜ˆì¸¡ ---
        # ì´ë¯¸ì§€ ì „ì²˜ë¦¬
        img_array = cv2.resize(crop_frame, (96, 96))
        img_array = np.expand_dims(img_array, axis=0)  # ë°°ì¹˜ ì°¨ì› ì¶”ê°€
        img_array = img_array.astype(np.float32) / 255.0  # ì •ê·œí™”

        # ëª¨ë¸ íƒ€ì…ì— ë”°ë¥¸ ì˜ˆì¸¡
        if model_type == "H5":
            predictions = model.predict(img_array, verbose=0)
        else:
            predictions = model.predict(img_array)
        
        predicted_class_idx = np.argmax(predictions[0])
        confidence = predictions[0][predicted_class_idx]

        # ê²°ê³¼ í…ìŠ¤íŠ¸ ìƒì„±
        if predicted_class_idx < len(class_names_kr):
            label = f"{class_names_kr[predicted_class_idx]} ({confidence*100:.1f}%)"
        else:
            label = f"ì•Œ ìˆ˜ ì—†ìŒ ({confidence*100:.1f}%)"
 
        # í™”ë©´ì— í‘œì‹œ (Pillow ì‚¬ìš©)
        img_pil = Image.fromarray(cv2.cvtColor(crop_frame, cv2.COLOR_BGR2RGB))
        draw = ImageDraw.Draw(img_pil)
        draw.text((10, 10), f"ì‹¤ì‹œê°„ ì˜ˆì¸¡ ({model_type}):", font=font, fill=(0, 255, 0))
        draw.text((10, 35), label, font=font, fill=(0, 255, 0))
        display_frame = cv2.cvtColor(np.array(img_pil), cv2.COLOR_RGB2BGR)

        cv2.imshow('ONNX Skin Disease Diagnosis', display_frame)

        key = cv2.waitKey(1) & 0xFF

        # --- 'c' í‚¤ë¥¼ ëˆŒëŸ¬ ì—°ì† ìº¡ì²˜ ë° ì§„ë‹¨ ---
        if key == ord('c'):
            # í™”ë©´ì„ ê²€ê²Œ ë§Œë“¤ê³  "ì˜ì‚¬ì˜ ë‹µë³€ ì¤€ë¹„ì¤‘..." ë©”ì‹œì§€ í‘œì‹œ
            black_screen = np.zeros_like(display_frame)
            
            # Pillowë¥¼ ì‚¬ìš©í•˜ì—¬ í…ìŠ¤íŠ¸ ì¶”ê°€
            img_pil_black = Image.fromarray(cv2.cvtColor(black_screen, cv2.COLOR_BGR2RGB))
            draw_black = ImageDraw.Draw(img_pil_black)
            
            text = "ì˜ì‚¬ì˜ ë‹µë³€ ì¤€ë¹„ì¤‘..."
            
            # í…ìŠ¤íŠ¸ í¬ê¸° ê³„ì‚°
            try:
                # Pillow 10.0.0 ì´ìƒ
                text_bbox = draw_black.textbbox((0, 0), text, font=font)
                text_width = text_bbox[2] - text_bbox[0]
                text_height = text_bbox[3] - text_bbox[1]
            except AttributeError:
                # ì´ì „ ë²„ì „ì˜ Pillow
                text_width, text_height = draw_black.textsize(text, font=font)

            text_x = (black_screen.shape[1] - text_width) // 2
            text_y = (black_screen.shape[0] - text_height) // 2
            
            draw_black.text((text_x, text_y), text, font=font, fill=(255, 255, 255))
            
            # OpenCV í˜•ì‹ìœ¼ë¡œ ë‹¤ì‹œ ë³€í™˜í•˜ì—¬ í‘œì‹œ
            black_screen_with_text = cv2.cvtColor(np.array(img_pil_black), cv2.COLOR_RGB2BGR)
            cv2.imshow('ONNX Skin Disease Diagnosis', black_screen_with_text)
            cv2.waitKey(1) # í™”ë©´ì„ ì¦‰ì‹œ ì—…ë°ì´íŠ¸

            print("\n" + "="*40)
            print(f"ì§„ë‹¨ì„ ì‹œì‘í•©ë‹ˆë‹¤. {CAPTURE_COUNT}ì´ˆ ë™ì•ˆ {CAPTURE_COUNT}ë²ˆ ì´¬ì˜í•©ë‹ˆë‹¤.")
            print("="*40)
            
            captured_classes = []
            
            for i in range(CAPTURE_COUNT):
                time.sleep(CAPTURE_INTERVAL)
                
                # í˜„ì¬ í”„ë ˆì„(crop_frame)ìœ¼ë¡œ ì˜ˆì¸¡
                current_img_array = cv2.resize(crop_frame, (96, 96))
                current_img_array = np.expand_dims(current_img_array, axis=0)
                current_img_array = current_img_array.astype(np.float32) / 255.0

                # ëª¨ë¸ íƒ€ì…ì— ë”°ë¥¸ ì˜ˆì¸¡
                if model_type == "H5":
                    current_predictions = model.predict(current_img_array, verbose=0)
                else:
                    current_predictions = model.predict(current_img_array)
                
                current_predicted_idx = np.argmax(current_predictions[0])
                
                predicted_name = class_names_kr[current_predicted_idx]
                captured_classes.append(predicted_name)
                
                # ìº¡ì²˜ ì´ë¯¸ì§€ ì €ì¥
                timestamp = time.strftime("%Y%m%d_%H%M%S")
                capture_path = os.path.join(CAPTURE_FOLDER, f"capture_{timestamp}_{i+1}.png")
                cv2.imwrite(capture_path, crop_frame)
                
                print(f"ì´¬ì˜ {i+1}/5... ì˜ˆì¸¡: {predicted_name} (ì´ë¯¸ì§€ ì €ì¥: {capture_path})")

            # --- ìµœì¢… ì§„ë‹¨ ---
            print("\n" + "-"*40)
            if len(set(captured_classes)) == 1:
                final_diagnosis = captured_classes[0]
                print(f"ìµœì¢… ì§„ë‹¨ ê²°ê³¼: **{final_diagnosis}**")
                print(f"ì‚¬ìš© ëª¨ë¸: {model_type}")
                print("-"*40)
                
                # Gemma3 í•´ê²°ì±… ìš”ì²­
                solution = get_solution_from_gemma(final_diagnosis)
                print("\n[Ollama Gemma3ì˜ ê±´ê°• ì¡°ì–¸]")
                print(solution)
                print("\n(ì£¼ì˜: ì´ ì •ë³´ëŠ” ì°¸ê³ ìš©ì´ë©°, ì •í™•í•œ ì§„ë‹¨ê³¼ ì¹˜ë£Œë¥¼ ìœ„í•´ ë°˜ë“œì‹œ ì „ë¬¸ ì˜ë£Œê¸°ê´€ì„ ë°©ë¬¸í•˜ì„¸ìš”.)")
                
            else:
                print("ì§„ë‹¨ ì‹¤íŒ¨: ì˜ˆì¸¡ ê²°ê³¼ê°€ ì¼ì¹˜í•˜ì§€ ì•ŠìŠµë‹ˆë‹¤.")
                print(f"ì§€ë‚œ {CAPTURE_COUNT}ë²ˆì˜ ì˜ˆì¸¡: {captured_classes}")
            
            print("="*40)
            print("\në‹¤ì‹œ ì§„ë‹¨í•˜ë ¤ë©´ 'c'ë¥¼, ì¢…ë£Œí•˜ë ¤ë©´ 'q'ë¥¼ ëˆ„ë¥´ì„¸ìš”.")

        # --- 'q' í‚¤ë¥¼ ëˆŒëŸ¬ ì¢…ë£Œ ---
        elif key == ord('q'):
            print("í”„ë¡œê·¸ë¨ì„ ì¢…ë£Œí•©ë‹ˆë‹¤.")
            break

    cap.release()
    cv2.destroyAllWindows()

if __name__ == "__main__":
    main() 
```

## Optimized.py
```python
import numpy as np
import cv2
from PIL import ImageFont, ImageDraw, Image
import time
import os
import ollama
import onnxruntime as ort
from onnxruntime.quantization import quantize_dynamic, QuantType
import psutil
import threading
import queue

# --- ì„¤ì • ---
CAPTURE_INTERVAL = 1  # ìº¡ì²˜ ê°„ê²© (ì´ˆ)
CAPTURE_COUNT = 5     # ìº¡ì²˜ íšŸìˆ˜
CAPTURE_FOLDER = "captures" # ìº¡ì²˜ ì´ë¯¸ì§€ ì €ì¥ í´ë”
OLLAMA_MODEL = "gemma3" # ì‚¬ìš©í•  Ollama ëª¨ë¸

# ëª¨ë¸ ê²½ë¡œ ì„¤ì •

ONNX_MODEL_PATH = "./model/skin_model.onnx"
ONNX_OPTIMIZED_PATH = "./model/skin_model_optimized.onnx"
ONNX_QUANTIZED_PATH = "./model/skin_model_quantized.onnx"
TFLITE_MODEL_PATH = "./model/skin_model_quantized.tflite"

# --- í´ë˜ìŠ¤ ë° ëª¨ë¸ ì„¤ì • ---
class_names_kr = [
    'ê¸°ì €ì„¸í¬ì•”',
    'ë³´ì›¬ë³‘',
    'í‘œí”¼ë‚­ì¢…',
    'ë¹„ë¦½ì¢…',
    'ì •ìƒí”¼ë¶€',
    'í™”ë†ì„± ìœ¡ì•„ì¢…',
    'í¸í‰ì„¸í¬ì•”',
    'ì‚¬ë§ˆê·€'
]

# --- ìµœì í™”ëœ ONNX ëª¨ë¸ í´ë˜ìŠ¤ ---
class OptimizedONNXModel:
    def __init__(self, model_path, optimization_level="all", use_gpu=False):
        """
        ìµœì í™”ëœ ONNX ëª¨ë¸ ë¡œë“œ
        
        Args:
            model_path: ëª¨ë¸ ê²½ë¡œ
            optimization_level: ìµœì í™” ë ˆë²¨ ("disable", "basic", "extended", "all")
            use_gpu: GPU ì‚¬ìš© ì—¬ë¶€
        """
        self.model_path = model_path
        self.optimization_level = optimization_level
        self.use_gpu = use_gpu
        
        # ì„¸ì…˜ ì˜µì…˜ ì„¤ì •
        self.session_options = ort.SessionOptions()
        
        # ìµœì í™” ë ˆë²¨ ì„¤ì •
        if optimization_level == "disable":
            self.session_options.graph_optimization_level = ort.GraphOptimizationLevel.ORT_DISABLE_ALL
        elif optimization_level == "basic":
            self.session_options.graph_optimization_level = ort.GraphOptimizationLevel.ORT_ENABLE_BASIC
        elif optimization_level == "extended":
            self.session_options.graph_optimization_level = ort.GraphOptimizationLevel.ORT_ENABLE_EXTENDED
        else:  # "all"
            self.session_options.graph_optimization_level = ort.GraphOptimizationLevel.ORT_ENABLE_ALL
        
        # ë³‘ë ¬ ì²˜ë¦¬ ì„¤ì •
        cpu_count = psutil.cpu_count(logical=False)
        self.session_options.intra_op_num_threads = cpu_count
        self.session_options.inter_op_num_threads = cpu_count
        
        # ë©”ëª¨ë¦¬ íŒ¨í„´ ìµœì í™”
        self.session_options.enable_mem_pattern = True
        self.session_options.enable_cpu_mem_arena = True
        
        # ì‹¤í–‰ ì œê³µì ì„¤ì •
        providers = self._get_providers()
        
        try:
            # ONNX ì„¸ì…˜ ìƒì„±
            self.session = ort.InferenceSession(
                model_path, 
                sess_options=self.session_options,
                providers=providers
            )
            
            # ì…ì¶œë ¥ ì •ë³´
            self.input_name = self.session.get_inputs()[0].name
            self.output_name = self.session.get_outputs()[0].name
            
            # ëª¨ë¸ ì •ë³´ ì¶œë ¥
            print(f" ìµœì í™”ëœ ONNX ëª¨ë¸ ë¡œë“œ ì„±ê³µ: {model_path}")
            print(f"    ìµœì í™” ë ˆë²¨: {optimization_level}")
            print(f"    Intra-op threads: {self.session_options.intra_op_num_threads}")
            print(f"    Inter-op threads: {self.session_options.inter_op_num_threads}")
            print(f"    ì‚¬ìš© ì¤‘ì¸ Providers: {self.session.get_providers()}")
            
        except Exception as e:
            print(f" ìµœì í™”ëœ ONNX ëª¨ë¸ ë¡œë“œ ì‹¤íŒ¨: {e}")
            raise
    
    def _get_providers(self):
        """ì‚¬ìš© ê°€ëŠ¥í•œ ì‹¤í–‰ ì œê³µì ë°˜í™˜"""
        providers = []
        
        # GPU ì‚¬ìš© ì‹œë„
        if self.use_gpu:
            # DirectML (Windows)
            if 'DmlExecutionProvider' in ort.get_available_providers():
                providers.append('DmlExecutionProvider')
                print(" DirectML Provider ì‚¬ìš© (Windows GPU)")
            
            # CUDA (NVIDIA)
            if 'CUDAExecutionProvider' in ort.get_available_providers():
                providers.append('CUDAExecutionProvider')
                print(" CUDA Provider ì‚¬ìš© (NVIDIA GPU)")
            
            # OpenVINO (Intel)
            if 'OpenVINOExecutionProvider' in ort.get_available_providers():
                providers.append('OpenVINOExecutionProvider')
                print(" OpenVINO Provider ì‚¬ìš© (Intel GPU)")
        
        # CPUëŠ” í•­ìƒ ë°±ì—…ìœ¼ë¡œ ì¶”ê°€
        providers.append('CPUExecutionProvider')
        
        return providers
    
    def predict(self, input_data):
        """ìµœì í™”ëœ ì˜ˆì¸¡"""
        try:
            result = self.session.run([self.output_name], {self.input_name: input_data})
            return result[0]
        except Exception as e:
            print(f" ì˜ˆì¸¡ ì‹¤íŒ¨: {e}")
            return None

# --- ë™ì  ì–‘ìí™” í•¨ìˆ˜ ---
def create_quantized_model(onnx_model_path, quantized_model_path):
    """ë™ì  ì–‘ìí™”ëœ ONNX ëª¨ë¸ ìƒì„±"""
    try:
        print(" ë™ì  ì–‘ìí™” ì‹œì‘...")
        
        # ë™ì  ì–‘ìí™” ì‹¤í–‰
        quantized_model = quantize_dynamic(
            onnx_model_path,
            quantized_model_path,
            weight_type=QuantType.QUInt8,
            optimize_model=True
        )
        
        # íŒŒì¼ í¬ê¸° ë¹„êµ
        original_size = os.path.getsize(onnx_model_path) / (1024 * 1024)
        quantized_size = os.path.getsize(quantized_model_path) / (1024 * 1024)
        
        print(f" ë™ì  ì–‘ìí™” ì™„ë£Œ")
        print(f"    í¬ê¸° ë³€í™”: {original_size:.2f}MB â†’ {quantized_size:.2f}MB ({quantized_size/original_size*100:.1f}%)")
        
        return True
        
    except Exception as e:
        print(f" ë™ì  ì–‘ìí™” ì‹¤íŒ¨: {e}")
        return False

# --- ì„±ëŠ¥ ë²¤ì¹˜ë§ˆí‚¹ í•¨ìˆ˜ ---
def benchmark_model(model, test_data, num_runs=100):
    """ëª¨ë¸ ì„±ëŠ¥ ë²¤ì¹˜ë§ˆí‚¹"""
    print(f" ì„±ëŠ¥ ë²¤ì¹˜ë§ˆí‚¹ ì‹œì‘ ({num_runs}íšŒ ì‹¤í–‰)...")
    
    # ì›Œë°ì—…
    for _ in range(10):
        model.predict(test_data)
    
    # ì‹¤ì œ ë²¤ì¹˜ë§ˆí‚¹
    start_time = time.time()
    for _ in range(num_runs):
        model.predict(test_data)
    end_time = time.time()
    
    avg_time = (end_time - start_time) / num_runs * 1000  # ms
    fps = 1 / (avg_time / 1000)
    
    print(f"    í‰ê·  ì¶”ë¡  ì‹œê°„: {avg_time:.2f}ms")
    print(f"    ì´ˆë‹¹ í”„ë ˆì„: {fps:.1f} FPS")
    
    return avg_time, fps

# --- ë¹„ë™ê¸° ì˜ˆì¸¡ í´ë˜ìŠ¤ ---
class AsyncPredictor:
    def __init__(self, model):
        self.model = model
        self.input_queue = queue.Queue(maxsize=2)
        self.output_queue = queue.Queue(maxsize=2)
        self.prediction_thread = threading.Thread(target=self._prediction_worker)
        self.prediction_thread.daemon = True
        self.prediction_thread.start()
        self.last_prediction = None
    
    def _prediction_worker(self):
        """ë°±ê·¸ë¼ìš´ë“œ ì˜ˆì¸¡ ì‘ì—…ì"""
        while True:
            try:
                input_data = self.input_queue.get(timeout=0.1)
                result = self.model.predict(input_data)
                
                # ê²°ê³¼ íê°€ ê°€ë“ ì°¬ ê²½ìš° ì´ì „ ê²°ê³¼ ì œê±°
                if self.output_queue.full():
                    try:
                        self.output_queue.get_nowait()
                    except queue.Empty:
                        pass
                
                self.output_queue.put(result)
                self.input_queue.task_done()
                
            except queue.Empty:
                continue
            except Exception as e:
                print(f" ë¹„ë™ê¸° ì˜ˆì¸¡ ì˜¤ë¥˜: {e}")
    
    def predict_async(self, input_data):
        """ë¹„ë™ê¸° ì˜ˆì¸¡ ìš”ì²­"""
        # ì…ë ¥ íê°€ ê°€ë“ ì°¬ ê²½ìš° ì´ì „ ìš”ì²­ ì œê±°
        if self.input_queue.full():
            try:
                self.input_queue.get_nowait()
            except queue.Empty:
                pass
        
        try:
            self.input_queue.put_nowait(input_data)
        except queue.Full:
            pass
    
    def get_prediction(self):
        """ì˜ˆì¸¡ ê²°ê³¼ ê°€ì ¸ì˜¤ê¸°"""
        try:
            result = self.output_queue.get_nowait()
            self.last_prediction = result
            return result
        except queue.Empty:
            return self.last_prediction

# --- ëª¨ë¸ ì´ˆê¸°í™” í•¨ìˆ˜ ---
def initialize_optimized_model():
    """ìµœì í™”ëœ ëª¨ë¸ ì´ˆê¸°í™”"""
    print(" ìµœì í™”ëœ ONNX ëª¨ë¸ ì´ˆê¸°í™” ì‹œì‘...")
    
    # 1. ì›ë³¸ ONNX ëª¨ë¸ í™•ì¸
    if not os.path.exists(ONNX_MODEL_PATH):
        print(f" ì›ë³¸ ONNX ëª¨ë¸ì´ ì—†ìŠµë‹ˆë‹¤: {ONNX_MODEL_PATH}")
        print(" ë¨¼ì € convert_h5_to_onnx.pyë¥¼ ì‹¤í–‰í•˜ì—¬ ëª¨ë¸ì„ ë³€í™˜í•˜ì„¸ìš”.")
        return None, None
    
    # 2. ë™ì  ì–‘ìí™” ëª¨ë¸ ìƒì„± (ì—†ëŠ” ê²½ìš°ì—ë§Œ)
    if not os.path.exists(ONNX_QUANTIZED_PATH):
        print(" ë™ì  ì–‘ìí™” ëª¨ë¸ ìƒì„± ì¤‘...")
        create_quantized_model(ONNX_MODEL_PATH, ONNX_QUANTIZED_PATH)
    
    # 3. ìµœì í™”ëœ ëª¨ë¸ ë¡œë“œ ì‹œë„
    models_to_try = [
        (ONNX_QUANTIZED_PATH, "ìµœì í™” + ë™ì  ì–‘ìí™”"),
        (ONNX_MODEL_PATH, "ê¸°ë³¸ ONNX")
    ]
    
    for model_path, description in models_to_try:
        if os.path.exists(model_path):
            try:
                # GPU ì‚¬ìš© ê°€ëŠ¥ ì—¬ë¶€ í™•ì¸
                use_gpu = len([p for p in ort.get_available_providers() 
                              if p in ['CUDAExecutionProvider', 'DmlExecutionProvider', 'OpenVINOExecutionProvider']]) > 0
                
                model = OptimizedONNXModel(
                    model_path, 
                    optimization_level="all",
                    use_gpu=use_gpu
                )
                
                print(f" {description} ëª¨ë¸ ë¡œë“œ ì„±ê³µ")
                return model, description
                
            except Exception as e:
                print(f" {description} ëª¨ë¸ ë¡œë“œ ì‹¤íŒ¨: {e}")
                continue
    
    # 4. ë°±ì—…ìœ¼ë¡œ H5 ëª¨ë¸ ì‹œë„
    try:
        import tensorflow as tf
        from tensorflow import keras
        h5_model_path = "C:/Users/kccistc/project/pth/jaehong_skin_model.h5"
        model = keras.models.load_model(h5_model_path)
        print(" ë°±ì—… H5 ëª¨ë¸ ë¡œë“œ ì„±ê³µ")
        return model, "H5 ë°±ì—…"
    except Exception as e:
        print(f" ë°±ì—… H5 ëª¨ë¸ ë¡œë“œ ì‹¤íŒ¨: {e}")
    
    return None, None

# --- Ollama Gemma3 í•¨ìˆ˜ ---
def get_solution_from_gemma(disease_name):
    """Ollamaë¥¼ í†µí•´ ë¡œì»¬ Gemma3 ëª¨ë¸ì„ í˜¸ì¶œí•˜ì—¬ ì§ˆí™˜ì— ëŒ€í•œ í•´ê²°ì±…ì„ ìƒì„±í•©ë‹ˆë‹¤."""
    prompt = f"""ë‹¹ì‹ ì€ í”¼ë¶€ ê±´ê°• ì „ë¬¸ AI ì–´ì‹œìŠ¤í„´íŠ¸ì…ë‹ˆë‹¤.
ë‹¤ìŒ í”¼ë¶€ ì§ˆí™˜ì— ëŒ€í•´ ì¼ë°˜ì¸ì´ ì´í•´í•˜ê¸° ì‰½ê²Œ ì„¤ëª…í•˜ê³ , ì§‘ì—ì„œ í•  ìˆ˜ ìˆëŠ” ê´€ë¦¬ ë°©ë²•ê³¼ ì „ë¬¸ì ì¸ ì¹˜ë£Œ ë°©ë²•ì„ ë‹¨ê³„ë³„ë¡œ ëª…í™•í•˜ê²Œ êµ¬ë¶„í•´ì„œ í•œêµ­ì–´ë¡œ ì•Œë ¤ì£¼ì„¸ìš”.

**ì§ˆí™˜ëª…: {disease_name}**

ë‹¤ìŒ í˜•ì‹ìœ¼ë¡œ ë‹µë³€í•´ì£¼ì„¸ìš”:
1. ì§ˆí™˜ ì„¤ëª… (ê°„ë‹¨ëª…ë£Œí•˜ê²Œ)
2. ì¦‰ì‹œ ì¡°ì¹˜ì‚¬í•­ (ì‘ê¸‰ì„± ì—¬ë¶€)
3. ê°€ì • ê´€ë¦¬ ë°©ë²•
4. ì „ë¬¸ ì¹˜ë£Œ ë°©ë²•
5. ì£¼ì˜ì‚¬í•­

200ì ë‚´ì™¸ë¡œ ê°„ê²°í•˜ê²Œ ë‹µë³€í•´ì£¼ì„¸ìš”."""
    
    print(f"\n[{OLLAMA_MODEL} ëª¨ë¸ì—ê²Œ ì¡°ì–¸ì„ ìš”ì²­í•©ë‹ˆë‹¤... ì ì‹œë§Œ ê¸°ë‹¤ë ¤ ì£¼ì„¸ìš”.]")
    
    try:
        response = ollama.chat(
            model=OLLAMA_MODEL,
            messages=[{'role': 'user', 'content': prompt}]
        )
        return response['message']['content']
    except Exception as e:
        return f"Ollama ëª¨ë¸ì„ í˜¸ì¶œí•˜ëŠ” ì¤‘ ì˜¤ë¥˜ê°€ ë°œìƒí–ˆìŠµë‹ˆë‹¤: {e}\nOllamaê°€ ì‹¤í–‰ ì¤‘ì¸ì§€ í™•ì¸í•˜ì„¸ìš”."

# --- ë©”ì¸ ë¡œì§ ---
def main():
    print(" ìµœì í™”ëœ ONNX ê¸°ë°˜ í”¼ë¶€ ì§ˆí™˜ ì§„ë‹¨ ì‹œìŠ¤í…œ")
    print("=" * 55)
    
    # ì‹œìŠ¤í…œ ì •ë³´ ì¶œë ¥
    print(f" CPU ì½”ì–´: {psutil.cpu_count(logical=False)} ë¬¼ë¦¬ / {psutil.cpu_count(logical=True)} ë…¼ë¦¬")
    print(f" ì‚¬ìš© ê°€ëŠ¥í•œ ë©”ëª¨ë¦¬: {psutil.virtual_memory().available / (1024**3):.1f} GB")
    print(f" ì‚¬ìš© ê°€ëŠ¥í•œ ONNX Providers: {ort.get_available_providers()}")
    print("=" * 55)
    
    # ìº¡ì²˜ í´ë” ìƒì„±
    if not os.path.exists(CAPTURE_FOLDER):
        os.makedirs(CAPTURE_FOLDER)
    
    # ìµœì í™”ëœ ëª¨ë¸ ì´ˆê¸°í™”
    model, model_type = initialize_optimized_model()
    if model is None:
        print(" ì‚¬ìš© ê°€ëŠ¥í•œ ëª¨ë¸ì´ ì—†ìŠµë‹ˆë‹¤.")
        return
    
    print(f" ì‚¬ìš© ì¤‘ì¸ ëª¨ë¸: {model_type}")
    
    # ì„±ëŠ¥ ë²¤ì¹˜ë§ˆí‚¹ (ONNX ëª¨ë¸ì˜ ê²½ìš°)
    if "ONNX" in model_type or "ìµœì í™”" in model_type:
        test_data = np.random.random((1, 96, 96, 3)).astype(np.float32)
        avg_time, fps = benchmark_model(model, test_data)
        
        # ë¹„ë™ê¸° ì˜ˆì¸¡ê¸° ì´ˆê¸°í™”
        async_predictor = AsyncPredictor(model)
        use_async = True
        print(" ë¹„ë™ê¸° ì˜ˆì¸¡ ëª¨ë“œ í™œì„±í™”")
    else:
        use_async = False
        print(" ë™ê¸° ì˜ˆì¸¡ ëª¨ë“œ ì‚¬ìš©")
    
    # í°íŠ¸ ì„¤ì •
    font_path = "/usr/share/fonts/truetype/nanum/NanumGothicExtraBold.ttf"
    try:
        font = ImageFont.truetype(font_path, 20)
        small_font = ImageFont.truetype(font_path, 14)
    except IOError:
        font = ImageFont.load_default()
        small_font = ImageFont.load_default()


    # ì¹´ë©”ë¼ ì„¤ì •
    cap = cv2.VideoCapture(0)
    if not cap.isOpened():
        cap = cv2.VideoCapture(0)
        if not cap.isOpened():
            print(" ì¹´ë©”ë¼ë¥¼ ì—´ ìˆ˜ ì—†ìŠµë‹ˆë‹¤.")
            return

    # ì¹´ë©”ë¼ í•´ìƒë„ ìµœì í™”
    cap.set(cv2.CAP_PROP_FRAME_WIDTH, 1280)
    cap.set(cv2.CAP_PROP_FRAME_HEIGHT, 720)
    cap.set(cv2.CAP_PROP_FPS, 30)

    print(" ì¹´ë©”ë¼ê°€ ì¤€ë¹„ë˜ì—ˆìŠµë‹ˆë‹¤.")
    print("í™”ë©´ì„ ë³´ë©° ì§„ë‹¨í•  ë¶€ìœ„ë¥¼ ì¤‘ì•™ì— ìœ„ì¹˜ì‹œí‚¤ì„¸ìš”.")
    print("í‚¤ë³´ë“œ 'c'ë¥¼ ëˆ„ë¥´ë©´ 5ì´ˆê°„ ì—°ì†ìœ¼ë¡œ ì´¬ì˜í•˜ì—¬ ì§„ë‹¨í•©ë‹ˆë‹¤.")
    print("í‚¤ë³´ë“œ 'q'ë¥¼ ëˆ„ë¥´ë©´ í”„ë¡œê·¸ë¨ì„ ì¢…ë£Œí•©ë‹ˆë‹¤.")
    print("í‚¤ë³´ë“œ 'b'ë¥¼ ëˆ„ë¥´ë©´ ë²¤ì¹˜ë§ˆí‚¹ì„ ë‹¤ì‹œ ì‹¤í–‰í•©ë‹ˆë‹¤.")

    # ì„±ëŠ¥ ì¸¡ì • ë³€ìˆ˜
    frame_count = 0
    fps_start_time = time.time()
    
    while True:
        ret, frame = cap.read()
        if not ret:
            print("ì˜¤ë¥˜: ì¹´ë©”ë¼ì—ì„œ í”„ë ˆì„ì„ ì½ì„ ìˆ˜ ì—†ìŠµë‹ˆë‹¤.")
            break

        # ì¤‘ì•™ 1:1 ì˜ì—­ crop
        h, w, _ = frame.shape
        min_dim = min(h, w)
        start_x = (w - min_dim) // 2
        start_y = (h - min_dim) // 2
        crop_frame = frame[start_y:start_y+min_dim, start_x:start_x+min_dim]

        # ì´ë¯¸ì§€ ì „ì²˜ë¦¬
        img_array = cv2.resize(crop_frame, (96, 96))
        img_array = np.expand_dims(img_array, axis=0)
        img_array = img_array.astype(np.float32) / 255.0

        # ì˜ˆì¸¡ ìˆ˜í–‰
        if use_async:
            # ë¹„ë™ê¸° ì˜ˆì¸¡
            async_predictor.predict_async(img_array)
            predictions = async_predictor.get_prediction()
            
            if predictions is not None:
                predicted_class_idx = np.argmax(predictions[0])
                confidence = predictions[0][predicted_class_idx]
            else:
                predicted_class_idx = 0
                confidence = 0.0
        else:
            # ë™ê¸° ì˜ˆì¸¡
            if "ONNX" in model_type or "ìµœì í™”" in model_type:
                predictions = model.predict(img_array)
                if predictions is not None:
                    predicted_class_idx = np.argmax(predictions[0])
                    confidence = predictions[0][predicted_class_idx]
                else:
                    predicted_class_idx = 0
                    confidence = 0.0
            else:
                predictions = model.predict(img_array, verbose=0)
                predicted_class_idx = np.argmax(predictions[0])
                confidence = predictions[0][predicted_class_idx]

        # FPS ê³„ì‚°
        frame_count += 1
        if frame_count % 30 == 0:
            fps_end_time = time.time()
            current_fps = 30 / (fps_end_time - fps_start_time)
            fps_start_time = fps_end_time
        else:
            current_fps = 0

        # ê²°ê³¼ í…ìŠ¤íŠ¸ ìƒì„±
        label = f"{class_names_kr[predicted_class_idx]} ({confidence*100:.1f}%)"
        
        # í™”ë©´ì— í‘œì‹œ
        img_pil = Image.fromarray(cv2.cvtColor(crop_frame, cv2.COLOR_BGR2RGB))
        draw = ImageDraw.Draw(img_pil)
        
        # ë©”ì¸ ì •ë³´
        draw.text((10, 10), f" ì‹¤ì‹œê°„ ì˜ˆì¸¡ ({model_type}):", font=font, fill=(0, 255, 0))
        draw.text((10, 35), label, font=font, fill=(0, 255, 0))
        
        # ì„±ëŠ¥ ì •ë³´
        if frame_count % 30 == 0 and current_fps > 0:
            draw.text((10, 65), f" FPS: {current_fps:.1f}", font=small_font, fill=(255, 255, 0))
        
        # ì‚¬ìš© ì¤‘ì¸ Provider ì •ë³´ (ONNX ëª¨ë¸ì˜ ê²½ìš°)
        if hasattr(model, 'session'):
            provider_info = model.session.get_providers()[0]
            draw.text((10, 85), f" Provider: {provider_info.replace('ExecutionProvider', '')}", font=small_font, fill=(255, 255, 0))
        
        display_frame = cv2.cvtColor(np.array(img_pil), cv2.COLOR_RGB2BGR)
        cv2.imshow('ìµœì í™”ëœ ONNX í”¼ë¶€ ì§„ë‹¨', display_frame)

        key = cv2.waitKey(1) & 0xFF

        # 'b' í‚¤ë¡œ ë²¤ì¹˜ë§ˆí‚¹ ì‹¤í–‰
        if key == ord('b') and ("ONNX" in model_type or "ìµœì í™”" in model_type):
            print("\n" + "="*50)
            print(" ì‹¤ì‹œê°„ ë²¤ì¹˜ë§ˆí‚¹ ì‹¤í–‰")
            print("="*50)
            avg_time, fps = benchmark_model(model, img_array)

        # 'c' í‚¤ë¡œ ì§„ë‹¨ ì‹¤í–‰
        elif key == ord('c'):
            # ì§„ë‹¨ ë¡œì§ (ê¸°ì¡´ê³¼ ë™ì¼)
            print("\n" + "="*40)
            print(f"ì§„ë‹¨ì„ ì‹œì‘í•©ë‹ˆë‹¤. {CAPTURE_COUNT}ì´ˆ ë™ì•ˆ {CAPTURE_COUNT}ë²ˆ ì´¬ì˜í•©ë‹ˆë‹¤.")
            print("="*40)
            
            captured_classes = []
            
            for i in range(CAPTURE_COUNT):
                time.sleep(CAPTURE_INTERVAL)
                
                # í˜„ì¬ í”„ë ˆì„ìœ¼ë¡œ ì˜ˆì¸¡
                if "ONNX" in model_type or "ìµœì í™”" in model_type:
                    current_predictions = model.predict(img_array)
                    if current_predictions is not None:
                        current_predicted_idx = np.argmax(current_predictions[0])
                    else:
                        current_predicted_idx = 0
                else:
                    current_predictions = model.predict(img_array, verbose=0)
                    current_predicted_idx = np.argmax(current_predictions[0])
                
                predicted_name = class_names_kr[current_predicted_idx]
                captured_classes.append(predicted_name)
                
                # ìº¡ì²˜ ì´ë¯¸ì§€ ì €ì¥
                timestamp = time.strftime("%Y%m%d_%H%M%S")
                capture_path = os.path.join(CAPTURE_FOLDER, f"capture_{timestamp}_{i+1}.png")
                cv2.imwrite(capture_path, crop_frame)
                
                print(f"ì´¬ì˜ {i+1}/5... ì˜ˆì¸¡: {predicted_name}")

            # ìµœì¢… ì§„ë‹¨
            print("\n" + "-"*40)
            if len(set(captured_classes)) == 1:
                final_diagnosis = captured_classes[0]
                print(f"ìµœì¢… ì§„ë‹¨ ê²°ê³¼: **{final_diagnosis}**")
                print(f"ì‚¬ìš© ëª¨ë¸: {model_type}")
                print("-"*40)
                
                # Gemma3 í•´ê²°ì±… ìš”ì²­
                solution = get_solution_from_gemma(final_diagnosis)
                print("\n[Ollama Gemma3ì˜ ê±´ê°• ì¡°ì–¸]")
                print(solution)
                print("\n(ì£¼ì˜: ì´ ì •ë³´ëŠ” ì°¸ê³ ìš©ì´ë©°, ì •í™•í•œ ì§„ë‹¨ê³¼ ì¹˜ë£Œë¥¼ ìœ„í•´ ë°˜ë“œì‹œ ì „ë¬¸ ì˜ë£Œê¸°ê´€ì„ ë°©ë¬¸í•˜ì„¸ìš”.)")
                
            else:
                print("ì§„ë‹¨ ì‹¤íŒ¨: ì˜ˆì¸¡ ê²°ê³¼ê°€ ì¼ì¹˜í•˜ì§€ ì•ŠìŠµë‹ˆë‹¤.")
                print(f"ì§€ë‚œ {CAPTURE_COUNT}ë²ˆì˜ ì˜ˆì¸¡: {captured_classes}")
            
            print("="*40)
            print("\në‹¤ì‹œ ì§„ë‹¨í•˜ë ¤ë©´ 'c'ë¥¼, ë²¤ì¹˜ë§ˆí‚¹ì€ 'b'ë¥¼, ì¢…ë£Œí•˜ë ¤ë©´ 'q'ë¥¼ ëˆ„ë¥´ì„¸ìš”.")

        # 'q' í‚¤ë¡œ ì¢…ë£Œ
        elif key == ord('q'):
            print("í”„ë¡œê·¸ë¨ì„ ì¢…ë£Œí•©ë‹ˆë‹¤.")
            break

    cap.release()
    cv2.destroyAllWindows()

if __name__ == "__main__":
    main() 
```